{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functions import equal, find_nearest, impute_immediate_mean, impute_mean_day, daylight_savings_shift, clean_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['madrid', 'seville', 'barcelona', 'bilbao', 'valencia']\n",
    "dfs = []\n",
    "for city in cities:\n",
    "    \n",
    "    # Read in city dataframe\n",
    "    city_df = pd.read_csv(f'../data/weather/{city}.csv', index_col=0)\n",
    "\n",
    "    # Clean the madrid data\n",
    "    city_df = clean_weather(city_df)\n",
    "\n",
    "    # Rename columns\n",
    "    city_df.columns = city_df.columns + f'_{city}'\n",
    "    \n",
    "    dfs.append(city_df)\n",
    "    \n",
    "# Create daterange\n",
    "index = pd.date_range(start=dt.datetime(2015,1,1),\n",
    "                      end = dt.datetime(2021,12,31, 23),\n",
    "                      freq='H')\n",
    "range_ = pd.DataFrame(index=index)\n",
    "\n",
    "# Join to date range in weather data\n",
    "weather = range_.join([dfs[0], dfs[1], dfs[2], dfs[3], dfs[4]])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "weather.drop(columns=['index_madrid', 'index_seville', 'index_barcelona',\n",
    "                      'index_bilbao', 'index_valencia'],\n",
    "             inplace=True)\n",
    "\n",
    "# Get list of categoricals and continuous variables\n",
    "categorical = weather.select_dtypes(include='object').columns\n",
    "continuous = weather.select_dtypes(exclude='object').columns\n",
    "\n",
    "# Impute the mean day for continuous variables with 12 or more missing\n",
    "for col in continuous:\n",
    "    impute_mean_day(weather, col, 12)\n",
    "\n",
    "# Interpolate remaining continuous Nans\n",
    "weather.loc[:, continuous] = weather.loc[:, continuous].interpolate(limit=12)\n",
    "\n",
    "# Back fill categorical nans\n",
    "weather.loc[:,categorical] = weather[categorical].fillna(value='unknown')\n",
    "\n",
    "# Back fill remaining nans (first five rows of dataset)\n",
    "weather = weather.fillna(method='bfill')\n",
    "\n",
    "# Drop duplicates in the index\n",
    "weather.reset_index(inplace=True)\n",
    "weather.drop_duplicates(subset='index', inplace=True)\n",
    "weather.set_index('index', inplace=True)\n",
    "\n",
    "# Drop precips cols\n",
    "weather.drop(columns = weather.filter(regex='precips').columns, inplace=True)\n",
    "\n",
    "# Lag the columns one day\n",
    "weather_lag = weather.shift(1, freq='D')\n",
    "\n",
    "# Rename weather_lag columns\n",
    "lag_names = [col+'_lag' for col in weather_lag.columns]\n",
    "lag_dict = dict(zip(weather_lag.columns, lag_names))\n",
    "weather_lag.rename(columns=lag_dict, inplace=True)\n",
    "\n",
    "# Export to clean folder\n",
    "weather.to_csv('../data/clean/weather_clean.csv')\n",
    "weather_lag.to_csv('../data/clean/weather_lag_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Border Transmission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in France Border Data\n",
    "france = pd.DataFrame()\n",
    "portugal = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir('../data/cross_border/france'):\n",
    "    data = pd.read_csv(f'../data/cross_border/france/{file}')\n",
    "    data.drop_duplicates(subset='Time (CET)', inplace=True)\n",
    "    france = pd.concat([france, data],axis=0)\n",
    "\n",
    "# Convert Time to datetime\n",
    "france['time'] = france['Time (CET)'].apply(lambda x: x[:13])\n",
    "france['time'] = pd.to_datetime(france['time'], format=\"%d.%m.%Y %H\")\n",
    "france = france.set_index('time').drop(columns='Time (CET)')\n",
    "\n",
    "# Read in Portugal Border Data\n",
    "for file in os.listdir('../data/cross_border/portugal'):\n",
    "    data = pd.read_csv(f'../data/cross_border/portugal/{file}')\n",
    "    data.drop_duplicates(subset='Time (CET)', inplace=True)\n",
    "    portugal = pd.concat([portugal, data],axis=0)\n",
    "portugal['time'] = portugal['Time (CET)'].apply(lambda x: x[:13])\n",
    "portugal['time'] = pd.to_datetime(portugal['time'], format=\"%d.%m.%Y %H\")\n",
    "portugal = portugal.set_index('time').drop(columns='Time (CET)')\n",
    "\n",
    "# Join France and Portugal Data\n",
    "border = portugal.join(france)\n",
    "border.fillna(method='ffill', inplace=True)\n",
    "cols = dict(zip(border.columns,['transmission_ps', \n",
    "                                'transmission_sp', \n",
    "                                'transmission_fs',\n",
    "                                'transmission_sf']))\n",
    "\n",
    "border.rename(columns=cols, inplace=True)\n",
    "\n",
    "# lag border data and rename cols\n",
    "border_lag = border.shift(1, freq='D')\n",
    "lag_names = [col+'_lag' for col in border_lag.columns]\n",
    "lag_dict = dict(zip(border_lag.columns, lag_names))\n",
    "border_lag.rename(columns=lag_dict, inplace=True)\n",
    "\n",
    "# Export to clean folder\n",
    "border.to_csv('../data/clean/border_clean.csv')\n",
    "border_lag.to_csv('../data/clean/border_lag_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pd.DataFrame()\n",
    "for file in os.listdir('../data/generation'):\n",
    "    load = pd.read_csv(f'../data/generation/{file}')\n",
    "    load.drop_duplicates(subset='MTU', inplace=True)\n",
    "    gen = pd.concat([gen, load], axis=0)\n",
    "    \n",
    "# Get rid of columns that do not contain any information\n",
    "gen = gen.drop(columns = gen.loc[:,gen.nunique()<=1].columns)\n",
    "\n",
    "# Convert Time to datetime\n",
    "gen['time'] = pd.to_datetime(gen[gen.columns[0]].apply(lambda x: x[:13]),\n",
    "                             format=\"%d.%m.%Y %H\")\n",
    "\n",
    "# Set index to the time col\n",
    "gen.set_index('time', inplace=True)\n",
    "\n",
    "# Drop MTU col\n",
    "gen.drop(columns='MTU', inplace=True)\n",
    "\n",
    "# Rename cols\n",
    "gen.columns = gen.columns.map(lambda x: ('generation '+ x[:-26]).lower())\n",
    "gen.rename(columns={'generation hydro pumped storage ': 'generation hydro pumped storage consumption'},\n",
    "           inplace=True)\n",
    "\n",
    "# Impute Immediate Mean for NaNs\n",
    "for col in gen.columns:\n",
    "    indices = gen.loc[gen[col].isna()].index\n",
    "    for i in indices:\n",
    "        gen.loc[i,col] = impute_immediate_mean(gen[col], i)\n",
    "\n",
    "# Rename cols\n",
    "col_map = dict(zip(gen.columns, [col.split(' ')[-1] for col in gen.columns]))\n",
    "gen.rename(columns=col_map, inplace=True)\n",
    "\n",
    "# Shift columns\n",
    "gen_lag = gen.shift(1, freq='D')\n",
    "\n",
    "# Rename Lag Columns\n",
    "col_map = dict(zip(gen.columns, [col + '_lag' for col in gen.columns]))\n",
    "gen_lag.rename(columns=col_map, inplace=True)\n",
    "\n",
    "# Export to clean folder\n",
    "gen.to_csv('../data/clean/generation_clean.csv')\n",
    "gen_lag.to_csv('../data/clean/generation_lag_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Forecast and Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_forecast = pd.DataFrame()\n",
    "for file in os.listdir('../data/load'):\n",
    "    load = pd.read_csv(f'../data/load/{file}')\n",
    "    if load.columns[0] == 'Time (CET/CEST)':\n",
    "        load.rename(columns={'Time (CET/CEST)':'Time (CET)'}, inplace=True)\n",
    "    load.drop_duplicates(subset=load.columns[0], inplace=True)\n",
    "    load_forecast = pd.concat([load_forecast, load], axis=0)\n",
    "\n",
    "# Convert Time to datetime\n",
    "load_forecast['time'] = pd.to_datetime(load_forecast['Time (CET)'].apply(lambda x: x[:13]),\n",
    "                                       format=\"%d.%m.%Y %H\")\n",
    "\n",
    "# Set index to the time col\n",
    "load_forecast.set_index('time', inplace=True)\n",
    "\n",
    "# Drop 'Time (CET)' col\n",
    "load_forecast.drop(columns='Time (CET)', inplace=True)\n",
    "\n",
    "# Rename cols\n",
    "load_forecast.rename(columns = {'Day-ahead Total Load Forecast [MW] - BZN|ES':'load_forecast',\n",
    "                                'Actual Total Load [MW] - BZN|ES':'load_actual'}, \n",
    "                     inplace=True)\n",
    "\n",
    "# Impute Immediate Mean for NaNs\n",
    "for col in load_forecast.columns:\n",
    "    indices = load_forecast.loc[load_forecast[col].isna()].index\n",
    "    for i in indices:\n",
    "        load_forecast.loc[i,col] = impute_immediate_mean(load_forecast[col], i)\n",
    "        \n",
    "# Shift load_actual\n",
    "load_forecast_lag = load_forecast.join(load_forecast.load_actual.shift(1, 'D'), how='outer', lsuffix='_drop')\n",
    "load_forecast_lag.drop(columns='load_actual_drop', inplace=True)\n",
    "\n",
    "# Rename Lagged cols\n",
    "load_forecast_lag.rename(columns={'load_actual':'load_actual_lag'}, inplace=True)\n",
    "\n",
    "# Export to clean folder\n",
    "load_forecast.to_csv('../data/clean/load_forecast_clean.csv')\n",
    "load_forecast_lag.to_csv('../data/clean/load_forecast_lag.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind and Solar Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = pd.DataFrame()\n",
    "for file in os.listdir('../data/wind_solar_day_ahead'):\n",
    "    load = pd.read_csv(f'../data/wind_solar_day_ahead/{file}')\n",
    "    if load.columns[0] != 'Time (CET)':\n",
    "        load.rename(columns={load.columns[0]:'Time (CET)'}, inplace=True)\n",
    "    load.drop_duplicates(subset=['Time (CET)'], inplace=True)\n",
    "    ws = pd.concat([ws, load], axis=0)\n",
    "\n",
    "# Convert Time to datetime\n",
    "ws['time'] = pd.to_datetime(ws[ws.columns[0]].apply(lambda x: x[:13]),\n",
    "                                       format=\"%d.%m.%Y %H\")\n",
    "\n",
    "# Set index to the time col\n",
    "ws.set_index('time', inplace=True)\n",
    "\n",
    "# Drop 'Time (CET)' col\n",
    "ws = ws[['Generation - Solar  [MW] Day Ahead/ BZN|ES',\n",
    "         'Generation - Wind Onshore  [MW] Day Ahead/ BZN|ES']].copy()\n",
    "\n",
    "# Rename cols\n",
    "ws.rename(columns = {'Generation - Solar  [MW] Day Ahead/ BZN|ES':'solar_forecast',\n",
    "                     'Generation - Wind Onshore  [MW] Day Ahead/ BZN|ES':'wind_forecast'}, \n",
    "                     inplace=True)\n",
    "\n",
    "# 2020-05-01 wind forecast is missing, impute average for that day in may\n",
    "avg_w = ws['wind_forecast'].groupby(by=[ws.index.month, \n",
    "                                        ws.index.day,\n",
    "                                        ws.index.hour]).mean().loc[(5,1)]\n",
    "for i, time in enumerate(ws.loc['2020-05-01'].index):\n",
    "    ws.loc[time, 'wind_forecast'] = avg_w[i]\n",
    "    \n",
    "# Impute the immediate mean for remaining NaNs\n",
    "for col in ws.columns:\n",
    "    indices = ws.loc[ws[col].isna()].index\n",
    "    for i in indices:\n",
    "        ws.loc[i,col] = impute_immediate_mean(ws[col], i)\n",
    "        \n",
    "# Shift columns\n",
    "ws_lag = ws.shift(1, freq='D')\n",
    "\n",
    "# Rename Lag Columns\n",
    "col_map = dict(zip(ws_lag.columns, [col + '_lag' for col in ws_lag.columns]))\n",
    "ws_lag.rename(columns=col_map, inplace=True)\n",
    "\n",
    "# Export to clean folder\n",
    "ws.to_csv('../data/clean/wind_solar_clean.csv')\n",
    "ws_lag.to_csv('../data/clean/wind_solar_clean_lag.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Forecast Day ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and combine all day ahead price data into single dataframe\n",
    "gen_forecast = pd.DataFrame()\n",
    "for file in os.listdir('../data/gen_forecast'):\n",
    "    load = pd.read_csv(f'../data/gen_forecast/{file}')\n",
    "    load.drop_duplicates(subset='MTU', inplace=True)\n",
    "    gen_forecast = pd.concat([gen_forecast, load], axis=0)\n",
    "    \n",
    "# Convert Time to datetime\n",
    "gen_forecast['time'] = gen_forecast['MTU'].apply(lambda x: x[:16])\n",
    "gen_forecast['time'] = pd.to_datetime(gen_forecast['time'])\n",
    "\n",
    "# Convert Time to datetime\n",
    "gen_forecast['time'] = pd.to_datetime(gen_forecast[gen_forecast.columns[0]].apply(lambda x: x[:13]),\n",
    "                                      format=\"%d.%m.%Y %H\")\n",
    "\n",
    "# Drop unused columns and rows\n",
    "gen_forecast.drop(columns='MTU', inplace=True)\n",
    "\n",
    "# Set index to the time col\n",
    "gen_forecast.set_index('time', inplace=True)\n",
    "\n",
    "\n",
    "# Rename cols\n",
    "gen_forecast.rename(columns={'Scheduled Generation [MW] (D) - BZN|ES':'generation_forecast',\n",
    "                             'Scheduled Consumption [MW] (D) - BZN|ES':'consumption_forecast'},\n",
    "                    inplace=True)\n",
    "\n",
    "for col in gen_forecast.columns:\n",
    "    \n",
    "    # Impute the mean of the nearest known date by hour\n",
    "    impute_mean_day(gen_forecast, col, 24)\n",
    "    \n",
    "    # Fill remaining Nans\n",
    "    gen_forecast[col].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Export to clean folder\n",
    "gen_forecast.to_csv('../data/clean/gen_forecast_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prices and Day Ahead Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change directory and get all files in data/price directory\n",
    "os.chdir('../data/price')\n",
    "files = os.listdir()\n",
    "\n",
    "# Read all files in directory\n",
    "prices = pd.DataFrame()\n",
    "for file in files:\n",
    "    prices =pd.concat([prices, pd.read_csv(file, delimiter=';', index_col=5, parse_dates=True)])\n",
    "\n",
    "# Join on date_range to make sure we aren't missing any rows\n",
    "price_df = pd.DataFrame(\n",
    "    index=pd.date_range(start=dt.datetime(2015,1,1),\n",
    "                        end=dt.datetime(2021,12,31,23),\n",
    "                        freq='H',\n",
    "                        tz='CET')\n",
    ")\n",
    "\n",
    "# Create cols dictionary to rename cols\n",
    "cols = {'Hourly average price final sum of components':'price_actual',\n",
    "        'Hourly average price Day Ahead market component':'price_day_ahead'}\n",
    "\n",
    "cols = {'Hourly average price final sum of components':'price_actual',\n",
    "       'Hourly average price intraday market tech. constraints component':'price_intraday_tech',\n",
    "       'Hourly average price PBF technical constraints component':'price_PBF_tech',\n",
    "       'Hourly average price real time technical constraints component':'price_rt_tech',\n",
    "       'Hourly average price intraday market component':'price_intraday_market',\n",
    "       'Hourly average price Day Ahead market component':'price_day_ahead',\n",
    "       'Hourly average price Upward reserve power component':'price_upward_reserve',\n",
    "       'Hourly average price secondary reserve component':'price_sec_reserve',\n",
    "       'Hourly average price measured imbalances component': 'price_measured_imbalances',\n",
    "       'Hourly average price imbalances net value component':'price_imbalances_net',\n",
    "       'Hourly average price capacity payment component':'price_capacity_payment',\n",
    "       'Hourly average price P.O.14.6 balance component':'price_P0146_balance',\n",
    "       'Hourly average price Generic Units Nom. Failure component':'price_generic_failure',\n",
    "       'Hourly average price interruptibility service component':'price_interupt_service',\n",
    "       'Hourly average price power factor control component':'price_power_factor',\n",
    "       'Hourly average price balance energy failure component':'price_balance_failure'}\n",
    "\n",
    "# Create price dictionary to hold column data\n",
    "price_dict = {cols[col]:prices.loc[prices.name==col, 'value'] for col in cols.keys()}\n",
    "\n",
    "# For each column, add to price_df\n",
    "for key in price_dict.keys():\n",
    "    price_dict[key] = price_dict[key].rename(key)\n",
    "    price_df = price_df.join(price_dict[key].groupby(by=price_dict[key].index).mean())\n",
    "\n",
    "# Drop all the duplicates\n",
    "price_df = price_df.groupby(by=price_df.index).mean().copy()\n",
    "\n",
    "# Make timezone unaware, shift by an hour to offset the unaware tz\n",
    "price_df.index = price_df.index.tz_convert(None)\n",
    "price_df = price_df.shift(1, freq='H')\n",
    "\n",
    "#Shift prices, drop cols, and rename\n",
    "#price_df = price_df.join(price_df[['price_actual', 'price_day_ahead']].shift(-1, freq='D'), rsuffix='_tomorrow')\n",
    "#price_df.drop(columns='price_day_ahead', inplace=True)\n",
    "#price_df.rename(columns={'price_actual_tomorrow':'price_tomorrow',\n",
    "#                         'price_day_ahead_tomorrow': 'price_forecast_tomorrow'}, inplace=True)\n",
    "price_df.fillna(value=0, inplace=True)\n",
    "# Change directory back to ../script\n",
    "os.chdir('../../scripts')\n",
    "\n",
    "# Export to clean folder\n",
    "price_df.to_csv('../data/clean/price_df_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df with date_range and copy it to second df\n",
    "df = pd.DataFrame(\n",
    "    index=pd.date_range(start = dt.datetime(2014,12,31),\n",
    "                        end = dt.datetime(2022,1,1,23),\n",
    "                        freq = 'H')\n",
    ")\n",
    "df_lag = df.copy()\n",
    "\n",
    "# Join all non-lagged data to create df, and lagged data to created lagged\n",
    "df = df.join([weather, border, gen, load_forecast, ws, gen_forecast, price_df])\n",
    "df_lag = df_lag.join([weather_lag, border_lag, gen_lag, load_forecast, ws_lag, gen_forecast, price_df])\n",
    "\n",
    "\n",
    "# Drop last day since no price data for 2022\n",
    "df.dropna(inplace=True)\n",
    "df_lag.dropna(inplace=True)\n",
    "\n",
    "# Export to clean folder\n",
    "df.to_csv('../data/clean/df_clean.csv')\n",
    "df_lag.to_csv('../data/clean/df__clean_lag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
