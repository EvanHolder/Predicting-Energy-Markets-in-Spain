{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_page(url):\n",
    "    driver = webdriver.Chrome('chromedriver')\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    r = driver.page_source\n",
    "    driver.quit()\n",
    "    return r\n",
    "\n",
    "def extract_table(table):\n",
    "    day = [[] for i in range(10)]\n",
    "    for row in table.find_all('tr', class_='ng-star-inserted'):\n",
    "        for i, col in enumerate(row.find_all('td', class_='ng-star-inserted')):\n",
    "            day[i].append(col.text)\n",
    "    return day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create city code mappings\n",
    "city_codes = {'LEZL':'seville',\n",
    "              'LEBL':'barcelona',\n",
    "              'LEBB':'bilboa',\n",
    "              'LEVC':'valencia'}\n",
    "\n",
    "# Create daterange for the pull\n",
    "dates_stamp = pd.date_range(start=dt.datetime(2015,1,1), end=dt.datetime(2021,12,31),freq='D')\n",
    "\n",
    "# Truncate dates for url\n",
    "dates = [str(x)[:10] for x in dates_stamp]\n",
    "\n",
    "# Create empty dictionary to store data for each city\n",
    "city_data = {}\n",
    "\n",
    "# Loop through each city code, collect data for each city\n",
    "for code in city_codes.keys():\n",
    "    \n",
    "    # Create data dictionary to hold data for each data for this city\n",
    "    data_1 = {}\n",
    "    for date in dates:\n",
    "        \n",
    "        # Request page\n",
    "        url = f'https://www.wunderground.com/history/daily/{code}/date/{date}'\n",
    "        \n",
    "        # Render page\n",
    "        r = render_page(url)\n",
    "        \n",
    "        # Parse page and find table\n",
    "        soup = BS(r, \"html.parser\")\n",
    "        container = soup.find('lib-city-history-observation')\n",
    "        table = container.find('tbody')\n",
    "\n",
    "        # Extract this date's data from the table and add it to dictionary\n",
    "        data_1[date] = extract_table(table)\n",
    "    \n",
    "    # Add all data to city_data dictionary\n",
    "    city_data[code] = data_1\n",
    "\n",
    "# Loop through each city in city_data\n",
    "for code in city_data.keys():\n",
    "    \n",
    "    # Define empty lists for new dataframe cols\n",
    "    dates_ =[]\n",
    "    temps = []\n",
    "    dew_points = []\n",
    "    humidities = []\n",
    "    winds = []\n",
    "    wind_speeds = []\n",
    "    wind_gusts = []\n",
    "    pressures = []\n",
    "    precips = []\n",
    "    conditions = []\n",
    "    \n",
    "    # Loop through each day in the city\n",
    "    for day_key in city_data[code].keys():\n",
    "        \n",
    "        # Loop through each row in the table and append info to col lists\n",
    "        for ind, t in enumerate(city_data[code][day_key][0]):\n",
    "            dates_.append(day_key+' '+city_data[code][day_key][0][ind])\n",
    "            temps.append(city_data[code][day_key][1][ind])\n",
    "            dew_points.append(city_data[code][day_key][2][ind])\n",
    "            humidities.append(city_data[code][day_key][3][ind])\n",
    "            winds.append(city_data[code][day_key][4][ind])\n",
    "            wind_speeds.append(city_data[code][day_key][5][ind])\n",
    "            wind_gusts.append(city_data[code][day_key][6][ind])\n",
    "            pressures.append(city_data[code][day_key][7][ind])\n",
    "            precips.append(city_data[code][day_key][8][ind])\n",
    "            conditions.append(city_data[code][day_key][9][ind])\n",
    "    \n",
    "    # Create dataframe with all column information\n",
    "    df = pd.DataFrame({'date':dates_,\n",
    "                       'temp':temps,\n",
    "                       'dew_point':dew_points,\n",
    "                       'humidities': humidities,\n",
    "                       'wind':winds,\n",
    "                       'wind_speeds':wind_speeds,\n",
    "                       'pressures':pressures,\n",
    "                       'precips':precips,\n",
    "                       'condition':conditions})\n",
    "    df.to_csv(f'../data/{city_data[code]}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
