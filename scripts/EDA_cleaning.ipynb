{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functions import equal, find_nearest, impute_immediate_mean, impute_mean_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = pd.read_csv('../data/energy_dataset.csv')\n",
    "\n",
    "# Chop of nanoseconds, and convert to datetime, reset index\n",
    "energy.time = pd.to_datetime(energy.time.apply(lambda x: x[:-6]))\n",
    "energy.set_index('time', inplace=True)\n",
    "\n",
    "# Get rid of columns that do not contain any information\n",
    "energy = energy.drop(columns = energy.loc[:,energy.nunique()<=1].columns)\n",
    "\n",
    "# Create start stop variable for indices\n",
    "start = dt.datetime(2015, 1, 1)\n",
    "stop = dt.datetime(2018, 12, 31, 23)\n",
    "\n",
    "# Create continuous list of indices by hour\n",
    "yr15_18 = pd.DataFrame(index = pd.date_range(start, stop, freq='H' ))\n",
    "\n",
    "# join existing data on the complete list of indices\n",
    "energy = energy.join(yr15_18,how='right')\n",
    "\n",
    "# Loop through each column and impute missing values\n",
    "for col in energy.columns:\n",
    "    \n",
    "    # Get the indices of missing values in this column\n",
    "    indices = energy.loc[energy[col].isna()].index\n",
    "    \n",
    "    # For each missing value, impute the mean of closest known values\n",
    "    for i in indices:\n",
    "        energy.loc[i, col] = impute_immediate_mean(energy[col], i)\n",
    "        \n",
    "# Get indices of duplicates\n",
    "indices = energy.loc[energy.index.value_counts()>1].index.unique()\n",
    "\n",
    "# average duplicate values for each column\n",
    "for col in energy.columns:\n",
    "    for i in indices:\n",
    "        energy.loc[i, col] = round(energy.loc[i,col].mean(),1)\n",
    "        \n",
    "\n",
    "# Drop duplicates\n",
    "energy.drop_duplicates(inplace=True)\n",
    "\n",
    "# Create total generation column summing all generation sources\n",
    "energy['generation total'] = energy.loc[:,:'generation wind onshore'].sum(axis=1)\n",
    "\n",
    "# Create diff column (difference between total generation and actual load)\n",
    "energy['diff'] = energy['generation total'] - energy['total load actual']\n",
    "\n",
    "columns = ['generation biomass',\n",
    "           'generation fossil brown coal/lignite',\n",
    "           'generation fossil hard coal',\n",
    "           'generation fossil oil',\n",
    "           'generation hydro run-of-river and poundage',\n",
    "           'generation hydro water reservoir',\n",
    "           'generation nuclear',\n",
    "           'generation other',\n",
    "           'generation other renewable',\n",
    "           'generation solar',\n",
    "           'generation waste',\n",
    "           'generation wind onshore',]\n",
    "for col in columns:\n",
    "    flag_indices = energy.loc[energy['diff']<-15000].index\n",
    "    for i in flag_indices:\n",
    "        energy.loc[i, col] = impute_immediate_mean(energy[col], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv('../data/weather_features.csv')\n",
    "\n",
    "# Drop duplicates\n",
    "weather.drop_duplicates(inplace=True)\n",
    "\n",
    "# Cut off nanoseconds and create datetime column\n",
    "weather.dt_iso = pd.to_datetime(weather.dt_iso.apply(lambda x: x[:-6]))\n",
    "\n",
    "# Drop columns since weather description provides most granular level of information\n",
    "weather.drop(columns=['weather_id', 'weather_main', 'weather_icon', 'temp_min', 'temp_max'], inplace=True)\n",
    "\n",
    "# Create weather by city df\n",
    "weather_by_city = pd.DataFrame()\n",
    "\n",
    "for city in weather.city_name.unique():\n",
    "    \n",
    "    # Copy data by city\n",
    "    city_df = weather.loc[weather.city_name == city].copy()\n",
    "\n",
    "    # Get the combined weather descriptions for timestamps with multiple descriptions\n",
    "    combined = city_df.groupby('dt_iso')['weather_description'].transform(lambda x : '/'.join(x))\n",
    "\n",
    "    # assign new to combined descriptions\n",
    "    city_df['description'] = combined\n",
    "\n",
    "    # Drop original weather description column\n",
    "    city_df.drop(columns=['city_name','weather_description'], inplace=True)\n",
    "\n",
    "    # Drop duplicates\n",
    "    city_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Set index\n",
    "    city_df = city_df.set_index('dt_iso')\n",
    "    \n",
    "    # Rename cols\n",
    "    city_df.columns = city_df.columns.map(lambda x: x + f'_{city}')\n",
    "    \n",
    "    # Join to the weather_cities df\n",
    "    weather_by_city = city_df.join(weather_by_city, how='left')\n",
    "\n",
    "# Replace values in duplicated indices with the max\n",
    "weather_by_city = weather_by_city.groupby(by=weather_by_city.index).max()\n",
    "\n",
    "# join existing data on the complete list of indices\n",
    "weather_by_city = weather_by_city.join(yr15_18,how='right')\n",
    "weather_by_city\n",
    "\n",
    "# Impute the immediate mean for all missing values in continuous cols\n",
    "continuous = weather_by_city.select_dtypes(exclude='object').columns\n",
    "for col in continuous:\n",
    "    indices = weather_by_city.loc[weather_by_city[col].isna()].index\n",
    "    for i in indices:\n",
    "        weather_by_city.loc[i,col] = impute_immediate_mean(weather_by_city[col], i)\n",
    "\n",
    "# Impute the previous known col for remaining cols\n",
    "weather_by_city.fillna(method='bfill', inplace=True)\n",
    "    \n",
    "# Get all unique weather descriptions for each city\n",
    "categorical = weather_by_city.select_dtypes(include='object').columns\n",
    "descriptions = set()\n",
    "for col in categorical:\n",
    "    unique = set(weather_by_city[col].unique())\n",
    "    descriptions = unique | descriptions\n",
    "descriptions = list(descriptions)\n",
    "\n",
    "# Instantiate LabelEncoder and transform cols\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(descriptions)\n",
    "for col in categorical:\n",
    "    weather_by_city[col] = encoder.transform(weather_by_city[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Border Transmission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in France Border Data\n",
    "france = pd.DataFrame()\n",
    "portugal = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir('../data/cross_border/france'):\n",
    "    data = pd.read_csv(f'../data/cross_border/france/{file}')\n",
    "    data.drop_duplicates(subset='Time (CET)', inplace=True)\n",
    "    france = pd.concat([france, data],axis=0)\n",
    "\n",
    "# Convert Time to datetime\n",
    "france['time'] = france['Time (CET)'].apply(lambda x: x[:16])\n",
    "france['time'] = pd.to_datetime(france['time'])\n",
    "france = france.set_index('time').drop(columns='Time (CET)')\n",
    "\n",
    "# Read in Portugal Border Data\n",
    "for file in os.listdir('../data/cross_border/portugal'):\n",
    "    data = pd.read_csv(f'../data/cross_border/portugal/{file}')\n",
    "    data.drop_duplicates(subset='Time (CET)', inplace=True)\n",
    "    portugal = pd.concat([portugal, data],axis=0)\n",
    "portugal['time'] = portugal['Time (CET)'].apply(lambda x: x[:16])\n",
    "portugal['time'] = pd.to_datetime(portugal['time'])\n",
    "portugal = portugal.set_index('time').drop(columns='Time (CET)')\n",
    "\n",
    "# Join France and Portugal Data\n",
    "border = portugal.join(france)\n",
    "border.fillna(method='ffill', inplace=True)\n",
    "cols = dict(zip(border.columns,['transmission_ps', \n",
    "                                'transmission_sp', \n",
    "                                'transmission_fs',\n",
    "                                'transmission_sf']))\n",
    "\n",
    "border.rename(columns=cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pd.DataFrame()\n",
    "for file in os.listdir('../data/generation'):\n",
    "    load = pd.read_csv(f'../data/generation/{file}')\n",
    "    load.drop_duplicates(subset='MTU', inplace=True)\n",
    "    gen = pd.concat([gen, load], axis=0)\n",
    "    \n",
    "# Get rid of columns that do not contain any information\n",
    "gen = gen.drop(columns = gen.loc[:,gen.nunique()<=1].columns)\n",
    "\n",
    "# Convert Time to datetime\n",
    "gen['time'] = pd.to_datetime(gen.MTU.apply(lambda x: x[:13]))\n",
    "\n",
    "# Set index to the time col\n",
    "gen.set_index('time', inplace=True)\n",
    "\n",
    "# Drop MTU col\n",
    "gen.drop(columns='MTU', inplace=True)\n",
    "\n",
    "# Rename cols\n",
    "gen.columns = gen.columns.map(lambda x: ('generation '+ x[:-26]).lower())\n",
    "gen.rename(columns={'generation hydro pumped storage ': 'generation hydro pumped storage consumption'},\n",
    "           inplace=True)\n",
    "\n",
    "# Impute Immediate Mean for NaNs\n",
    "for col in gen.columns:\n",
    "    indices = gen.loc[gen[col].isna()].index\n",
    "    for i in indices:\n",
    "        gen.loc[i,col] = impute_immediate_mean(gen[col], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Forecast and Actual (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_forecast = pd.DataFrame()\n",
    "for file in os.listdir('../data/load'):\n",
    "    load = pd.read_csv(f'../data/load/{file}')\n",
    "    load.drop_duplicates(subset='Time (CET)', inplace=True)\n",
    "    load_forecast = pd.concat([load_forecast, load], axis=0)\n",
    "\n",
    "# Convert Time to datetime\n",
    "load_forecast['time'] = pd.to_datetime(load_forecast['Time (CET)'].apply(lambda x: x[:13]))\n",
    "\n",
    "# Set index to the time col\n",
    "load_forecast.set_index('time', inplace=True)\n",
    "\n",
    "# Drop 'Time (CET)' col\n",
    "load_forecast.drop(columns='Time (CET)', inplace=True)\n",
    "\n",
    "# Rename cols\n",
    "load_forecast.rename(columns = {'Day-ahead Total Load Forecast [MW] - BZN|ES':'total load forecast',\n",
    "                                'Actual Total Load [MW] - BZN|ES':'total load actual'}, \n",
    "                     inplace=True)\n",
    "\n",
    "# Impute Immediate Mean for NaNs\n",
    "for col in load_forecast.columns:\n",
    "    indices = load_forecast.loc[load_forecast[col].isna()].index\n",
    "    for i in indices:\n",
    "        load_forecast.loc[i,col] = impute_immediate_mean(load_forecast[col], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind and Solar Forecast (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = pd.DataFrame()\n",
    "for file in os.listdir('../data/wind_solar_day_ahead'):\n",
    "    load = pd.read_csv(f'../data/wind_solar_day_ahead/{file}')\n",
    "    load.drop_duplicates(subset='MTU (CET)', inplace=True)\n",
    "    ws = pd.concat([ws, load], axis=0)\n",
    "    \n",
    "# Convert Time to datetime\n",
    "ws['time'] = pd.to_datetime(ws['MTU (CET)'].apply(lambda x: x[:13]))\n",
    "\n",
    "# Set index to the time col\n",
    "ws.set_index('time', inplace=True)\n",
    "\n",
    "# Drop 'Time (CET)' col\n",
    "ws = ws[['Generation - Solar  [MW] Day Ahead/ BZN|ES',\n",
    "         'Generation - Wind Onshore  [MW] Day Ahead/ BZN|ES']].copy()\n",
    "\n",
    "# Rename cols\n",
    "ws.rename(columns = {'Generation - Solar  [MW] Day Ahead/ BZN|ES':'forecast solar day ahead',\n",
    "                     'Generation - Wind Onshore  [MW] Day Ahead/ BZN|ES':'forecast wind onshore day ahead'}, \n",
    "                     inplace=True)\n",
    "\n",
    "# 2020-05-01 wind forecast is missing, impute average for that day in may\n",
    "avg_w = ws['forecast wind onshore day ahead'].groupby(by=[ws.index.month, \n",
    "                                                          ws.index.day,\n",
    "                                                          ws.index.hour]).mean().loc[(5,1)]\n",
    "for i, time in enumerate(ws.loc['2020-05-01'].index):\n",
    "    ws.loc[time, 'forecast wind onshore day ahead'] = avg_w[i]\n",
    "    \n",
    "# Impute the immediate mean for remaining NaNs\n",
    "for col in ws.columns:\n",
    "    indices = ws.loc[ws[col].isna()].index\n",
    "    for i in indices:\n",
    "        ws.loc[i,col] = impute_immediate_mean(ws[col], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day Ahead Prices (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine all day ahead price data into single dataframe\n",
    "price_ahead = pd.DataFrame()\n",
    "for file in os.listdir('../data/day ahead price'):\n",
    "    load = pd.read_csv(f'../data/day ahead price/{file}')\n",
    "    load.drop_duplicates(subset='MTU (CET)', inplace=True)\n",
    "    price_ahead = pd.concat([price_ahead, load], axis=0)\n",
    "    \n",
    "# Convert Time to datetime\n",
    "price_ahead['time'] = price_ahead['MTU (CET)'].apply(lambda x: x[:16])\n",
    "price_ahead['time'] = pd.to_datetime(price_ahead['time'])\n",
    "\n",
    "# Drop unused columns and rows\n",
    "price_ahead.drop(columns=['MTU (CET)', 'BZN|ES'], inplace=True)\n",
    "\n",
    "# Set index to the time col\n",
    "price_ahead.set_index('time', inplace=True)\n",
    "\n",
    "# Drop all data in 2022 since incomplete\n",
    "price_ahead = price_ahead.loc[:'2021'].copy()\n",
    "\n",
    "# Rename col\n",
    "price_ahead.rename(columns={'Day-ahead Price [EUR/MWh]':'price day ahead'}, inplace=True)\n",
    "\n",
    "# Impute the immediate mean for remaining NaNs\n",
    "for col in price_ahead.columns:\n",
    "    indices = price_ahead.loc[price_ahead[col].isna()].index\n",
    "    for i in indices:\n",
    "        price_ahead.loc[i,col] = impute_immediate_mean(price_ahead[col], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Forecast Day ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and combine all day ahead price data into single dataframe\n",
    "gen_forecast = pd.DataFrame()\n",
    "for file in os.listdir('../data/gen_forecast'):\n",
    "    load = pd.read_csv(f'../data/gen_forecast/{file}')\n",
    "    load.drop_duplicates(subset='MTU', inplace=True)\n",
    "    gen_forecast = pd.concat([gen_forecast, load], axis=0)\n",
    "    \n",
    "# Convert Time to datetime\n",
    "gen_forecast['time'] = gen_forecast['MTU'].apply(lambda x: x[:16])\n",
    "gen_forecast['time'] = pd.to_datetime(gen_forecast['time'])\n",
    "\n",
    "# Drop unused columns and rows\n",
    "gen_forecast.drop(columns='MTU', inplace=True)\n",
    "\n",
    "# Set index to the time col\n",
    "gen_forecast.set_index('time', inplace=True)\n",
    "\n",
    "\n",
    "# Rename cols\n",
    "gen_forecast.rename(columns={'Scheduled Generation [MW] (D) - BZN|ES':'generation_scheduled',\n",
    "                             'Scheduled Consumption [MW] (D) - BZN|ES':'consumption_scheduled'},\n",
    "                    inplace=True)\n",
    "\n",
    "for col in gen_forecast.columns:\n",
    "    \n",
    "    # Impute the mean of the nearest known date by hour\n",
    "    impute_mean_day(gen_forecast, col)\n",
    "    \n",
    "    # Fill remaining Nans\n",
    "    gen_forecast[col].fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prices (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "prices_19_21 = pd.read_csv('../data/prices.csv')\n",
    "\n",
    "# Change column to datetime and set as index\n",
    "prices_19_21['datetime'] = pd.to_datetime(prices_19_21['datetime'])\n",
    "prices_19_21.set_index('datetime', inplace=True)\n",
    "\n",
    "# Drop duplicates and all rows in 2022\n",
    "prices_19_21 = prices_19_21.groupby(by=prices_19_21.index).max()\n",
    "prices_19_21 = prices_19_21.loc['2019':'2021'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data from years 2015-2018\n",
    "df_15_18 = energy.join(weather_by_city)\n",
    "\n",
    "# Combine data from years 2019-2021\n",
    "df_19_21 = gen.join([load_forecast, ws, price_ahead, prices_19_21])\n",
    "\n",
    "# Join data from 2015-2021\n",
    "df_15_21 = gen_forecast.join(border)\n",
    "\n",
    "# Concatenate data from 15-18 with 19-21 and transmission data\n",
    "df = pd.concat([df_15_18, df_19_21], axis=0).join(df_15_21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 5/20/21 - 5/31/21 since missing price data\n",
    "drop = pd.date_range(start='2021-05-20 08', end='2021-05-31 23', freq='H')\n",
    "df.drop(drop, inplace=True)\n",
    "\n",
    "# Impute remaining missing price data\n",
    "indices = df.loc[df['price actual'].isna()].index\n",
    "for i in indices:\n",
    "    df.loc[i, 'price actual'] = impute_immediate_mean(df['price actual'], i)\n",
    "    \n",
    "df['generation total'] = df.loc[:,:'generation wind onshore'].sum(axis=1)\n",
    "df['diff'] = df['total load actual'] - df['generation total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "energy.to_csv('../data/energy_clean.csv')\n",
    "weather_by_city.to_csv('../data/weather_clean.csv')\n",
    "border.to_csv('../data/transmission_clean.csv')\n",
    "gen.to_csv('../data/generation_2019-21.csv')\n",
    "load_forecast.to_csv('../data/load_forecast_2019-21.csv')\n",
    "ws.to_csv('../data/wind_solar_2019-21.csv')\n",
    "price_ahead.to_csv('../data/price_ahead_2019-21.csv')\n",
    "gen_forecast.to_csv('../data/gen_forecast.csv')\n",
    "df.to_csv('../data/df_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
