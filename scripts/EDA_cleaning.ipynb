{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functions import equal, find_nearest, impute_immediate_mean, max_duplicated_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = pd.read_csv('data/energy_dataset.csv')\n",
    "\n",
    "# Chop of nanoseconds, and convert to datetime, reset index\n",
    "energy.time = pd.to_datetime(energy.time.apply(lambda x: x[:-6]))\n",
    "energy.set_index('time', inplace=True)\n",
    "\n",
    "# Get rid of columns that do not contain any information\n",
    "energy = energy.drop(columns = energy.loc[:,energy.nunique()<=1].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create start stop variable for indices\n",
    "start = dt.datetime(2015, 1, 1)\n",
    "stop = dt.datetime(2018, 12, 31, 23)\n",
    "\n",
    "# Create continuous list of indices by hour\n",
    "data = pd.DataFrame(index = pd.date_range(start, stop, freq='H' ))\n",
    "\n",
    "# join existing data on the complete list of indices\n",
    "energy = energy.join(data,how='right')\n",
    "\n",
    "# Loop through each column and impute missing values\n",
    "for col in energy.columns:\n",
    "    \n",
    "    # Get the indices of missing values in this column\n",
    "    indices = energy.loc[energy[col].isna()].index\n",
    "    \n",
    "    # For each missing value, impute the mean of closest known values\n",
    "    for i in indices:\n",
    "        energy.loc[i, col] = impute_immediate_mean(energy[col], i)\n",
    "        \n",
    "# Get indices of duplicates\n",
    "indices = energy.loc[energy.index.value_counts()>1].index.unique()\n",
    "\n",
    "# average duplicate values for each column\n",
    "for col in energy.columns:\n",
    "    for i in indices:\n",
    "        energy.loc[i, col] = round(energy.loc[i,col].mean(),1)\n",
    "        \n",
    "\n",
    "# Drop duplicates\n",
    "energy.drop_duplicates(inplace=True)\n",
    "\n",
    "# Create total generation column summing all generation sources\n",
    "energy['generation total'] = energy.loc[:,:'generation wind onshore'].sum(axis=1)\n",
    "\n",
    "# Create diff column (difference between total generation and actual load)\n",
    "energy['diff'] = energy['generation total'] - energy['total load actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['generation biomass',\n",
    "           'generation fossil brown coal/lignite',\n",
    "           'generation fossil hard coal',\n",
    "           'generation fossil oil',\n",
    "           'generation hydro run-of-river and poundage',\n",
    "           'generation hydro water reservoir',\n",
    "           'generation nuclear',\n",
    "           'generation other',\n",
    "           'generation other renewable',\n",
    "           'generation solar',\n",
    "           'generation waste',\n",
    "           'generation wind onshore',]\n",
    "for col in columns:\n",
    "    flag_indices = energy.loc[energy['diff']<-15000].index\n",
    "    for i in flag_indices:\n",
    "        energy.loc[i, col] = impute_immediate_mean(energy[col], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "energy.to_csv('data/energy_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('data/weather_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "weather.drop_duplicates(inplace=True)\n",
    "\n",
    "# Cut off nanoseconds and create datetime column\n",
    "weather.dt_iso = pd.to_datetime(weather.dt_iso.apply(lambda x: x[:-6]))\n",
    "\n",
    "# Drop columns since weather description provides most granular level of information\n",
    "weather.drop(columns=['weather_id', 'weather_main', 'weather_icon', 'temp_min', 'temp_max'], inplace=True)\n",
    "\n",
    "for city in weather.city_name.unique():\n",
    "    \n",
    "    # Copy data by city\n",
    "    city_df = weather.loc[weather.city_name == city].copy()\n",
    "\n",
    "    # Get the combined weather descriptions for timestamps with multiple descriptions\n",
    "    combined = city_df.groupby('dt_iso')['weather_description'].transform(lambda x : '/'.join(x))\n",
    "\n",
    "    # assign new to combined descriptions\n",
    "    city_df['description'] = combined\n",
    "\n",
    "    # Drop original weather description column\n",
    "    city_df.drop(columns=['city_name','weather_description'], inplace=True)\n",
    "\n",
    "    # Drop duplicates\n",
    "    city_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Change column names\n",
    "    city_df.columns = city_df.columns.map(lambda x: x + f'_{city}')\n",
    "\n",
    "    # join to the data df\n",
    "    data = data.join(city_df.set_index(f'dt_iso_{city}'))\n",
    "\n",
    "# Join energy data with weather data\n",
    "df = energy.join(data)\n",
    "\n",
    "# Replace values in duplicated indices with the max of each column\n",
    "max_duplicated_indices(df, inplace=True)\n",
    "\n",
    "# Fillna with previous (only affects 4 rows)\n",
    "for col in df.columns:\n",
    "    df[col].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "# Get all unique weather descriptions for each city\n",
    "categorical = df.select_dtypes(exclude=[np.float]).columns\n",
    "descriptions = set()\n",
    "for col in categorical:\n",
    "    unique = set(df[col].unique())\n",
    "    descriptions = unique | descriptions\n",
    "descriptions = list(descriptions)\n",
    "\n",
    "# Instantiate LabelEncoder and transform cols\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(descriptions)\n",
    "for col in categorical:\n",
    "    df[col] = encoder.transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "weather.to_csv('data/weather_clean.csv')\n",
    "df.to_csv('data/df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
