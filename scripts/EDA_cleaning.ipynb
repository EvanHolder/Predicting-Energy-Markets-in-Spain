{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functions import equal, find_nearest, impute_immediate_mean, impute_mean_day, daylight_savings_shift, clean_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = pd.read_csv('../data/energy_dataset.csv')\n",
    "\n",
    "# Chop of nanoseconds, and convert to datetime, reset index\n",
    "energy.time = pd.to_datetime(energy.time.apply(lambda x: x[:-6]), )\n",
    "energy.set_index('time', inplace=True)\n",
    "\n",
    "# Get rid of columns that do not contain any information\n",
    "energy = energy.drop(columns = energy.loc[:,energy.nunique()<=1].columns)\n",
    "\n",
    "# Create start stop variable for indices\n",
    "start = dt.datetime(2015, 1, 1)\n",
    "stop = dt.datetime(2018, 12, 31, 23)\n",
    "\n",
    "# Create continuous list of indices by hour\n",
    "yr15_18 = pd.DataFrame(index = pd.date_range(start, stop, freq='H' ))\n",
    "\n",
    "# join existing data on the complete list of indices\n",
    "energy = energy.join(yr15_18,how='right')\n",
    "\n",
    "# Loop through each column and impute missing values\n",
    "for col in energy.columns:\n",
    "    \n",
    "    # Get the indices of missing values in this column\n",
    "    indices = energy.loc[energy[col].isna()].index\n",
    "    \n",
    "    # For each missing value, impute the mean of closest known values\n",
    "    for i in indices:\n",
    "        energy.loc[i, col] = impute_immediate_mean(energy[col], i)\n",
    "        \n",
    "# Get indices of duplicates\n",
    "indices = energy.loc[energy.index.value_counts()>1].index.unique()\n",
    "\n",
    "# average duplicate values for each column\n",
    "for col in energy.columns:\n",
    "    for i in indices:\n",
    "        energy.loc[i, col] = round(energy.loc[i,col].mean(),1)\n",
    "        \n",
    "\n",
    "# Drop duplicates\n",
    "energy.drop_duplicates(inplace=True)\n",
    "\n",
    "# Create total generation column summing all generation sources\n",
    "energy['generation total'] = energy.loc[:,:'generation wind onshore'].sum(axis=1)\n",
    "\n",
    "# Create diff column (difference between total generation and actual load)\n",
    "energy['diff'] = energy['generation total'] - energy['total load actual']\n",
    "\n",
    "columns = ['generation biomass',\n",
    "           'generation fossil brown coal/lignite',\n",
    "           'generation fossil hard coal',\n",
    "           'generation fossil oil',\n",
    "           'generation hydro run-of-river and poundage',\n",
    "           'generation hydro water reservoir',\n",
    "           'generation nuclear',\n",
    "           'generation other',\n",
    "           'generation other renewable',\n",
    "           'generation solar',\n",
    "           'generation waste',\n",
    "           'generation wind onshore',]\n",
    "for col in columns:\n",
    "    flag_indices = energy.loc[energy['diff']<-15000].index\n",
    "    for i in flag_indices:\n",
    "        energy.loc[i, col] = impute_immediate_mean(energy[col], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Shift cols\n",
    "shift_cols = ['forecast solar day ahead', \n",
    "              'forecast wind onshore day ahead', \n",
    "              'total load forecast', \n",
    "              'price day ahead', \n",
    "              'price actual']\n",
    "\n",
    "# Copy actual price to new series\n",
    "tomorrow = energy[shift_cols].copy()\n",
    "\n",
    "# Shift the actual price index by a single day backwards\n",
    "tomorrow = tomorrow.shift(-1, freq='D')\n",
    "\n",
    "# Copy the df and merge tomorrows price into each row\n",
    "energy = energy.join(tomorrow, how='inner', rsuffix='_tomorrow')\n",
    "\n",
    "energy.rename(columns={'forecast solar day ahead_tomorrow':'solar_forecast_tomorrow',\n",
    "                       'forecast wind onshore day ahead_tomorrow': 'wind_forecast_tomorrow',\n",
    "                       'total load forecast_tomorrow': 'load_forecast_tomorrow',\n",
    "                       'price day ahead_tomorrow':'price_forecast_tomorrow',\n",
    "                       'price actual_tomorrow': 'price_tomorrow'},\n",
    "              inplace=True)\n",
    "\n",
    "# Remove price actual from shift_cols\n",
    "shift_cols.remove('price actual')\n",
    "\n",
    "# Remove shifted cols and rename price actual\n",
    "energy.drop(columns=shift_cols, inplace=True)\n",
    "energy.rename(columns={'price actual': 'price_actual'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['madrid', 'seville', 'barcelona', 'bilbao', 'valencia']\n",
    "dfs = []\n",
    "for city in cities:\n",
    "    \n",
    "    # Read in city dataframe\n",
    "    city_df = pd.read_csv(f'../data/weather/{city}.csv', index_col=0)\n",
    "\n",
    "    # Clean the madrid data\n",
    "    city_df = clean_weather(city_df)\n",
    "\n",
    "    # Rename columns\n",
    "    city_df.columns = city_df.columns + f'_{city}'\n",
    "    \n",
    "    dfs.append(city_df)\n",
    "    \n",
    "# Create daterange\n",
    "index = pd.date_range(start=dt.datetime(2015,1,1),\n",
    "                      end = dt.datetime(2021,12,31, 23),\n",
    "                      freq='H')\n",
    "range_ = pd.DataFrame(index=index)\n",
    "\n",
    "# Join to date range in weather data\n",
    "weather = range_.join([dfs[0], dfs[1], dfs[2], dfs[3], dfs[4]])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "weather.drop(columns=['index_madrid', 'index_seville', 'index_barcelona',\n",
    "                      'index_bilbao', 'index_valencia'],\n",
    "             inplace=True)\n",
    "\n",
    "# Get list of categoricals and continuous variables\n",
    "categorical = weather.select_dtypes(include='object').columns\n",
    "continuous = weather.select_dtypes(exclude='object').columns\n",
    "\n",
    "# Impute the mean day for continuous variables with 12 or more missing\n",
    "for col in continuous:\n",
    "    impute_mean_day(weather, col, 12)\n",
    "\n",
    "# Interpolate remaining continuous Nans\n",
    "weather.loc[:, continuous] = weather.loc[:, continuous].interpolate(limit=12)\n",
    "\n",
    "# Back fill categorical nans\n",
    "weather.loc[:,categorical] = weather[categorical].fillna(value='unknown')\n",
    "\n",
    "# Back fill remaining nans (first five rows of dataset)\n",
    "weather = weather.fillna(method='bfill')\n",
    "\n",
    "# Drop duplicates in the index\n",
    "weather.reset_index(inplace=True)\n",
    "weather.drop_duplicates(subset='index', inplace=True)\n",
    "weather.set_index('index', inplace=True)\n",
    "\n",
    "# Drop precips cols\n",
    "weather.drop(columns = weather.filter(regex='precips').columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Border Transmission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in France Border Data\n",
    "france = pd.DataFrame()\n",
    "portugal = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir('../data/cross_border/france'):\n",
    "    data = pd.read_csv(f'../data/cross_border/france/{file}')\n",
    "    data.drop_duplicates(subset='Time (CET)', inplace=True)\n",
    "    france = pd.concat([france, data],axis=0)\n",
    "\n",
    "# Convert Time to datetime\n",
    "france['time'] = france['Time (CET)'].apply(lambda x: x[:16])\n",
    "france['time'] = pd.to_datetime(france['time'])\n",
    "france = france.set_index('time').drop(columns='Time (CET)')\n",
    "\n",
    "# Read in Portugal Border Data\n",
    "for file in os.listdir('../data/cross_border/portugal'):\n",
    "    data = pd.read_csv(f'../data/cross_border/portugal/{file}')\n",
    "    data.drop_duplicates(subset='Time (CET)', inplace=True)\n",
    "    portugal = pd.concat([portugal, data],axis=0)\n",
    "portugal['time'] = portugal['Time (CET)'].apply(lambda x: x[:16])\n",
    "portugal['time'] = pd.to_datetime(portugal['time'])\n",
    "portugal = portugal.set_index('time').drop(columns='Time (CET)')\n",
    "\n",
    "# Join France and Portugal Data\n",
    "border = portugal.join(france)\n",
    "border.fillna(method='ffill', inplace=True)\n",
    "cols = dict(zip(border.columns,['transmission_ps', \n",
    "                                'transmission_sp', \n",
    "                                'transmission_fs',\n",
    "                                'transmission_sf']))\n",
    "\n",
    "border.rename(columns=cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pd.DataFrame()\n",
    "for file in os.listdir('../data/generation'):\n",
    "    load = pd.read_csv(f'../data/generation/{file}')\n",
    "    load.drop_duplicates(subset='MTU', inplace=True)\n",
    "    gen = pd.concat([gen, load], axis=0)\n",
    "    \n",
    "# Get rid of columns that do not contain any information\n",
    "gen = gen.drop(columns = gen.loc[:,gen.nunique()<=1].columns)\n",
    "\n",
    "# Convert Time to datetime\n",
    "gen['time'] = pd.to_datetime(gen.MTU.apply(lambda x: x[:13]))\n",
    "\n",
    "# Set index to the time col\n",
    "gen.set_index('time', inplace=True)\n",
    "\n",
    "# Drop MTU col\n",
    "gen.drop(columns='MTU', inplace=True)\n",
    "\n",
    "# Rename cols\n",
    "gen.columns = gen.columns.map(lambda x: ('generation '+ x[:-26]).lower())\n",
    "gen.rename(columns={'generation hydro pumped storage ': 'generation hydro pumped storage consumption'},\n",
    "           inplace=True)\n",
    "\n",
    "# Impute Immediate Mean for NaNs\n",
    "for col in gen.columns:\n",
    "    indices = gen.loc[gen[col].isna()].index\n",
    "    for i in indices:\n",
    "        gen.loc[i,col] = impute_immediate_mean(gen[col], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Forecast and Actual (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_forecast = pd.DataFrame()\n",
    "for file in os.listdir('../data/load'):\n",
    "    load = pd.read_csv(f'../data/load/{file}')\n",
    "    load.drop_duplicates(subset='Time (CET)', inplace=True)\n",
    "    load_forecast = pd.concat([load_forecast, load], axis=0)\n",
    "\n",
    "# Convert Time to datetime\n",
    "load_forecast['time'] = pd.to_datetime(load_forecast['Time (CET)'].apply(lambda x: x[:13]))\n",
    "\n",
    "# Set index to the time col\n",
    "load_forecast.set_index('time', inplace=True)\n",
    "\n",
    "# Drop 'Time (CET)' col\n",
    "load_forecast.drop(columns='Time (CET)', inplace=True)\n",
    "\n",
    "# Rename cols\n",
    "load_forecast.rename(columns = {'Day-ahead Total Load Forecast [MW] - BZN|ES':'total load forecast',\n",
    "                                'Actual Total Load [MW] - BZN|ES':'total load actual'}, \n",
    "                     inplace=True)\n",
    "\n",
    "# Impute Immediate Mean for NaNs\n",
    "for col in load_forecast.columns:\n",
    "    indices = load_forecast.loc[load_forecast[col].isna()].index\n",
    "    for i in indices:\n",
    "        load_forecast.loc[i,col] = impute_immediate_mean(load_forecast[col], i)\n",
    "        \n",
    "# Copy actual price to new series\n",
    "tomorrow = load_forecast['total load forecast'].copy()\n",
    "\n",
    "# Shift the actual price index by a single day backwards\n",
    "tomorrow = tomorrow.shift(-1, freq='D')\n",
    "\n",
    "# Copy the df and merge tomorrows price into each row\n",
    "load_forecast = load_forecast.join(tomorrow, how='inner', rsuffix='_tomorrow')\n",
    "load_forecast.rename(columns={'total load forecast_tomorrow':'load_forecast_tomorrow'}, inplace=True)\n",
    "load_forecast.drop(columns = 'total load forecast', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind and Solar Forecast (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = pd.DataFrame()\n",
    "for file in os.listdir('../data/wind_solar_day_ahead'):\n",
    "    load = pd.read_csv(f'../data/wind_solar_day_ahead/{file}')\n",
    "    load.drop_duplicates(subset='MTU (CET)', inplace=True)\n",
    "    ws = pd.concat([ws, load], axis=0)\n",
    "    \n",
    "# Convert Time to datetime\n",
    "ws['time'] = pd.to_datetime(ws['MTU (CET)'].apply(lambda x: x[:13]))\n",
    "\n",
    "# Set index to the time col\n",
    "ws.set_index('time', inplace=True)\n",
    "\n",
    "# Drop 'Time (CET)' col\n",
    "ws = ws[['Generation - Solar  [MW] Day Ahead/ BZN|ES',\n",
    "         'Generation - Wind Onshore  [MW] Day Ahead/ BZN|ES']].copy()\n",
    "\n",
    "# Rename cols\n",
    "ws.rename(columns = {'Generation - Solar  [MW] Day Ahead/ BZN|ES':'forecast solar day ahead',\n",
    "                     'Generation - Wind Onshore  [MW] Day Ahead/ BZN|ES':'forecast wind onshore day ahead'}, \n",
    "                     inplace=True)\n",
    "\n",
    "# 2020-05-01 wind forecast is missing, impute average for that day in may\n",
    "avg_w = ws['forecast wind onshore day ahead'].groupby(by=[ws.index.month, \n",
    "                                                          ws.index.day,\n",
    "                                                          ws.index.hour]).mean().loc[(5,1)]\n",
    "for i, time in enumerate(ws.loc['2020-05-01'].index):\n",
    "    ws.loc[time, 'forecast wind onshore day ahead'] = avg_w[i]\n",
    "    \n",
    "# Impute the immediate mean for remaining NaNs\n",
    "for col in ws.columns:\n",
    "    indices = ws.loc[ws[col].isna()].index\n",
    "    for i in indices:\n",
    "        ws.loc[i,col] = impute_immediate_mean(ws[col], i)\n",
    "        \n",
    "# Copy actual price to new series\n",
    "tomorrow = ws.copy()\n",
    "\n",
    "# Shift the actual price index by a single day backwards\n",
    "tomorrow = tomorrow.shift(-1, freq='D')\n",
    "\n",
    "# Copy the df and merge tomorrows price into each row\n",
    "ws = ws.join(tomorrow, how='inner', rsuffix='_tomorrow')\n",
    "ws.rename(columns={'forecast solar day ahead_tomorrow':'solar_forecast_tomorrow',\n",
    "                   'forecast wind onshore day ahead_tomorrow':'wind_forecast_tomorrow'},\n",
    "          inplace=True)\n",
    "ws.drop(columns = ['forecast solar day ahead', \n",
    "                   'forecast wind onshore day ahead'], \n",
    "        inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day Ahead Prices (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine all day ahead price data into single dataframe\n",
    "price_ahead = pd.DataFrame()\n",
    "for file in os.listdir('../data/day ahead price'):\n",
    "    load = pd.read_csv(f'../data/day ahead price/{file}')\n",
    "    load.drop_duplicates(subset='MTU (CET)', inplace=True)\n",
    "    price_ahead = pd.concat([price_ahead, load], axis=0)\n",
    "    \n",
    "# Convert Time to datetime\n",
    "price_ahead['time'] = price_ahead['MTU (CET)'].apply(lambda x: x[:16])\n",
    "price_ahead['time'] = pd.to_datetime(price_ahead['time'])\n",
    "\n",
    "# Drop unused columns and rows\n",
    "price_ahead.drop(columns=['MTU (CET)', 'BZN|ES'], inplace=True)\n",
    "\n",
    "# Set index to the time col\n",
    "price_ahead.set_index('time', inplace=True)\n",
    "\n",
    "# Drop all data in 2022 since incomplete\n",
    "price_ahead = price_ahead.loc[:'2021'].copy()\n",
    "\n",
    "# Rename col\n",
    "price_ahead.rename(columns={'Day-ahead Price [EUR/MWh]':'price day ahead'}, inplace=True)\n",
    "\n",
    "# Impute the immediate mean for remaining NaNs\n",
    "for col in price_ahead.columns:\n",
    "    indices = price_ahead.loc[price_ahead[col].isna()].index\n",
    "    for i in indices:\n",
    "        price_ahead.loc[i,col] = impute_immediate_mean(price_ahead[col], i)\n",
    "        \n",
    "# Shift the actual price index by a single day backwards\n",
    "price_ahead = price_ahead.shift(-1, freq='D')\n",
    "\n",
    "# Rename to price_forecast_tomorrow\n",
    "price_ahead.rename(columns={'price day ahead':'price_forecast_tomorrow'},\n",
    "                   inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Forecast Day ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and combine all day ahead price data into single dataframe\n",
    "gen_forecast = pd.DataFrame()\n",
    "for file in os.listdir('../data/gen_forecast'):\n",
    "    load = pd.read_csv(f'../data/gen_forecast/{file}')\n",
    "    load.drop_duplicates(subset='MTU', inplace=True)\n",
    "    gen_forecast = pd.concat([gen_forecast, load], axis=0)\n",
    "    \n",
    "# Convert Time to datetime\n",
    "gen_forecast['time'] = gen_forecast['MTU'].apply(lambda x: x[:16])\n",
    "gen_forecast['time'] = pd.to_datetime(gen_forecast['time'])\n",
    "\n",
    "# Drop unused columns and rows\n",
    "gen_forecast.drop(columns='MTU', inplace=True)\n",
    "\n",
    "# Set index to the time col\n",
    "gen_forecast.set_index('time', inplace=True)\n",
    "\n",
    "\n",
    "# Rename cols\n",
    "gen_forecast.rename(columns={'Scheduled Generation [MW] (D) - BZN|ES':'generation_scheduled',\n",
    "                             'Scheduled Consumption [MW] (D) - BZN|ES':'consumption_scheduled'},\n",
    "                    inplace=True)\n",
    "\n",
    "for col in gen_forecast.columns:\n",
    "    \n",
    "    # Impute the mean of the nearest known date by hour\n",
    "    impute_mean_day(gen_forecast, col, 24)\n",
    "    \n",
    "    # Fill remaining Nans\n",
    "    gen_forecast[col].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "# Shift index by a single day backwards\n",
    "gen_forecast = gen_forecast.shift(-1, freq='D')\n",
    "\n",
    "# Rename to price_forecast_tomorrow\n",
    "gen_forecast.rename(columns={'generation_scheduled':'generation_forecast',\n",
    "                             'consumption_scheduled': 'consumption_forecast'},\n",
    "                    inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prices (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num missing cols: 0\n"
     ]
    }
   ],
   "source": [
    "# Change directory and get all files in data/price directory\n",
    "os.chdir('../data/price')\n",
    "files = os.listdir()\n",
    "\n",
    "# Read all files in directory\n",
    "prices = pd.DataFrame()\n",
    "for file in files:\n",
    "    prices =pd.concat([prices, pd.read_csv(file, delimiter=';', index_col=5, parse_dates=True)])\n",
    "\n",
    "# Subset the data to only the final price\n",
    "prices = prices.loc[prices.name=='Hourly average price final sum of components'].copy()\n",
    "\n",
    "# Sort by the date\n",
    "prices.sort_index(inplace=True)\n",
    "\n",
    "# Drop unnecessary cols\n",
    "prices.drop(columns=['id', 'name', 'geoid', 'geoname'], inplace=True)\n",
    "\n",
    "# Join on date_range to make sure we aren't missing any rows\n",
    "dates = pd.DataFrame(\n",
    "    index=pd.date_range(start=dt.datetime(2019,1,1),\n",
    "                        end=dt.datetime(2021,12,31,23),\n",
    "                        freq='H',\n",
    "                        tz='CET')\n",
    ")\n",
    "prices = dates.join(prices)\n",
    "print('num missing cols:', prices.isna().sum().sum())\n",
    "\n",
    "# Make timezone unaware, shift by an hour to offset the unaware tz\n",
    "prices.index = prices.index.tz_convert(None)\n",
    "prices = prices.shift(1, freq='H')\n",
    "\n",
    "# Rename col to price_actual\n",
    "prices.rename(columns={'value':'price_actual'}, inplace=True)\n",
    "\n",
    "#Shift prices and drop\n",
    "prices_tomorrow = prices.shift(-1, freq='D')\n",
    "prices_tomorrow.rename(columns={'price_actual':'price_tomorrow'}, inplace=True)\n",
    "\n",
    "# Join price_actual with price_tomorrow\n",
    "prices = prices.join(prices_tomorrow)\n",
    "\n",
    "\n",
    "# Change directory back to ../script\n",
    "os.chdir('../../scripts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine data from years 2019-2021\n",
    "df_19_21 = gen.join([load_forecast, ws, price_ahead, prices])\n",
    "\n",
    "# Join data from 2015-2021\n",
    "df_15_21 = gen_forecast.join([border, weather])\n",
    "\n",
    "# Concatenate data from 15-18 with 19-21 and transmission data\n",
    "df = pd.concat([energy, df_19_21]).join(df_15_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 2021-12-31\n",
    "df = df.loc[:'2021-12-30'].copy()\n",
    "\n",
    "# Create generation total, diff columns\n",
    "df['generation total'] = df.loc[:,:'generation wind onshore'].sum(axis=1)\n",
    "df['diff'] = df['total load actual'] - df['generation total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([], )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby(by=[df.index.year, df.index.month, df.index.day]).count()\n",
    "grouped.loc[grouped['generation biomass']<24].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='price_tomorrow'), df['price_tomorrow'], test_size=.3,\n",
    "                                                    random_state=17)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=.5, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "energy.to_csv('../data/energy_clean.csv')\n",
    "weather.to_csv('../data/weather/weather_clean.csv')\n",
    "border.to_csv('../data/transmission_clean.csv')\n",
    "gen.to_csv('../data/generation_2019-21.csv')\n",
    "load_forecast.to_csv('../data/load_forecast_2019-21.csv')\n",
    "ws.to_csv('../data/wind_solar_2019-21.csv')\n",
    "price_ahead.to_csv('../data/price_ahead_2019-21.csv')\n",
    "gen_forecast.to_csv('../data/gen_forecast.csv')\n",
    "df.to_csv('../data/df_clean.csv')\n",
    "X_train.to_csv('../data/X_train.csv')\n",
    "X_test.to_csv('../data/X_test.csv')\n",
    "X_val.to_csv('../data/X_val.csv')\n",
    "y_train.to_csv('../data/y_train.csv')\n",
    "y_test.to_csv('../data/y_test.csv')\n",
    "y_val.to_csv('../data/y_val.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
