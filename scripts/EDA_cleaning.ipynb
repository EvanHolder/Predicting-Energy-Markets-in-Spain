{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functions import equal, find_nearest, impute_immediate_mean, impute_mean_day, daylight_savings_shift, clean_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = pd.read_csv('../data/energy_dataset.csv')\n",
    "\n",
    "# Chop of nanoseconds, and convert to datetime, reset index\n",
    "energy.time = pd.to_datetime(energy.time.apply(lambda x: x[:-6]))\n",
    "energy.set_index('time', inplace=True)\n",
    "\n",
    "# Get rid of columns that do not contain any information\n",
    "energy = energy.drop(columns = energy.loc[:,energy.nunique()<=1].columns)\n",
    "\n",
    "# Create start stop variable for indices\n",
    "start = dt.datetime(2015, 1, 1)\n",
    "stop = dt.datetime(2018, 12, 31, 23)\n",
    "\n",
    "# Create continuous list of indices by hour\n",
    "yr15_18 = pd.DataFrame(index = pd.date_range(start, stop, freq='H' ))\n",
    "\n",
    "# join existing data on the complete list of indices\n",
    "energy = energy.join(yr15_18,how='right')\n",
    "\n",
    "# Loop through each column and impute missing values\n",
    "for col in energy.columns:\n",
    "    \n",
    "    # Get the indices of missing values in this column\n",
    "    indices = energy.loc[energy[col].isna()].index\n",
    "    \n",
    "    # For each missing value, impute the mean of closest known values\n",
    "    for i in indices:\n",
    "        energy.loc[i, col] = impute_immediate_mean(energy[col], i)\n",
    "        \n",
    "# Get indices of duplicates\n",
    "indices = energy.loc[energy.index.value_counts()>1].index.unique()\n",
    "\n",
    "# average duplicate values for each column\n",
    "for col in energy.columns:\n",
    "    for i in indices:\n",
    "        energy.loc[i, col] = round(energy.loc[i,col].mean(),1)\n",
    "        \n",
    "\n",
    "# Drop duplicates\n",
    "energy.drop_duplicates(inplace=True)\n",
    "\n",
    "# Create total generation column summing all generation sources\n",
    "energy['generation total'] = energy.loc[:,:'generation wind onshore'].sum(axis=1)\n",
    "\n",
    "# Create diff column (difference between total generation and actual load)\n",
    "energy['diff'] = energy['generation total'] - energy['total load actual']\n",
    "\n",
    "columns = ['generation biomass',\n",
    "           'generation fossil brown coal/lignite',\n",
    "           'generation fossil hard coal',\n",
    "           'generation fossil oil',\n",
    "           'generation hydro run-of-river and poundage',\n",
    "           'generation hydro water reservoir',\n",
    "           'generation nuclear',\n",
    "           'generation other',\n",
    "           'generation other renewable',\n",
    "           'generation solar',\n",
    "           'generation waste',\n",
    "           'generation wind onshore',]\n",
    "for col in columns:\n",
    "    flag_indices = energy.loc[energy['diff']<-15000].index\n",
    "    for i in flag_indices:\n",
    "        energy.loc[i, col] = impute_immediate_mean(energy[col], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['madrid', 'seville', 'barcelona', 'bilbao', 'valencia']\n",
    "dfs = []\n",
    "for city in cities:\n",
    "    \n",
    "    # Read in city dataframe\n",
    "    city_df = pd.read_csv(f'../data/weather/{city}.csv', index_col=0)\n",
    "\n",
    "    # Clean the madrid data\n",
    "    city_df = clean_weather(city_df)\n",
    "\n",
    "    # Rename columns\n",
    "    city_df.columns = city_df.columns + f'_{city}'\n",
    "    \n",
    "    dfs.append(city_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create daterange\n",
    "index = pd.date_range(start=dt.datetime(2015,1,1),\n",
    "                      end = dt.datetime(2021,12,31, 23),\n",
    "                      freq='H')\n",
    "range_ = pd.DataFrame(index=index)\n",
    "\n",
    "# Join to date range in weather data\n",
    "weather = range_.join([dfs[0], dfs[1], dfs[2], dfs[3], dfs[4]])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "weather.drop(columns=['index_madrid', 'index_seville', 'index_barcelona',\n",
    "                      'index_bilbao', 'index_valencia'],\n",
    "             inplace=True)\n",
    "\n",
    "# Get list of categoricals and continuous variables\n",
    "categorical = weather.select_dtypes(include='object').columns\n",
    "continuous = weather.select_dtypes(exclude='object').columns\n",
    "\n",
    "# Impute the mean day for continuous variables with 12 or more missing\n",
    "for col in continuous:\n",
    "    impute_mean_day(weather, col, 12)\n",
    "\n",
    "# Interpolate remaining continuous Nans\n",
    "weather.loc[:, continuous] = weather.loc[:, continuous].interpolate(limit=12)\n",
    "\n",
    "# Back fill categorical nans\n",
    "weather.loc[:,categorical] = weather[categorical].fillna(value='unknown')\n",
    "\n",
    "# Back fill remaining nans (first five rows of dataset)\n",
    "weather = weather.fillna(method='bfill')\n",
    "\n",
    "# Drop duplicates in the index\n",
    "weather.reset_index(inplace=True)\n",
    "weather.drop_duplicates(subset='index', inplace=True)\n",
    "weather.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv('../data/weather_features.csv')\n",
    "\n",
    "# Drop duplicates\n",
    "weather.drop_duplicates(inplace=True)\n",
    "\n",
    "# Cut off nanoseconds and create datetime column\n",
    "weather.dt_iso = pd.to_datetime(weather.dt_iso.apply(lambda x: x[:-6]))\n",
    "\n",
    "# Drop columns since weather description provides most granular level of information\n",
    "weather.drop(columns=['weather_id', 'weather_main', 'weather_icon', 'temp_min', 'temp_max'], inplace=True)\n",
    "\n",
    "# Create weather by city df\n",
    "weather_by_city = pd.DataFrame()\n",
    "\n",
    "for city in weather.city_name.unique():\n",
    "    \n",
    "    # Copy data by city\n",
    "    city_df = weather.loc[weather.city_name == city].copy()\n",
    "\n",
    "    # Get the combined weather descriptions for timestamps with multiple descriptions\n",
    "    combined = city_df.groupby('dt_iso')['weather_description'].transform(lambda x : '/'.join(x))\n",
    "\n",
    "    # assign new to combined descriptions\n",
    "    city_df['description'] = combined\n",
    "\n",
    "    # Drop original weather description column\n",
    "    city_df.drop(columns=['city_name','weather_description'], inplace=True)\n",
    "\n",
    "    # Drop duplicates\n",
    "    city_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Set index\n",
    "    city_df = city_df.set_index('dt_iso')\n",
    "    \n",
    "    # Rename cols\n",
    "    city_df.columns = city_df.columns.map(lambda x: x + f'_{city}')\n",
    "    \n",
    "    # Join to the weather_cities df\n",
    "    weather_by_city = city_df.join(weather_by_city, how='left')\n",
    "\n",
    "# Replace values in duplicated indices with the max\n",
    "weather_by_city = weather_by_city.groupby(by=weather_by_city.index).max()\n",
    "\n",
    "# join existing data on the complete list of indices\n",
    "weather_by_city = weather_by_city.join(yr15_18,how='right')\n",
    "weather_by_city\n",
    "\n",
    "# Impute the immediate mean for all missing values in continuous cols\n",
    "continuous = weather_by_city.select_dtypes(exclude='object').columns\n",
    "for col in continuous:\n",
    "    indices = weather_by_city.loc[weather_by_city[col].isna()].index\n",
    "    for i in indices:\n",
    "        weather_by_city.loc[i,col] = impute_immediate_mean(weather_by_city[col], i)\n",
    "\n",
    "# Impute the previous known col for remaining cols\n",
    "weather_by_city.fillna(method='bfill', inplace=True)\n",
    "    \n",
    "# Get all unique weather descriptions for each city\n",
    "categorical = weather_by_city.select_dtypes(include='object').columns\n",
    "descriptions = set()\n",
    "for col in categorical:\n",
    "    unique = set(weather_by_city[col].unique())\n",
    "    descriptions = unique | descriptions\n",
    "descriptions = list(descriptions)\n",
    "\n",
    "# Instantiate LabelEncoder and transform cols\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(descriptions)\n",
    "for col in categorical:\n",
    "    weather_by_city[col] = encoder.transform(weather_by_city[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Border Transmission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in France Border Data\n",
    "france = pd.DataFrame()\n",
    "portugal = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir('../data/cross_border/france'):\n",
    "    data = pd.read_csv(f'../data/cross_border/france/{file}')\n",
    "    data.drop_duplicates(subset='Time (CET)', inplace=True)\n",
    "    france = pd.concat([france, data],axis=0)\n",
    "\n",
    "# Convert Time to datetime\n",
    "france['time'] = france['Time (CET)'].apply(lambda x: x[:16])\n",
    "france['time'] = pd.to_datetime(france['time'])\n",
    "france = france.set_index('time').drop(columns='Time (CET)')\n",
    "\n",
    "# Read in Portugal Border Data\n",
    "for file in os.listdir('../data/cross_border/portugal'):\n",
    "    data = pd.read_csv(f'../data/cross_border/portugal/{file}')\n",
    "    data.drop_duplicates(subset='Time (CET)', inplace=True)\n",
    "    portugal = pd.concat([portugal, data],axis=0)\n",
    "portugal['time'] = portugal['Time (CET)'].apply(lambda x: x[:16])\n",
    "portugal['time'] = pd.to_datetime(portugal['time'])\n",
    "portugal = portugal.set_index('time').drop(columns='Time (CET)')\n",
    "\n",
    "# Join France and Portugal Data\n",
    "border = portugal.join(france)\n",
    "border.fillna(method='ffill', inplace=True)\n",
    "cols = dict(zip(border.columns,['transmission_ps', \n",
    "                                'transmission_sp', \n",
    "                                'transmission_fs',\n",
    "                                'transmission_sf']))\n",
    "\n",
    "border.rename(columns=cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pd.DataFrame()\n",
    "for file in os.listdir('../data/generation'):\n",
    "    load = pd.read_csv(f'../data/generation/{file}')\n",
    "    load.drop_duplicates(subset='MTU', inplace=True)\n",
    "    gen = pd.concat([gen, load], axis=0)\n",
    "    \n",
    "# Get rid of columns that do not contain any information\n",
    "gen = gen.drop(columns = gen.loc[:,gen.nunique()<=1].columns)\n",
    "\n",
    "# Convert Time to datetime\n",
    "gen['time'] = pd.to_datetime(gen.MTU.apply(lambda x: x[:13]))\n",
    "\n",
    "# Set index to the time col\n",
    "gen.set_index('time', inplace=True)\n",
    "\n",
    "# Drop MTU col\n",
    "gen.drop(columns='MTU', inplace=True)\n",
    "\n",
    "# Rename cols\n",
    "gen.columns = gen.columns.map(lambda x: ('generation '+ x[:-26]).lower())\n",
    "gen.rename(columns={'generation hydro pumped storage ': 'generation hydro pumped storage consumption'},\n",
    "           inplace=True)\n",
    "\n",
    "# Impute Immediate Mean for NaNs\n",
    "for col in gen.columns:\n",
    "    indices = gen.loc[gen[col].isna()].index\n",
    "    for i in indices:\n",
    "        gen.loc[i,col] = impute_immediate_mean(gen[col], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Forecast and Actual (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_forecast = pd.DataFrame()\n",
    "for file in os.listdir('../data/load'):\n",
    "    load = pd.read_csv(f'../data/load/{file}')\n",
    "    load.drop_duplicates(subset='Time (CET)', inplace=True)\n",
    "    load_forecast = pd.concat([load_forecast, load], axis=0)\n",
    "\n",
    "# Convert Time to datetime\n",
    "load_forecast['time'] = pd.to_datetime(load_forecast['Time (CET)'].apply(lambda x: x[:13]))\n",
    "\n",
    "# Set index to the time col\n",
    "load_forecast.set_index('time', inplace=True)\n",
    "\n",
    "# Drop 'Time (CET)' col\n",
    "load_forecast.drop(columns='Time (CET)', inplace=True)\n",
    "\n",
    "# Rename cols\n",
    "load_forecast.rename(columns = {'Day-ahead Total Load Forecast [MW] - BZN|ES':'total load forecast',\n",
    "                                'Actual Total Load [MW] - BZN|ES':'total load actual'}, \n",
    "                     inplace=True)\n",
    "\n",
    "# Impute Immediate Mean for NaNs\n",
    "for col in load_forecast.columns:\n",
    "    indices = load_forecast.loc[load_forecast[col].isna()].index\n",
    "    for i in indices:\n",
    "        load_forecast.loc[i,col] = impute_immediate_mean(load_forecast[col], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind and Solar Forecast (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = pd.DataFrame()\n",
    "for file in os.listdir('../data/wind_solar_day_ahead'):\n",
    "    load = pd.read_csv(f'../data/wind_solar_day_ahead/{file}')\n",
    "    load.drop_duplicates(subset='MTU (CET)', inplace=True)\n",
    "    ws = pd.concat([ws, load], axis=0)\n",
    "    \n",
    "# Convert Time to datetime\n",
    "ws['time'] = pd.to_datetime(ws['MTU (CET)'].apply(lambda x: x[:13]))\n",
    "\n",
    "# Set index to the time col\n",
    "ws.set_index('time', inplace=True)\n",
    "\n",
    "# Drop 'Time (CET)' col\n",
    "ws = ws[['Generation - Solar  [MW] Day Ahead/ BZN|ES',\n",
    "         'Generation - Wind Onshore  [MW] Day Ahead/ BZN|ES']].copy()\n",
    "\n",
    "# Rename cols\n",
    "ws.rename(columns = {'Generation - Solar  [MW] Day Ahead/ BZN|ES':'forecast solar day ahead',\n",
    "                     'Generation - Wind Onshore  [MW] Day Ahead/ BZN|ES':'forecast wind onshore day ahead'}, \n",
    "                     inplace=True)\n",
    "\n",
    "# 2020-05-01 wind forecast is missing, impute average for that day in may\n",
    "avg_w = ws['forecast wind onshore day ahead'].groupby(by=[ws.index.month, \n",
    "                                                          ws.index.day,\n",
    "                                                          ws.index.hour]).mean().loc[(5,1)]\n",
    "for i, time in enumerate(ws.loc['2020-05-01'].index):\n",
    "    ws.loc[time, 'forecast wind onshore day ahead'] = avg_w[i]\n",
    "    \n",
    "# Impute the immediate mean for remaining NaNs\n",
    "for col in ws.columns:\n",
    "    indices = ws.loc[ws[col].isna()].index\n",
    "    for i in indices:\n",
    "        ws.loc[i,col] = impute_immediate_mean(ws[col], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day Ahead Prices (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine all day ahead price data into single dataframe\n",
    "price_ahead = pd.DataFrame()\n",
    "for file in os.listdir('../data/day ahead price'):\n",
    "    load = pd.read_csv(f'../data/day ahead price/{file}')\n",
    "    load.drop_duplicates(subset='MTU (CET)', inplace=True)\n",
    "    price_ahead = pd.concat([price_ahead, load], axis=0)\n",
    "    \n",
    "# Convert Time to datetime\n",
    "price_ahead['time'] = price_ahead['MTU (CET)'].apply(lambda x: x[:16])\n",
    "price_ahead['time'] = pd.to_datetime(price_ahead['time'])\n",
    "\n",
    "# Drop unused columns and rows\n",
    "price_ahead.drop(columns=['MTU (CET)', 'BZN|ES'], inplace=True)\n",
    "\n",
    "# Set index to the time col\n",
    "price_ahead.set_index('time', inplace=True)\n",
    "\n",
    "# Drop all data in 2022 since incomplete\n",
    "price_ahead = price_ahead.loc[:'2021'].copy()\n",
    "\n",
    "# Rename col\n",
    "price_ahead.rename(columns={'Day-ahead Price [EUR/MWh]':'price day ahead'}, inplace=True)\n",
    "\n",
    "# Impute the immediate mean for remaining NaNs\n",
    "for col in price_ahead.columns:\n",
    "    indices = price_ahead.loc[price_ahead[col].isna()].index\n",
    "    for i in indices:\n",
    "        price_ahead.loc[i,col] = impute_immediate_mean(price_ahead[col], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Forecast Day ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and combine all day ahead price data into single dataframe\n",
    "gen_forecast = pd.DataFrame()\n",
    "for file in os.listdir('../data/gen_forecast'):\n",
    "    load = pd.read_csv(f'../data/gen_forecast/{file}')\n",
    "    load.drop_duplicates(subset='MTU', inplace=True)\n",
    "    gen_forecast = pd.concat([gen_forecast, load], axis=0)\n",
    "    \n",
    "# Convert Time to datetime\n",
    "gen_forecast['time'] = gen_forecast['MTU'].apply(lambda x: x[:16])\n",
    "gen_forecast['time'] = pd.to_datetime(gen_forecast['time'])\n",
    "\n",
    "# Drop unused columns and rows\n",
    "gen_forecast.drop(columns='MTU', inplace=True)\n",
    "\n",
    "# Set index to the time col\n",
    "gen_forecast.set_index('time', inplace=True)\n",
    "\n",
    "\n",
    "# Rename cols\n",
    "gen_forecast.rename(columns={'Scheduled Generation [MW] (D) - BZN|ES':'generation_scheduled',\n",
    "                             'Scheduled Consumption [MW] (D) - BZN|ES':'consumption_scheduled'},\n",
    "                    inplace=True)\n",
    "\n",
    "for col in gen_forecast.columns:\n",
    "    \n",
    "    # Impute the mean of the nearest known date by hour\n",
    "    impute_mean_day(gen_forecast, col, 24)\n",
    "    \n",
    "    # Fill remaining Nans\n",
    "    gen_forecast[col].fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generation_scheduled</th>\n",
       "      <th>consumption_scheduled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>23921.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00</th>\n",
       "      <td>23242.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00</th>\n",
       "      <td>21878.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 03:00:00</th>\n",
       "      <td>20228.0</td>\n",
       "      <td>465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 04:00:00</th>\n",
       "      <td>19579.0</td>\n",
       "      <td>504.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31 19:00:00</th>\n",
       "      <td>22319.0</td>\n",
       "      <td>766.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31 20:00:00</th>\n",
       "      <td>21886.0</td>\n",
       "      <td>766.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31 21:00:00</th>\n",
       "      <td>21315.0</td>\n",
       "      <td>766.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31 22:00:00</th>\n",
       "      <td>20411.0</td>\n",
       "      <td>766.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31 23:00:00</th>\n",
       "      <td>19699.0</td>\n",
       "      <td>912.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61368 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     generation_scheduled  consumption_scheduled\n",
       "time                                                            \n",
       "2015-01-01 00:00:00               23921.0                    0.0\n",
       "2015-01-01 01:00:00               23242.0                  176.0\n",
       "2015-01-01 02:00:00               21878.0                  121.0\n",
       "2015-01-01 03:00:00               20228.0                  465.0\n",
       "2015-01-01 04:00:00               19579.0                  504.0\n",
       "...                                   ...                    ...\n",
       "2021-12-31 19:00:00               22319.0                  766.0\n",
       "2021-12-31 20:00:00               21886.0                  766.0\n",
       "2021-12-31 21:00:00               21315.0                  766.0\n",
       "2021-12-31 22:00:00               20411.0                  766.0\n",
       "2021-12-31 23:00:00               19699.0                  912.0\n",
       "\n",
       "[61368 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prices (2019-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "prices_19_21 = pd.read_csv('../data/prices.csv')\n",
    "\n",
    "# Change column to datetime and set as index\n",
    "prices_19_21['datetime'] = pd.to_datetime(prices_19_21['datetime'])\n",
    "prices_19_21.set_index('datetime', inplace=True)\n",
    "\n",
    "# Drop duplicates and all rows in 2022\n",
    "prices_19_21 = prices_19_21.groupby(by=prices_19_21.index).max()\n",
    "prices_19_21 = prices_19_21.loc['2019':'2021'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data from years 2019-2021\n",
    "df_19_21 = gen.join([load_forecast, ws, price_ahead, prices_19_21])\n",
    "\n",
    "# Join data from 2015-2021\n",
    "df_15_21 = gen_forecast.join([border, weather])\n",
    "\n",
    "# Concatenate data from 15-18 with 19-21 and transmission data\n",
    "df = pd.concat([energy, df_19_21]).join(df_15_21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 5/20/21 - 5/31/21 since missing price data\n",
    "drop = pd.date_range(start='2021-05-20 08', end='2021-05-31 23', freq='H')\n",
    "df.drop(drop, inplace=True)\n",
    "\n",
    "# Impute remaining missing price data\n",
    "indices = df.loc[df['price actual'].isna()].index\n",
    "for i in indices:\n",
    "    df.loc[i, 'price actual'] = impute_immediate_mean(df['price actual'], i)\n",
    "    \n",
    "df['generation total'] = df.loc[:,:'generation wind onshore'].sum(axis=1)\n",
    "df['diff'] = df['total load actual'] - df['generation total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "energy.to_csv('../data/energy_clean.csv')\n",
    "weather.to_csv('../data/weather/weather_clean.csv')\n",
    "border.to_csv('../data/transmission_clean.csv')\n",
    "gen.to_csv('../data/generation_2019-21.csv')\n",
    "load_forecast.to_csv('../data/load_forecast_2019-21.csv')\n",
    "ws.to_csv('../data/wind_solar_2019-21.csv')\n",
    "price_ahead.to_csv('../data/price_ahead_2019-21.csv')\n",
    "gen_forecast.to_csv('../data/gen_forecast.csv')\n",
    "df.to_csv('../data/df_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
