{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from keras import layers, models, regularizers\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import os\n",
    "import winsound\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime as dt\n",
    "\n",
    "os.chdir('../scripts')\n",
    "from functions import impute_immediate_mean, split_data, SMAPE,sMAPE, r2, compute_metrics, to_supervised\n",
    "os.chdir('../notebooks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up alarm for notification of model completion\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag =pd.read_csv('../data/clean/df_clean_lag.csv', index_col=0, parse_dates=True)\n",
    "TSO_preds = df_lag.price_day_ahead.copy()\n",
    "y_true_train = df_lag.loc[:'2019', 'price_actual'].copy()\n",
    "y_true_val = df_lag.loc['2020', 'price_actual'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = df_lag.select_dtypes(exclude='object').drop(columns=['price_actual', 'price_day_ahead']).columns\n",
    "\n",
    "# Get rid of negatives\n",
    "time = dt.datetime(2021,3,24,22)\n",
    "df_lag.loc[time, 'dew_point_bilbao_lag'] = impute_immediate_mean(df_lag['dew_point_bilbao_lag'], time)\n",
    "\n",
    "# Rescale data [-1,1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_lag[continuous] = scaler.fit_transform(df_lag[continuous])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get catergorical \n",
    "categorical = df_lag.select_dtypes(include='object').columns\n",
    "\n",
    "# Get wind direction cols\n",
    "wind_dirs = df_lag.filter(regex='wind(?!_speeds|_forecast)').columns\n",
    "\n",
    "# Instantiate encoder and transfrom wind_dir cols\n",
    "wind_dir_coder = LabelEncoder()\n",
    "wind_dir_coder.fit(df_lag['wind_madrid_lag'])\n",
    "for col in wind_dirs:\n",
    "    df_lag[col] = wind_dir_coder.transform(df_lag[col])\n",
    "    \n",
    "# Stack condition columns into single col\n",
    "stacked_conditions = df_lag.filter(regex='condition').stack()\n",
    "\n",
    "# Instantiate Label encoder, fit and transform on condition cols\n",
    "condition_coder = LabelEncoder()\n",
    "condition_coder.fit(stacked_conditions)\n",
    "for col in df_lag.filter(regex='condition').columns:\n",
    "    df_lag[col] = condition_coder.transform(df_lag[col])\n",
    "\n",
    "# Rescale data [-1,1]\n",
    "df_lag[categorical] = scaler.fit_transform(df_lag[categorical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get price components to drop\n",
    "price_drop = df_lag.filter(regex='price_(?!actual|day)').columns\n",
    "\n",
    "# Split data\n",
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSO_metrics = [round(sMAPE(y_true_train, TSO_preds.loc[:'2019']),3),\n",
    "               round(sMAPE(y_true_val, TSO_preds.loc['2020']),3), \n",
    "               round(r2(y_true_train, TSO_preds.loc[:'2019']),3),\n",
    "               round(r2(y_true_val, TSO_preds.loc['2020']),3)]\n",
    "results_actual = pd.DataFrame({'TSO':TSO_metrics}, \n",
    "                              index=['sMAPE_train', 'sMAPE_val', 'r2_train', 'r2_val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_fit(nn, train, validation,\n",
    "                patience=10,\n",
    "                metric=SMAPE,\n",
    "                #metric = tf.keras.metrics.MeanAbsolutePercentageError(name='MAPE'),\n",
    "                loss = tf.keras.metrics.mean_absolute_error,\n",
    "                batch_size = None):\n",
    "    \n",
    "    # Create early stopping point\n",
    "    metric.name='SMAPE'\n",
    "    callback = keras.callbacks.EarlyStopping(\n",
    "        patience=patience,\n",
    "        monitor='val_'+metric.name,\n",
    "        mode='min',\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    # Compile the model\n",
    "    nn.compile(\n",
    "        loss=loss, \n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=metric\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    history = nn.fit(\n",
    "        x = train[0],\n",
    "        y = train[1],\n",
    "        batch_size=batch_size,\n",
    "        epochs = 200,\n",
    "        callbacks=[callback],\n",
    "        validation_data=validation\n",
    "    )\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(59, activation='relu', input_shape=input_shape))\n",
    "nn.add(layers.Dense(239, activation='relu'))\n",
    "nn.add(layers.Dense(162, activation='relu'))\n",
    "nn.add(layers.Dense(1, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 2.1026 - SMAPE: 4.3472 - val_loss: 1.6477 - val_SMAPE: 4.7383\n",
      "Epoch 2/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.5031 - SMAPE: 2.8752 - val_loss: 2.0099 - val_SMAPE: 5.6838\n",
      "Epoch 3/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.4405 - SMAPE: 2.7427 - val_loss: 1.7656 - val_SMAPE: 5.0484\n",
      "Epoch 4/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.3750 - SMAPE: 2.6169 - val_loss: 1.3846 - val_SMAPE: 4.0602\n",
      "Epoch 5/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.3396 - SMAPE: 2.5426 - val_loss: 2.1048 - val_SMAPE: 6.1020\n",
      "Epoch 6/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.2878 - SMAPE: 2.4418 - val_loss: 1.5532 - val_SMAPE: 4.7048\n",
      "Epoch 7/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.2821 - SMAPE: 2.4256 - val_loss: 1.6325 - val_SMAPE: 4.9195\n",
      "Epoch 8/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.2372 - SMAPE: 2.3477 - val_loss: 1.4925 - val_SMAPE: 4.4315\n",
      "Epoch 9/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.2092 - SMAPE: 2.2884 - val_loss: 1.8251 - val_SMAPE: 5.4849\n",
      "Epoch 10/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.2296 - SMAPE: 2.3188 - val_loss: 1.3893 - val_SMAPE: 4.0873\n",
      "Epoch 11/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.2004 - SMAPE: 2.2662 - val_loss: 2.2548 - val_SMAPE: 6.5332\n",
      "Epoch 12/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.1956 - SMAPE: 2.2502 - val_loss: 1.4752 - val_SMAPE: 4.4273\n",
      "Epoch 13/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.1782 - SMAPE: 2.2192 - val_loss: 1.8932 - val_SMAPE: 5.6235\n",
      "Epoch 14/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.1669 - SMAPE: 2.1948 - val_loss: 1.5002 - val_SMAPE: 4.4074\n"
     ]
    }
   ],
   "source": [
    "nn1 = compile_fit(nn, (X_train,y_train), (X_val, y_val), patience=10,\n",
    "                  loss = tf.keras.metrics.mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSO</th>\n",
       "      <th>nn1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sMAPE_train</th>\n",
       "      <td>16.030</td>\n",
       "      <td>2.908806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sMAPE_val</th>\n",
       "      <td>16.922</td>\n",
       "      <td>4.066236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_train</th>\n",
       "      <td>0.954</td>\n",
       "      <td>0.984056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_val</th>\n",
       "      <td>0.971</td>\n",
       "      <td>0.977619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                TSO       nn1\n",
       "sMAPE_train  16.030  2.908806\n",
       "sMAPE_val    16.922  4.066236\n",
       "r2_train      0.954  0.984056\n",
       "r2_val        0.971  0.977619"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_actual['nn1'] = compute_metrics(nn1, (X_train,y_train), (X_val, y_val))\n",
    "results_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['temp_madrid_lag', 'dew_point_madrid_lag', 'humidities_madrid_lag',\n",
       "       'wind_madrid_lag', 'wind_speeds_madrid_lag', 'pressures_madrid_lag',\n",
       "       'condition_madrid_lag', 'temp_seville_lag', 'dew_point_seville_lag',\n",
       "       'humidities_seville_lag', 'wind_seville_lag', 'wind_speeds_seville_lag',\n",
       "       'pressures_seville_lag', 'condition_seville_lag', 'temp_barcelona_lag',\n",
       "       'dew_point_barcelona_lag', 'humidities_barcelona_lag',\n",
       "       'wind_barcelona_lag', 'wind_speeds_barcelona_lag',\n",
       "       'pressures_barcelona_lag', 'condition_barcelona_lag', 'temp_bilbao_lag',\n",
       "       'dew_point_bilbao_lag', 'humidities_bilbao_lag', 'wind_bilbao_lag',\n",
       "       'wind_speeds_bilbao_lag', 'pressures_bilbao_lag',\n",
       "       'condition_bilbao_lag', 'temp_valencia_lag', 'dew_point_valencia_lag',\n",
       "       'humidities_valencia_lag', 'wind_valencia_lag',\n",
       "       'wind_speeds_valencia_lag', 'pressures_valencia_lag',\n",
       "       'condition_valencia_lag', 'transmission_ps_lag', 'transmission_sp_lag',\n",
       "       'transmission_fs_lag', 'transmission_sf_lag', 'biomass_lag',\n",
       "       'coal/lignite_lag', 'gas_lag', 'coal_lag', 'oil_lag', 'consumption_lag',\n",
       "       'poundage_lag', 'reservoir_lag', 'nuclear_lag', 'other_lag',\n",
       "       'renewable_lag', 'solar_lag', 'waste_lag', 'onshore_lag',\n",
       "       'load_forecast', 'load_actual', 'solar_forecast', 'wind_forecast',\n",
       "       'generation_forecast', 'consumption_forecast'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network - Multivariate (24 hour prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restructure data for 24 hour input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "#X_train, y_train, X_val, y_val = create_windows((X_train,y_train), (X_val, y_val), 24, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df, input_window):\n",
    "    \n",
    "    # Find minimum number of rows to add to df that makes divisible by input_window\n",
    "    whole = len(df)//input_window\n",
    "    decimal = len(df)/input_window\n",
    "    missing_frac = 1 + (whole-decimal)\n",
    "    num_rows_to_add = int(round(missing_frac*input_window))\n",
    "    \n",
    "    # Append the df to the first subset of rows of df and return\n",
    "    df = df.iloc[:num_rows_to_add].append(df).copy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(train, test, input_window, output_window, padding='repeat'):\n",
    "    \n",
    "    # If the training set is not divisible by the output_window\n",
    "    if train[0].shape[0]%input_window != 0:\n",
    "        X_train = pad_df(train[0], input_window)\n",
    "        y_train = train[1]\n",
    "    if test[0].shape[0]%input_window != 0:\n",
    "        X_test = pad_df(test[0], input_window)\n",
    "        y_test =  test[1]\n",
    "\n",
    "    else:\n",
    "        X_train, y_train = train[0], train[1]\n",
    "        X_test, y_test = test[0], test[1]\n",
    "    X_train_windows = np.array(np.split(X_train, len(X_train)/input_window))\n",
    "    y_train_windows = np.array(np.split(y_train, len(y_train)/output_window))\n",
    "    X_val_windows = np.array(np.split(X_test, len(X_test)/input_window))\n",
    "    y_val_windows = np.array(np.split(y_test, len(y_test)/output_window))\n",
    "    return X_train_windows, y_train_windows, X_val_windows, y_val_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "input_window=24*7\n",
    "output_window=24\n",
    "train, test = (X_train, y_train), (X_val, y_val)\n",
    "stride = 24\n",
    "\n",
    "\n",
    "def pad_df(df, input_window, output_window):\n",
    "    # Find number of rows needed for first sample\n",
    "    nrows = input_window-output_window\n",
    "    # Append the df to the first subset of rows of df and return\n",
    "    df = df.iloc[:nrows].append(df).copy()\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_samples(train, test, input_window, output_window, padding='repeat'):\n",
    "    if input_window >24: \n",
    "        X_train = pad_df(train[0], input_window, output_window)\n",
    "        X_test = pad_df(test[0], input_window, output_window)\n",
    "        y_train, y_test = train[1], test[1]\n",
    "\n",
    "    else:\n",
    "        X_train, y_train = train[0], train[1]\n",
    "        X_test, y_test = test[0], test[1]\n",
    "\n",
    "    i = 0\n",
    "    X_train_resampled = pd.DataFrame()\n",
    "    while i < len(X_train):\n",
    "        X_train_resampled = pd.concat([X_train_resampled, X_train.iloc[i:i+input_window].copy()])\n",
    "        i += stride\n",
    "    i = 0\n",
    "    X_test_resampled = pd.DataFrame()\n",
    "    while i < len(X_test):\n",
    "        X_test_resampled = pd.concat([X_test_resampled, X_val.iloc[i:i+input_window].copy()])\n",
    "        i += stride\n",
    "\n",
    "    # Split the data by the input and output windows\n",
    "    X_train_windows = np.array(np.split(X_train_resampled, input_window))\n",
    "    y_train_windows = np.array(np.split(y_train, len(y_train)/output_window))\n",
    "    X_val_windows = np.array(np.split(X_test_resampled, input_window))\n",
    "    y_val_windows = np.array(np.split(y_test, len(y_test)/output_window))\n",
    "    return X_train_windows, y_train_windows, X_val_windows, y_val_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "\n",
    "X_train, y_train, X_val, y_val = create_samples((X_train, y_train), (X_val, y_val), 24*7, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 1828, 60)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 24)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_windows = np.array(np.split(X_train_resampled, input_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 1828, 60)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1828.0"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_resampled)/input_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307104, 60)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60984, 60)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307104, 60)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_madrid_lag</th>\n",
       "      <th>dew_point_madrid_lag</th>\n",
       "      <th>humidities_madrid_lag</th>\n",
       "      <th>wind_madrid_lag</th>\n",
       "      <th>wind_speeds_madrid_lag</th>\n",
       "      <th>pressures_madrid_lag</th>\n",
       "      <th>condition_madrid_lag</th>\n",
       "      <th>temp_seville_lag</th>\n",
       "      <th>dew_point_seville_lag</th>\n",
       "      <th>humidities_seville_lag</th>\n",
       "      <th>...</th>\n",
       "      <th>solar_lag</th>\n",
       "      <th>waste_lag</th>\n",
       "      <th>onshore_lag</th>\n",
       "      <th>load_forecast</th>\n",
       "      <th>load_actual</th>\n",
       "      <th>solar_forecast</th>\n",
       "      <th>wind_forecast</th>\n",
       "      <th>generation_forecast</th>\n",
       "      <th>consumption_forecast</th>\n",
       "      <th>price_day_ahead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.184211</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>-0.444444</td>\n",
       "      <td>-0.956522</td>\n",
       "      <td>0.970097</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.099099</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.993716</td>\n",
       "      <td>0.484594</td>\n",
       "      <td>-0.821197</td>\n",
       "      <td>-0.395978</td>\n",
       "      <td>-0.492593</td>\n",
       "      <td>-0.997127</td>\n",
       "      <td>-0.867984</td>\n",
       "      <td>-0.546393</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>41.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:00:00</th>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.184211</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.968707</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.135135</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.993716</td>\n",
       "      <td>0.462185</td>\n",
       "      <td>-0.824212</td>\n",
       "      <td>-0.451013</td>\n",
       "      <td>-0.556217</td>\n",
       "      <td>-0.997666</td>\n",
       "      <td>-0.861960</td>\n",
       "      <td>-0.589977</td>\n",
       "      <td>-0.976371</td>\n",
       "      <td>38.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 02:00:00</th>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.184211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>-0.913043</td>\n",
       "      <td>0.968707</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.135135</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.993716</td>\n",
       "      <td>0.439776</td>\n",
       "      <td>-0.831248</td>\n",
       "      <td>-0.546265</td>\n",
       "      <td>-0.657175</td>\n",
       "      <td>-0.998025</td>\n",
       "      <td>-0.856463</td>\n",
       "      <td>-0.602506</td>\n",
       "      <td>-0.853321</td>\n",
       "      <td>36.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 03:00:00</th>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.184211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.968707</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.135135</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.995461</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>-0.818986</td>\n",
       "      <td>-0.639174</td>\n",
       "      <td>-0.751142</td>\n",
       "      <td>-0.998384</td>\n",
       "      <td>-0.853715</td>\n",
       "      <td>-0.580334</td>\n",
       "      <td>-0.599198</td>\n",
       "      <td>32.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 04:00:00</th>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>-0.782609</td>\n",
       "      <td>0.968707</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.171171</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.86</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.996509</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>-0.815770</td>\n",
       "      <td>-0.694512</td>\n",
       "      <td>-0.810556</td>\n",
       "      <td>-0.998025</td>\n",
       "      <td>-0.852553</td>\n",
       "      <td>-0.557631</td>\n",
       "      <td>-0.244316</td>\n",
       "      <td>30.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 19:00:00</th>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.608696</td>\n",
       "      <td>0.941586</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.189189</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.993890</td>\n",
       "      <td>0.344538</td>\n",
       "      <td>0.013920</td>\n",
       "      <td>0.230269</td>\n",
       "      <td>0.205687</td>\n",
       "      <td>-0.994614</td>\n",
       "      <td>-0.170489</td>\n",
       "      <td>0.212301</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>60.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 20:00:00</th>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.941586</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.225225</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994414</td>\n",
       "      <td>0.378151</td>\n",
       "      <td>0.019448</td>\n",
       "      <td>0.196553</td>\n",
       "      <td>0.163033</td>\n",
       "      <td>-0.997127</td>\n",
       "      <td>-0.190995</td>\n",
       "      <td>0.139787</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>56.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 21:00:00</th>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.941586</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.261261</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994588</td>\n",
       "      <td>0.355742</td>\n",
       "      <td>0.025278</td>\n",
       "      <td>0.068264</td>\n",
       "      <td>0.074387</td>\n",
       "      <td>-0.998205</td>\n",
       "      <td>-0.181799</td>\n",
       "      <td>-0.004480</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>52.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 22:00:00</th>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>-0.913043</td>\n",
       "      <td>0.941586</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.297297</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994588</td>\n",
       "      <td>0.350140</td>\n",
       "      <td>0.014624</td>\n",
       "      <td>-0.143559</td>\n",
       "      <td>-0.136423</td>\n",
       "      <td>-0.997666</td>\n",
       "      <td>-0.189092</td>\n",
       "      <td>-0.149582</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>51.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 23:00:00</th>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>-0.913043</td>\n",
       "      <td>0.943672</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.297297</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994414</td>\n",
       "      <td>0.366947</td>\n",
       "      <td>-0.007990</td>\n",
       "      <td>-0.256350</td>\n",
       "      <td>-0.249772</td>\n",
       "      <td>-0.998923</td>\n",
       "      <td>-0.222070</td>\n",
       "      <td>-0.207137</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>52.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60984 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     temp_madrid_lag  dew_point_madrid_lag  \\\n",
       "2020-01-01 00:00:00            -0.50             -0.184211   \n",
       "2020-01-01 01:00:00            -0.50             -0.184211   \n",
       "2020-01-01 02:00:00            -0.54             -0.184211   \n",
       "2020-01-01 03:00:00            -0.54             -0.184211   \n",
       "2020-01-01 04:00:00            -0.54             -0.236842   \n",
       "...                              ...                   ...   \n",
       "2020-12-31 19:00:00            -0.32             -0.315789   \n",
       "2020-12-31 20:00:00            -0.36             -0.289474   \n",
       "2020-12-31 21:00:00            -0.46             -0.289474   \n",
       "2020-12-31 22:00:00            -0.50             -0.289474   \n",
       "2020-12-31 23:00:00            -0.54             -0.289474   \n",
       "\n",
       "                     humidities_madrid_lag  wind_madrid_lag  \\\n",
       "2020-01-01 00:00:00               0.851064        -0.444444   \n",
       "2020-01-01 01:00:00               0.851064        -1.000000   \n",
       "2020-01-01 02:00:00               1.000000        -0.222222   \n",
       "2020-01-01 03:00:00               1.000000        -0.222222   \n",
       "2020-01-01 04:00:00               0.851064        -0.222222   \n",
       "...                                    ...              ...   \n",
       "2020-12-31 19:00:00               0.000000         0.666667   \n",
       "2020-12-31 20:00:00               0.170213        -0.555556   \n",
       "2020-12-31 21:00:00               0.468085        -1.000000   \n",
       "2020-12-31 22:00:00               0.595745         0.555556   \n",
       "2020-12-31 23:00:00               0.723404         0.555556   \n",
       "\n",
       "                     wind_speeds_madrid_lag  pressures_madrid_lag  \\\n",
       "2020-01-01 00:00:00               -0.956522              0.970097   \n",
       "2020-01-01 01:00:00               -1.000000              0.968707   \n",
       "2020-01-01 02:00:00               -0.913043              0.968707   \n",
       "2020-01-01 03:00:00               -0.869565              0.968707   \n",
       "2020-01-01 04:00:00               -0.782609              0.968707   \n",
       "...                                     ...                   ...   \n",
       "2020-12-31 19:00:00               -0.608696              0.941586   \n",
       "2020-12-31 20:00:00               -0.869565              0.941586   \n",
       "2020-12-31 21:00:00               -1.000000              0.941586   \n",
       "2020-12-31 22:00:00               -0.913043              0.941586   \n",
       "2020-12-31 23:00:00               -0.913043              0.943672   \n",
       "\n",
       "                     condition_madrid_lag  temp_seville_lag  \\\n",
       "2020-01-01 00:00:00             -0.864407         -0.099099   \n",
       "2020-01-01 01:00:00             -0.864407         -0.135135   \n",
       "2020-01-01 02:00:00             -0.864407         -0.135135   \n",
       "2020-01-01 03:00:00             -0.864407         -0.135135   \n",
       "2020-01-01 04:00:00             -0.864407         -0.171171   \n",
       "...                                   ...               ...   \n",
       "2020-12-31 19:00:00             -0.864407         -0.189189   \n",
       "2020-12-31 20:00:00             -0.864407         -0.225225   \n",
       "2020-12-31 21:00:00             -0.864407         -0.261261   \n",
       "2020-12-31 22:00:00             -0.864407         -0.297297   \n",
       "2020-12-31 23:00:00             -0.864407         -0.297297   \n",
       "\n",
       "                     dew_point_seville_lag  humidities_seville_lag  ...  \\\n",
       "2020-01-01 00:00:00               0.232877                    0.64  ...   \n",
       "2020-01-01 01:00:00               0.232877                    0.74  ...   \n",
       "2020-01-01 02:00:00               0.232877                    0.74  ...   \n",
       "2020-01-01 03:00:00               0.232877                    0.74  ...   \n",
       "2020-01-01 04:00:00               0.232877                    0.86  ...   \n",
       "...                                    ...                     ...  ...   \n",
       "2020-12-31 19:00:00               0.068493                    0.62  ...   \n",
       "2020-12-31 20:00:00               0.013699                    0.62  ...   \n",
       "2020-12-31 21:00:00               0.013699                    0.74  ...   \n",
       "2020-12-31 22:00:00              -0.013699                    0.74  ...   \n",
       "2020-12-31 23:00:00              -0.013699                    0.74  ...   \n",
       "\n",
       "                     solar_lag  waste_lag  onshore_lag  load_forecast  \\\n",
       "2020-01-01 00:00:00  -0.993716   0.484594    -0.821197      -0.395978   \n",
       "2020-01-01 01:00:00  -0.993716   0.462185    -0.824212      -0.451013   \n",
       "2020-01-01 02:00:00  -0.993716   0.439776    -0.831248      -0.546265   \n",
       "2020-01-01 03:00:00  -0.995461   0.450980    -0.818986      -0.639174   \n",
       "2020-01-01 04:00:00  -0.996509   0.450980    -0.815770      -0.694512   \n",
       "...                        ...        ...          ...            ...   \n",
       "2020-12-31 19:00:00  -0.993890   0.344538     0.013920       0.230269   \n",
       "2020-12-31 20:00:00  -0.994414   0.378151     0.019448       0.196553   \n",
       "2020-12-31 21:00:00  -0.994588   0.355742     0.025278       0.068264   \n",
       "2020-12-31 22:00:00  -0.994588   0.350140     0.014624      -0.143559   \n",
       "2020-12-31 23:00:00  -0.994414   0.366947    -0.007990      -0.256350   \n",
       "\n",
       "                     load_actual  solar_forecast  wind_forecast  \\\n",
       "2020-01-01 00:00:00    -0.492593       -0.997127      -0.867984   \n",
       "2020-01-01 01:00:00    -0.556217       -0.997666      -0.861960   \n",
       "2020-01-01 02:00:00    -0.657175       -0.998025      -0.856463   \n",
       "2020-01-01 03:00:00    -0.751142       -0.998384      -0.853715   \n",
       "2020-01-01 04:00:00    -0.810556       -0.998025      -0.852553   \n",
       "...                          ...             ...            ...   \n",
       "2020-12-31 19:00:00     0.205687       -0.994614      -0.170489   \n",
       "2020-12-31 20:00:00     0.163033       -0.997127      -0.190995   \n",
       "2020-12-31 21:00:00     0.074387       -0.998205      -0.181799   \n",
       "2020-12-31 22:00:00    -0.136423       -0.997666      -0.189092   \n",
       "2020-12-31 23:00:00    -0.249772       -0.998923      -0.222070   \n",
       "\n",
       "                     generation_forecast  consumption_forecast  \\\n",
       "2020-01-01 00:00:00            -0.546393             -1.000000   \n",
       "2020-01-01 01:00:00            -0.589977             -0.976371   \n",
       "2020-01-01 02:00:00            -0.602506             -0.853321   \n",
       "2020-01-01 03:00:00            -0.580334             -0.599198   \n",
       "2020-01-01 04:00:00            -0.557631             -0.244316   \n",
       "...                                  ...                   ...   \n",
       "2020-12-31 19:00:00             0.212301             -1.000000   \n",
       "2020-12-31 20:00:00             0.139787             -1.000000   \n",
       "2020-12-31 21:00:00            -0.004480             -1.000000   \n",
       "2020-12-31 22:00:00            -0.149582             -1.000000   \n",
       "2020-12-31 23:00:00            -0.207137             -1.000000   \n",
       "\n",
       "                     price_day_ahead  \n",
       "2020-01-01 00:00:00            41.88  \n",
       "2020-01-01 01:00:00            38.60  \n",
       "2020-01-01 02:00:00            36.55  \n",
       "2020-01-01 03:00:00            32.32  \n",
       "2020-01-01 04:00:00            30.85  \n",
       "...                              ...  \n",
       "2020-12-31 19:00:00            60.54  \n",
       "2020-12-31 20:00:00            56.75  \n",
       "2020-12-31 21:00:00            52.44  \n",
       "2020-12-31 22:00:00            51.86  \n",
       "2020-12-31 23:00:00            52.26  \n",
       "\n",
       "[60984 rows x 60 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "stride = 24\n",
    "X_train_resampled = pd.DataFrame()\n",
    "X_test_resampled = pd.DataFrame()\n",
    "while i < len(X_train):\n",
    "    X_train_resampled = pd.concat([X_train_resampled, X_train.iloc[i:i+input_window].copy()])\n",
    "    X_test_resampled = pd.concat([X_test_resampled, X_val.iloc[i:i+input_window].copy()])\n",
    "    i += stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307104, 60)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 60)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "stride=24\n",
    "X_train_resampled = pd.DataFrame()\n",
    "X_train_resampled = pd.concat([X_train_resampled, X_train.iloc[i:i+stride].copy()])\n",
    "X_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 60)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 48\n",
    "X_train_resampled = pd.concat([X_train_resampled, X_train.iloc[i:i+stride].copy()])\n",
    "X_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_madrid_lag</th>\n",
       "      <th>dew_point_madrid_lag</th>\n",
       "      <th>humidities_madrid_lag</th>\n",
       "      <th>wind_madrid_lag</th>\n",
       "      <th>wind_speeds_madrid_lag</th>\n",
       "      <th>pressures_madrid_lag</th>\n",
       "      <th>condition_madrid_lag</th>\n",
       "      <th>temp_seville_lag</th>\n",
       "      <th>dew_point_seville_lag</th>\n",
       "      <th>humidities_seville_lag</th>\n",
       "      <th>...</th>\n",
       "      <th>solar_lag</th>\n",
       "      <th>waste_lag</th>\n",
       "      <th>onshore_lag</th>\n",
       "      <th>load_forecast</th>\n",
       "      <th>load_actual</th>\n",
       "      <th>solar_forecast</th>\n",
       "      <th>wind_forecast</th>\n",
       "      <th>generation_forecast</th>\n",
       "      <th>consumption_forecast</th>\n",
       "      <th>price_day_ahead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02 00:00:00</th>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.913043</td>\n",
       "      <td>0.974965</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.123288</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991446</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>-0.358963</td>\n",
       "      <td>-0.347672</td>\n",
       "      <td>-0.386552</td>\n",
       "      <td>-0.997666</td>\n",
       "      <td>-0.655745</td>\n",
       "      <td>-0.464617</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>47.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 01:00:00</th>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.977051</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.123288</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991272</td>\n",
       "      <td>0.092437</td>\n",
       "      <td>-0.408010</td>\n",
       "      <td>-0.519050</td>\n",
       "      <td>-0.578538</td>\n",
       "      <td>-0.998205</td>\n",
       "      <td>-0.687982</td>\n",
       "      <td>-0.566894</td>\n",
       "      <td>-0.898350</td>\n",
       "      <td>40.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 02:00:00</th>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>-0.913043</td>\n",
       "      <td>0.977051</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.339339</td>\n",
       "      <td>-0.123288</td>\n",
       "      <td>0.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991272</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>-0.451128</td>\n",
       "      <td>-0.628288</td>\n",
       "      <td>-0.688629</td>\n",
       "      <td>-0.998384</td>\n",
       "      <td>-0.719374</td>\n",
       "      <td>-0.624222</td>\n",
       "      <td>-0.740080</td>\n",
       "      <td>36.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 03:00:00</th>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>-0.956522</td>\n",
       "      <td>0.977051</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.345345</td>\n",
       "      <td>-0.123288</td>\n",
       "      <td>0.70</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991272</td>\n",
       "      <td>0.070028</td>\n",
       "      <td>-0.473541</td>\n",
       "      <td>-0.682567</td>\n",
       "      <td>-0.749394</td>\n",
       "      <td>-0.998025</td>\n",
       "      <td>-0.718317</td>\n",
       "      <td>-0.643508</td>\n",
       "      <td>-0.505127</td>\n",
       "      <td>37.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:00:00</th>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.977051</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.351351</td>\n",
       "      <td>-0.123288</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.992668</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>-0.503995</td>\n",
       "      <td>-0.707590</td>\n",
       "      <td>-0.767664</td>\n",
       "      <td>-0.998564</td>\n",
       "      <td>-0.734489</td>\n",
       "      <td>-0.647304</td>\n",
       "      <td>-0.497994</td>\n",
       "      <td>35.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:00:00</th>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.421053</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.979138</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.178082</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994065</td>\n",
       "      <td>0.053221</td>\n",
       "      <td>-0.535856</td>\n",
       "      <td>-0.660493</td>\n",
       "      <td>-0.718972</td>\n",
       "      <td>-0.998564</td>\n",
       "      <td>-0.700772</td>\n",
       "      <td>-0.627411</td>\n",
       "      <td>-0.522069</td>\n",
       "      <td>37.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 06:00:00</th>\n",
       "      <td>-0.72</td>\n",
       "      <td>-0.473684</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>-0.913043</td>\n",
       "      <td>0.979138</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.178082</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994065</td>\n",
       "      <td>0.042017</td>\n",
       "      <td>-0.558068</td>\n",
       "      <td>-0.524493</td>\n",
       "      <td>-0.556932</td>\n",
       "      <td>-0.998205</td>\n",
       "      <td>-0.690413</td>\n",
       "      <td>-0.552012</td>\n",
       "      <td>-0.641551</td>\n",
       "      <td>43.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 07:00:00</th>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.421053</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.979138</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.351351</td>\n",
       "      <td>-0.178082</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.993890</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>-0.598774</td>\n",
       "      <td>-0.315921</td>\n",
       "      <td>-0.150641</td>\n",
       "      <td>-0.996768</td>\n",
       "      <td>-0.693056</td>\n",
       "      <td>-0.407517</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>56.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 08:00:00</th>\n",
       "      <td>-0.72</td>\n",
       "      <td>-0.421053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.980529</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.351351</td>\n",
       "      <td>-0.232877</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.990573</td>\n",
       "      <td>0.109244</td>\n",
       "      <td>-0.635258</td>\n",
       "      <td>-0.088373</td>\n",
       "      <td>0.052504</td>\n",
       "      <td>-0.988688</td>\n",
       "      <td>-0.690519</td>\n",
       "      <td>-0.229613</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>57.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 09:00:00</th>\n",
       "      <td>-0.76</td>\n",
       "      <td>-0.473684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.982615</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.387387</td>\n",
       "      <td>-0.232877</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.870298</td>\n",
       "      <td>0.109244</td>\n",
       "      <td>-0.691140</td>\n",
       "      <td>0.155806</td>\n",
       "      <td>0.154073</td>\n",
       "      <td>-0.875931</td>\n",
       "      <td>-0.717789</td>\n",
       "      <td>-0.003341</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>60.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 10:00:00</th>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.421053</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.984701</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.297297</td>\n",
       "      <td>-0.123288</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.647552</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.730338</td>\n",
       "      <td>0.291881</td>\n",
       "      <td>0.255650</td>\n",
       "      <td>-0.677350</td>\n",
       "      <td>-0.759751</td>\n",
       "      <td>0.091268</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>62.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 11:00:00</th>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>-0.956522</td>\n",
       "      <td>0.984701</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.171171</td>\n",
       "      <td>-0.123288</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.441913</td>\n",
       "      <td>0.154062</td>\n",
       "      <td>-0.721494</td>\n",
       "      <td>0.262398</td>\n",
       "      <td>0.253664</td>\n",
       "      <td>-0.447347</td>\n",
       "      <td>-0.819469</td>\n",
       "      <td>0.141382</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>60.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 12:00:00</th>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>-0.782609</td>\n",
       "      <td>0.984701</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.063063</td>\n",
       "      <td>-0.068493</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.321812</td>\n",
       "      <td>0.170868</td>\n",
       "      <td>-0.707925</td>\n",
       "      <td>0.211597</td>\n",
       "      <td>0.317090</td>\n",
       "      <td>-0.359368</td>\n",
       "      <td>-0.867984</td>\n",
       "      <td>0.090281</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>59.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 13:00:00</th>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>-0.956522</td>\n",
       "      <td>0.984701</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.009009</td>\n",
       "      <td>-0.232877</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.300515</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>-0.706216</td>\n",
       "      <td>0.210160</td>\n",
       "      <td>0.348807</td>\n",
       "      <td>-0.358650</td>\n",
       "      <td>-0.891132</td>\n",
       "      <td>0.030752</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>59.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 14:00:00</th>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>-0.127660</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>-0.956522</td>\n",
       "      <td>0.982615</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>0.063063</td>\n",
       "      <td>-0.260274</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306450</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>-0.704005</td>\n",
       "      <td>0.175083</td>\n",
       "      <td>0.364661</td>\n",
       "      <td>-0.376246</td>\n",
       "      <td>-0.880985</td>\n",
       "      <td>-0.040850</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>58.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 15:00:00</th>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.421053</td>\n",
       "      <td>-0.425532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.980529</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>0.063063</td>\n",
       "      <td>-0.178082</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333508</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>-0.727725</td>\n",
       "      <td>0.118385</td>\n",
       "      <td>0.372588</td>\n",
       "      <td>-0.473023</td>\n",
       "      <td>-0.874326</td>\n",
       "      <td>-0.080638</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>58.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 16:00:00</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>-0.574468</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.980529</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>-0.260274</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460941</td>\n",
       "      <td>0.182073</td>\n",
       "      <td>-0.743505</td>\n",
       "      <td>0.087390</td>\n",
       "      <td>0.376552</td>\n",
       "      <td>-0.572134</td>\n",
       "      <td>-0.847691</td>\n",
       "      <td>-0.097950</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>59.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 17:00:00</th>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>-0.531915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.913043</td>\n",
       "      <td>0.980529</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>0.063063</td>\n",
       "      <td>-0.232877</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.743912</td>\n",
       "      <td>0.165266</td>\n",
       "      <td>-0.728730</td>\n",
       "      <td>0.093967</td>\n",
       "      <td>0.378530</td>\n",
       "      <td>-0.782566</td>\n",
       "      <td>-0.834056</td>\n",
       "      <td>-0.089218</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>59.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 18:00:00</th>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.552632</td>\n",
       "      <td>-0.531915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.982615</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>-0.123288</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.929475</td>\n",
       "      <td>0.170868</td>\n",
       "      <td>-0.722197</td>\n",
       "      <td>0.279634</td>\n",
       "      <td>0.379523</td>\n",
       "      <td>-0.929617</td>\n",
       "      <td>-0.808371</td>\n",
       "      <td>0.025057</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>63.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:00:00</th>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.421053</td>\n",
       "      <td>-0.212766</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.982615</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.099099</td>\n",
       "      <td>-0.068493</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.975735</td>\n",
       "      <td>0.165266</td>\n",
       "      <td>-0.707221</td>\n",
       "      <td>0.349637</td>\n",
       "      <td>0.380023</td>\n",
       "      <td>-0.982404</td>\n",
       "      <td>-0.820738</td>\n",
       "      <td>0.108656</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>65.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 20:00:00</th>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>-0.956522</td>\n",
       "      <td>0.984701</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.135135</td>\n",
       "      <td>-0.068493</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.982543</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>-0.686617</td>\n",
       "      <td>0.474826</td>\n",
       "      <td>0.380516</td>\n",
       "      <td>-0.990663</td>\n",
       "      <td>-0.827185</td>\n",
       "      <td>0.116097</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>65.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 21:00:00</th>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.984701</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.189189</td>\n",
       "      <td>-0.068493</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.986908</td>\n",
       "      <td>0.187675</td>\n",
       "      <td>-0.660988</td>\n",
       "      <td>0.441034</td>\n",
       "      <td>0.372255</td>\n",
       "      <td>-0.994614</td>\n",
       "      <td>-0.839552</td>\n",
       "      <td>0.035991</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>61.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 22:00:00</th>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.984701</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.225225</td>\n",
       "      <td>-0.068493</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.987082</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>-0.647419</td>\n",
       "      <td>0.306622</td>\n",
       "      <td>0.250963</td>\n",
       "      <td>-0.996589</td>\n",
       "      <td>-0.820209</td>\n",
       "      <td>-0.072134</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 23:00:00</th>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>-0.869565</td>\n",
       "      <td>0.986787</td>\n",
       "      <td>-0.864407</td>\n",
       "      <td>-0.261261</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.990224</td>\n",
       "      <td>0.170868</td>\n",
       "      <td>-0.645510</td>\n",
       "      <td>0.126852</td>\n",
       "      <td>0.055721</td>\n",
       "      <td>-0.997127</td>\n",
       "      <td>-0.812176</td>\n",
       "      <td>-0.233333</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>58.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     temp_madrid_lag  dew_point_madrid_lag  \\\n",
       "2015-01-02 00:00:00            -0.58             -0.368421   \n",
       "2015-01-02 01:00:00            -0.58             -0.368421   \n",
       "2015-01-02 02:00:00            -0.64             -0.368421   \n",
       "2015-01-02 03:00:00            -0.64             -0.315789   \n",
       "2015-01-02 04:00:00            -0.68             -0.368421   \n",
       "2015-01-02 05:00:00            -0.68             -0.421053   \n",
       "2015-01-02 06:00:00            -0.72             -0.473684   \n",
       "2015-01-02 07:00:00            -0.68             -0.421053   \n",
       "2015-01-02 08:00:00            -0.72             -0.421053   \n",
       "2015-01-02 09:00:00            -0.76             -0.473684   \n",
       "2015-01-02 10:00:00            -0.64             -0.421053   \n",
       "2015-01-02 11:00:00            -0.54             -0.368421   \n",
       "2015-01-02 12:00:00            -0.44             -0.315789   \n",
       "2015-01-02 13:00:00            -0.28             -0.289474   \n",
       "2015-01-02 14:00:00            -0.18             -0.236842   \n",
       "2015-01-02 15:00:00            -0.14             -0.421053   \n",
       "2015-01-02 16:00:00            -0.10             -0.526316   \n",
       "2015-01-02 17:00:00            -0.14             -0.526316   \n",
       "2015-01-02 18:00:00            -0.18             -0.552632   \n",
       "2015-01-02 19:00:00            -0.28             -0.421053   \n",
       "2015-01-02 20:00:00            -0.44             -0.368421   \n",
       "2015-01-02 21:00:00            -0.50             -0.368421   \n",
       "2015-01-02 22:00:00            -0.54             -0.368421   \n",
       "2015-01-02 23:00:00            -0.54             -0.368421   \n",
       "\n",
       "                     humidities_madrid_lag  wind_madrid_lag  \\\n",
       "2015-01-02 00:00:00               0.574468         0.000000   \n",
       "2015-01-02 01:00:00               0.574468         0.444444   \n",
       "2015-01-02 02:00:00               0.851064         0.222222   \n",
       "2015-01-02 03:00:00               1.000000         0.555556   \n",
       "2015-01-02 04:00:00               1.000000        -1.000000   \n",
       "2015-01-02 05:00:00               0.851064        -0.555556   \n",
       "2015-01-02 06:00:00               0.851064        -0.555556   \n",
       "2015-01-02 07:00:00               0.851064        -0.222222   \n",
       "2015-01-02 08:00:00               1.000000        -1.000000   \n",
       "2015-01-02 09:00:00               1.000000        -0.555556   \n",
       "2015-01-02 10:00:00               0.702128        -0.555556   \n",
       "2015-01-02 11:00:00               0.468085         0.555556   \n",
       "2015-01-02 12:00:00               0.255319        -0.222222   \n",
       "2015-01-02 13:00:00               0.000000         0.555556   \n",
       "2015-01-02 14:00:00              -0.127660        -0.222222   \n",
       "2015-01-02 15:00:00              -0.425532         0.000000   \n",
       "2015-01-02 16:00:00              -0.574468        -1.000000   \n",
       "2015-01-02 17:00:00              -0.531915         0.000000   \n",
       "2015-01-02 18:00:00              -0.531915         0.000000   \n",
       "2015-01-02 19:00:00              -0.212766         0.333333   \n",
       "2015-01-02 20:00:00               0.148936         0.555556   \n",
       "2015-01-02 21:00:00               0.361702        -0.555556   \n",
       "2015-01-02 22:00:00               0.468085        -0.555556   \n",
       "2015-01-02 23:00:00               0.468085        -0.222222   \n",
       "\n",
       "                     wind_speeds_madrid_lag  pressures_madrid_lag  \\\n",
       "2015-01-02 00:00:00               -0.913043              0.974965   \n",
       "2015-01-02 01:00:00               -0.869565              0.977051   \n",
       "2015-01-02 02:00:00               -0.913043              0.977051   \n",
       "2015-01-02 03:00:00               -0.956522              0.977051   \n",
       "2015-01-02 04:00:00               -1.000000              0.977051   \n",
       "2015-01-02 05:00:00               -0.869565              0.979138   \n",
       "2015-01-02 06:00:00               -0.913043              0.979138   \n",
       "2015-01-02 07:00:00               -0.869565              0.979138   \n",
       "2015-01-02 08:00:00               -1.000000              0.980529   \n",
       "2015-01-02 09:00:00               -0.869565              0.982615   \n",
       "2015-01-02 10:00:00               -0.869565              0.984701   \n",
       "2015-01-02 11:00:00               -0.956522              0.984701   \n",
       "2015-01-02 12:00:00               -0.782609              0.984701   \n",
       "2015-01-02 13:00:00               -0.956522              0.984701   \n",
       "2015-01-02 14:00:00               -0.956522              0.982615   \n",
       "2015-01-02 15:00:00               -0.869565              0.980529   \n",
       "2015-01-02 16:00:00               -1.000000              0.980529   \n",
       "2015-01-02 17:00:00               -0.913043              0.980529   \n",
       "2015-01-02 18:00:00               -0.869565              0.982615   \n",
       "2015-01-02 19:00:00               -0.869565              0.982615   \n",
       "2015-01-02 20:00:00               -0.956522              0.984701   \n",
       "2015-01-02 21:00:00               -0.869565              0.984701   \n",
       "2015-01-02 22:00:00               -0.869565              0.984701   \n",
       "2015-01-02 23:00:00               -0.869565              0.986787   \n",
       "\n",
       "                     condition_madrid_lag  temp_seville_lag  \\\n",
       "2015-01-02 00:00:00             -0.864407         -0.333333   \n",
       "2015-01-02 01:00:00             -0.864407         -0.333333   \n",
       "2015-01-02 02:00:00             -0.864407         -0.339339   \n",
       "2015-01-02 03:00:00             -0.864407         -0.345345   \n",
       "2015-01-02 04:00:00             -0.864407         -0.351351   \n",
       "2015-01-02 05:00:00             -0.864407         -0.333333   \n",
       "2015-01-02 06:00:00             -0.864407         -0.333333   \n",
       "2015-01-02 07:00:00             -0.864407         -0.351351   \n",
       "2015-01-02 08:00:00             -0.864407         -0.351351   \n",
       "2015-01-02 09:00:00             -0.864407         -0.387387   \n",
       "2015-01-02 10:00:00             -0.864407         -0.297297   \n",
       "2015-01-02 11:00:00             -0.864407         -0.171171   \n",
       "2015-01-02 12:00:00             -0.864407         -0.063063   \n",
       "2015-01-02 13:00:00             -0.864407         -0.009009   \n",
       "2015-01-02 14:00:00             -0.864407          0.063063   \n",
       "2015-01-02 15:00:00             -0.864407          0.063063   \n",
       "2015-01-02 16:00:00             -0.864407          0.099099   \n",
       "2015-01-02 17:00:00             -0.864407          0.063063   \n",
       "2015-01-02 18:00:00             -0.864407          0.027027   \n",
       "2015-01-02 19:00:00             -0.864407         -0.099099   \n",
       "2015-01-02 20:00:00             -0.864407         -0.135135   \n",
       "2015-01-02 21:00:00             -0.864407         -0.189189   \n",
       "2015-01-02 22:00:00             -0.864407         -0.225225   \n",
       "2015-01-02 23:00:00             -0.864407         -0.261261   \n",
       "\n",
       "                     dew_point_seville_lag  humidities_seville_lag  ...  \\\n",
       "2015-01-02 00:00:00              -0.123288                    0.62  ...   \n",
       "2015-01-02 01:00:00              -0.123288                    0.62  ...   \n",
       "2015-01-02 02:00:00              -0.123288                    0.66  ...   \n",
       "2015-01-02 03:00:00              -0.123288                    0.70  ...   \n",
       "2015-01-02 04:00:00              -0.123288                    0.74  ...   \n",
       "2015-01-02 05:00:00              -0.178082                    0.50  ...   \n",
       "2015-01-02 06:00:00              -0.178082                    0.50  ...   \n",
       "2015-01-02 07:00:00              -0.178082                    0.62  ...   \n",
       "2015-01-02 08:00:00              -0.232877                    0.50  ...   \n",
       "2015-01-02 09:00:00              -0.232877                    0.62  ...   \n",
       "2015-01-02 10:00:00              -0.123288                    0.50  ...   \n",
       "2015-01-02 11:00:00              -0.123288                    0.14  ...   \n",
       "2015-01-02 12:00:00              -0.068493                    0.00  ...   \n",
       "2015-01-02 13:00:00              -0.232877                   -0.28  ...   \n",
       "2015-01-02 14:00:00              -0.260274                   -0.42  ...   \n",
       "2015-01-02 15:00:00              -0.178082                   -0.32  ...   \n",
       "2015-01-02 16:00:00              -0.260274                   -0.46  ...   \n",
       "2015-01-02 17:00:00              -0.232877                   -0.38  ...   \n",
       "2015-01-02 18:00:00              -0.123288                   -0.24  ...   \n",
       "2015-01-02 19:00:00              -0.068493                    0.08  ...   \n",
       "2015-01-02 20:00:00              -0.068493                    0.14  ...   \n",
       "2015-01-02 21:00:00              -0.068493                    0.32  ...   \n",
       "2015-01-02 22:00:00              -0.068493                    0.40  ...   \n",
       "2015-01-02 23:00:00              -0.013699                    0.62  ...   \n",
       "\n",
       "                     solar_lag  waste_lag  onshore_lag  load_forecast  \\\n",
       "2015-01-02 00:00:00  -0.991446   0.098039    -0.358963      -0.347672   \n",
       "2015-01-02 01:00:00  -0.991272   0.092437    -0.408010      -0.519050   \n",
       "2015-01-02 02:00:00  -0.991272   0.098039    -0.451128      -0.628288   \n",
       "2015-01-02 03:00:00  -0.991272   0.070028    -0.473541      -0.682567   \n",
       "2015-01-02 04:00:00  -0.992668   0.058824    -0.503995      -0.707590   \n",
       "2015-01-02 05:00:00  -0.994065   0.053221    -0.535856      -0.660493   \n",
       "2015-01-02 06:00:00  -0.994065   0.042017    -0.558068      -0.524493   \n",
       "2015-01-02 07:00:00  -0.993890   0.058824    -0.598774      -0.315921   \n",
       "2015-01-02 08:00:00  -0.990573   0.109244    -0.635258      -0.088373   \n",
       "2015-01-02 09:00:00  -0.870298   0.109244    -0.691140       0.155806   \n",
       "2015-01-02 10:00:00  -0.647552   0.142857    -0.730338       0.291881   \n",
       "2015-01-02 11:00:00  -0.441913   0.154062    -0.721494       0.262398   \n",
       "2015-01-02 12:00:00  -0.321812   0.170868    -0.707925       0.211597   \n",
       "2015-01-02 13:00:00  -0.300515   0.176471    -0.706216       0.210160   \n",
       "2015-01-02 14:00:00  -0.306450   0.176471    -0.704005       0.175083   \n",
       "2015-01-02 15:00:00  -0.333508   0.176471    -0.727725       0.118385   \n",
       "2015-01-02 16:00:00  -0.460941   0.182073    -0.743505       0.087390   \n",
       "2015-01-02 17:00:00  -0.743912   0.165266    -0.728730       0.093967   \n",
       "2015-01-02 18:00:00  -0.929475   0.170868    -0.722197       0.279634   \n",
       "2015-01-02 19:00:00  -0.975735   0.165266    -0.707221       0.349637   \n",
       "2015-01-02 20:00:00  -0.982543   0.176471    -0.686617       0.474826   \n",
       "2015-01-02 21:00:00  -0.986908   0.187675    -0.660988       0.441034   \n",
       "2015-01-02 22:00:00  -0.987082   0.176471    -0.647419       0.306622   \n",
       "2015-01-02 23:00:00  -0.990224   0.170868    -0.645510       0.126852   \n",
       "\n",
       "                     load_actual  solar_forecast  wind_forecast  \\\n",
       "2015-01-02 00:00:00    -0.386552       -0.997666      -0.655745   \n",
       "2015-01-02 01:00:00    -0.578538       -0.998205      -0.687982   \n",
       "2015-01-02 02:00:00    -0.688629       -0.998384      -0.719374   \n",
       "2015-01-02 03:00:00    -0.749394       -0.998025      -0.718317   \n",
       "2015-01-02 04:00:00    -0.767664       -0.998564      -0.734489   \n",
       "2015-01-02 05:00:00    -0.718972       -0.998564      -0.700772   \n",
       "2015-01-02 06:00:00    -0.556932       -0.998205      -0.690413   \n",
       "2015-01-02 07:00:00    -0.150641       -0.996768      -0.693056   \n",
       "2015-01-02 08:00:00     0.052504       -0.988688      -0.690519   \n",
       "2015-01-02 09:00:00     0.154073       -0.875931      -0.717789   \n",
       "2015-01-02 10:00:00     0.255650       -0.677350      -0.759751   \n",
       "2015-01-02 11:00:00     0.253664       -0.447347      -0.819469   \n",
       "2015-01-02 12:00:00     0.317090       -0.359368      -0.867984   \n",
       "2015-01-02 13:00:00     0.348807       -0.358650      -0.891132   \n",
       "2015-01-02 14:00:00     0.364661       -0.376246      -0.880985   \n",
       "2015-01-02 15:00:00     0.372588       -0.473023      -0.874326   \n",
       "2015-01-02 16:00:00     0.376552       -0.572134      -0.847691   \n",
       "2015-01-02 17:00:00     0.378530       -0.782566      -0.834056   \n",
       "2015-01-02 18:00:00     0.379523       -0.929617      -0.808371   \n",
       "2015-01-02 19:00:00     0.380023       -0.982404      -0.820738   \n",
       "2015-01-02 20:00:00     0.380516       -0.990663      -0.827185   \n",
       "2015-01-02 21:00:00     0.372255       -0.994614      -0.839552   \n",
       "2015-01-02 22:00:00     0.250963       -0.996589      -0.820209   \n",
       "2015-01-02 23:00:00     0.055721       -0.997127      -0.812176   \n",
       "\n",
       "                     generation_forecast  consumption_forecast  \\\n",
       "2015-01-02 00:00:00            -0.464617             -1.000000   \n",
       "2015-01-02 01:00:00            -0.566894             -0.898350   \n",
       "2015-01-02 02:00:00            -0.624222             -0.740080   \n",
       "2015-01-02 03:00:00            -0.643508             -0.505127   \n",
       "2015-01-02 04:00:00            -0.647304             -0.497994   \n",
       "2015-01-02 05:00:00            -0.627411             -0.522069   \n",
       "2015-01-02 06:00:00            -0.552012             -0.641551   \n",
       "2015-01-02 07:00:00            -0.407517             -1.000000   \n",
       "2015-01-02 08:00:00            -0.229613             -1.000000   \n",
       "2015-01-02 09:00:00            -0.003341             -1.000000   \n",
       "2015-01-02 10:00:00             0.091268             -1.000000   \n",
       "2015-01-02 11:00:00             0.141382             -1.000000   \n",
       "2015-01-02 12:00:00             0.090281             -1.000000   \n",
       "2015-01-02 13:00:00             0.030752             -1.000000   \n",
       "2015-01-02 14:00:00            -0.040850             -1.000000   \n",
       "2015-01-02 15:00:00            -0.080638             -1.000000   \n",
       "2015-01-02 16:00:00            -0.097950             -1.000000   \n",
       "2015-01-02 17:00:00            -0.089218             -1.000000   \n",
       "2015-01-02 18:00:00             0.025057             -1.000000   \n",
       "2015-01-02 19:00:00             0.108656             -1.000000   \n",
       "2015-01-02 20:00:00             0.116097             -1.000000   \n",
       "2015-01-02 21:00:00             0.035991             -1.000000   \n",
       "2015-01-02 22:00:00            -0.072134             -1.000000   \n",
       "2015-01-02 23:00:00            -0.233333             -1.000000   \n",
       "\n",
       "                     price_day_ahead  \n",
       "2015-01-02 00:00:00            47.34  \n",
       "2015-01-02 01:00:00            40.40  \n",
       "2015-01-02 02:00:00            36.00  \n",
       "2015-01-02 03:00:00            37.00  \n",
       "2015-01-02 04:00:00            35.99  \n",
       "2015-01-02 05:00:00            37.98  \n",
       "2015-01-02 06:00:00            43.61  \n",
       "2015-01-02 07:00:00            56.10  \n",
       "2015-01-02 08:00:00            57.69  \n",
       "2015-01-02 09:00:00            60.01  \n",
       "2015-01-02 10:00:00            62.25  \n",
       "2015-01-02 11:00:00            60.01  \n",
       "2015-01-02 12:00:00            59.97  \n",
       "2015-01-02 13:00:00            59.69  \n",
       "2015-01-02 14:00:00            58.69  \n",
       "2015-01-02 15:00:00            58.13  \n",
       "2015-01-02 16:00:00            59.00  \n",
       "2015-01-02 17:00:00            59.69  \n",
       "2015-01-02 18:00:00            63.76  \n",
       "2015-01-02 19:00:00            65.01  \n",
       "2015-01-02 20:00:00            65.01  \n",
       "2015-01-02 21:00:00            61.75  \n",
       "2015-01-02 22:00:00            59.96  \n",
       "2015-01-02 23:00:00            58.69  \n",
       "\n",
       "[24 rows x 60 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame().append(X_train.iloc[0:24].copy())\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape =  (X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(59, activation='relu', input_shape=input_shape))\n",
    "nn.add(layers.Dense(239, activation='relu'))\n",
    "nn.add(layers.Dense(162, activation='relu'))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 12.5959 - SMAPE: 31.7973 - val_loss: 2.5820 - val_SMAPE: 7.4359\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.0735 - SMAPE: 4.0576 - val_loss: 1.8394 - val_SMAPE: 5.3152\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.8062 - SMAPE: 3.5551 - val_loss: 1.6810 - val_SMAPE: 4.9280\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.8048 - SMAPE: 3.4808 - val_loss: 1.7235 - val_SMAPE: 5.0120\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.6775 - SMAPE: 3.2389 - val_loss: 2.2996 - val_SMAPE: 6.5282\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.6385 - SMAPE: 3.1830 - val_loss: 1.5770 - val_SMAPE: 4.5829\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.6894 - SMAPE: 3.2160 - val_loss: 2.0257 - val_SMAPE: 5.7407\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.5641 - SMAPE: 3.0052 - val_loss: 1.4641 - val_SMAPE: 4.2904\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.5960 - SMAPE: 3.0470 - val_loss: 1.6953 - val_SMAPE: 4.9203\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.5098 - SMAPE: 2.9062 - val_loss: 2.4240 - val_SMAPE: 6.9289\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.4791 - SMAPE: 2.8397 - val_loss: 1.9001 - val_SMAPE: 5.4362\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.4563 - SMAPE: 2.7934 - val_loss: 3.4861 - val_SMAPE: 9.6047\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.7646 - SMAPE: 3.3331 - val_loss: 1.3453 - val_SMAPE: 3.9041\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.5641 - SMAPE: 2.9513 - val_loss: 2.0106 - val_SMAPE: 5.7786\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.4119 - SMAPE: 2.7304 - val_loss: 2.5968 - val_SMAPE: 7.3284\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.5757 - SMAPE: 2.9586 - val_loss: 1.6267 - val_SMAPE: 4.8353\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.3913 - SMAPE: 2.6455 - val_loss: 1.6190 - val_SMAPE: 4.8033\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.4016 - SMAPE: 2.6936 - val_loss: 3.2836 - val_SMAPE: 8.9619\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.5598 - SMAPE: 2.9449 - val_loss: 1.5149 - val_SMAPE: 4.5291\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.3774 - SMAPE: 2.6021 - val_loss: 1.5662 - val_SMAPE: 4.6546\n",
      "Epoch 21/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.3570 - SMAPE: 2.6076 - val_loss: 2.7695 - val_SMAPE: 7.9176\n",
      "Epoch 22/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.4292 - SMAPE: 2.7123 - val_loss: 1.4693 - val_SMAPE: 4.3759\n",
      "Epoch 23/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.3420 - SMAPE: 2.5369 - val_loss: 1.7088 - val_SMAPE: 5.0939\n"
     ]
    }
   ],
   "source": [
    "nn2 = compile_fit(nn, (X_train, y_train), (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSO</th>\n",
       "      <th>nn1</th>\n",
       "      <th>nn2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sMAPE_train</th>\n",
       "      <td>16.030</td>\n",
       "      <td>2.908806</td>\n",
       "      <td>4.032132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sMAPE_val</th>\n",
       "      <td>16.922</td>\n",
       "      <td>4.066236</td>\n",
       "      <td>3.924490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_train</th>\n",
       "      <td>0.954</td>\n",
       "      <td>0.984056</td>\n",
       "      <td>0.979786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_val</th>\n",
       "      <td>0.971</td>\n",
       "      <td>0.977619</td>\n",
       "      <td>0.980031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                TSO       nn1       nn2\n",
       "sMAPE_train  16.030  2.908806  4.032132\n",
       "sMAPE_val    16.922  4.066236  3.924490\n",
       "r2_train      0.954  0.984056  0.979786\n",
       "r2_val        0.971  0.977619  0.980031"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_actual['nn2'] = compute_metrics(nn2, (X_train,y_train), (X_val, y_val))\n",
    "results_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLIDING WINDOW GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_gen(data, input_window, output_window, stride):\n",
    "    '''\n",
    "    Yields train and test samples of the given provided datasets, at specified input and out lengths, and specified strides\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    train: tuple,\n",
    "        Tuple of length 2, which provides the training features and training targets respectively\n",
    "    test: tuple,\n",
    "        Tuple of length 2, which provides the testing features and testing targets respectively\n",
    "    input_window: int,\n",
    "        Length of the sequence of input data\n",
    "    output_window: int,\n",
    "        Length of the sequence of output data\n",
    "    stride, int\n",
    "        Number of steps to move between first sample and second sample\n",
    "    '''\n",
    "    # Define X_train, y_train, X_test, y_test\n",
    "    X, y = data[0], data[1]\n",
    "    \n",
    "    # Compute number of samples \n",
    "    n = len(X)/stride\n",
    "    \n",
    "    #\n",
    "    if input_window > 24:\n",
    "        n_add = input_window - 24\n",
    "        X = X.iloc[:n_add].append(X)\n",
    "    \n",
    "    i=0\n",
    "    for i in range(0, len(X)-n_add, stride):\n",
    "        yield X.iloc[i:i+input_window].to_numpy(), y.iloc[i:i+output_window].to_numpy()\n",
    "    #while i < len(X_train):\n",
    "        #X_sample = X.iloc[i:i+input_window]\n",
    "        #y_sample = y.iloc[i:i+output_window]\n",
    "        #i += stride\n",
    "        #yield X_sample, y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "train = window_gen((X_train, y_train), input_window=24*7, output_window=24, stride=24)\n",
    "val = window_gen((X_val, y_val), input_window=24*7, output_window=24, stride=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(data, input_window, output_window, stride):\n",
    "    win = window_gen((data[0], data[1]), input_window=input_window, output_window=output_window, stride=stride)\n",
    "    \n",
    "    n = int(len(X_train)/stride)\n",
    "    X_data = np.array([])\n",
    "    y_data = np.array([])\n",
    "    \n",
    "    for i in range(n):\n",
    "        X_sample, y_sample = next(win)\n",
    "        X_data = np.append(X_data, X_sample)\n",
    "        y_data = np.append(y_data, y_sample)\n",
    "        \n",
    "    # Reshape\n",
    "    X_data = X_data.reshape(n,input_window,len(data[0].columns))\n",
    "    y_data = y_data.reshape(n, output_window)\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "X_train, y_train = resample((X_train, y_train), 24*7, 24, 24)\n",
    "X_val, y_val = resample((X_val,y_val), 24*7, 24, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1825, 168, 60)\n",
      "(1825, 24)\n",
      "(76, 168, 60)\n",
      "(76, 24)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window=24*7\n",
    "output_window=24\n",
    "n = int(len(X_train)/stride)\n",
    "X_data = np.array([])\n",
    "y_data = np.array([])\n",
    "for i in range(n):\n",
    "    X_sample, y_sample = next(win)\n",
    "    X_data = np.append(X_data, X_sample)\n",
    "    y_data = np.append(y_data, y_sample)\n",
    "X_data = X_data.reshape(n,input_window,len(X_train.columns))\n",
    "y_data = y_data.reshape(n, output_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1825, 168, 60)\n",
      "(1825, 24)\n"
     ]
    }
   ],
   "source": [
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-5.80000000e-01, -3.68421053e-01,  5.74468085e-01, ...,\n",
       "         -4.64616553e-01, -1.00000000e+00,  4.73400000e+01],\n",
       "        [-5.80000000e-01, -3.68421053e-01,  5.74468085e-01, ...,\n",
       "         -5.66894457e-01, -8.98350424e-01,  4.04000000e+01],\n",
       "        [-6.40000000e-01, -3.68421053e-01,  8.51063830e-01, ...,\n",
       "         -6.24221716e-01, -7.40080250e-01,  3.60000000e+01],\n",
       "        ...,\n",
       "        [-5.00000000e-01, -3.68421053e-01,  3.61702128e-01, ...,\n",
       "          3.59908884e-02, -1.00000000e+00,  6.17500000e+01],\n",
       "        [-5.40000000e-01, -3.68421053e-01,  4.68085106e-01, ...,\n",
       "         -7.21336371e-02, -1.00000000e+00,  5.99600000e+01],\n",
       "        [-5.40000000e-01, -3.68421053e-01,  4.68085106e-01, ...,\n",
       "         -2.33333333e-01, -1.00000000e+00,  5.86900000e+01]],\n",
       "\n",
       "       [[-6.20000000e-01, -3.68421053e-01,  7.02127660e-01, ...,\n",
       "         -3.88078967e-01, -9.41150245e-01,  5.21000000e+01],\n",
       "        [-6.20000000e-01, -3.68421053e-01,  7.02127660e-01, ...,\n",
       "         -4.72703113e-01, -6.55595185e-01,  4.78400000e+01],\n",
       "        [-6.40000000e-01, -4.21052632e-01,  7.02127660e-01, ...,\n",
       "         -5.40812453e-01, -5.56843513e-01,  4.36600000e+01],\n",
       "        ...,\n",
       "        [-4.40000000e-01, -2.89473684e-01,  3.61702128e-01, ...,\n",
       "         -1.55277145e-02, -1.00000000e+00,  5.95100000e+01],\n",
       "        [-4.60000000e-01, -2.89473684e-01,  4.68085106e-01, ...,\n",
       "         -1.24487472e-01, -1.00000000e+00,  5.81000000e+01],\n",
       "        [-5.00000000e-01, -2.89473684e-01,  5.95744681e-01, ...,\n",
       "         -2.84776006e-01, -1.00000000e+00,  5.61000000e+01]],\n",
       "\n",
       "       [[-5.40000000e-01, -2.89473684e-01,  7.23404255e-01, ...,\n",
       "         -3.11541382e-01, -8.82300490e-01,  3.98200000e+01],\n",
       "        [-6.20000000e-01, -3.15789474e-01,  8.51063830e-01, ...,\n",
       "         -3.78511769e-01, -4.12839947e-01,  3.48200000e+01],\n",
       "        [-5.80000000e-01, -3.15789474e-01,  7.02127660e-01, ...,\n",
       "         -4.57403189e-01, -3.73606777e-01,  3.40000000e+01],\n",
       "        ...,\n",
       "        [-4.40000000e-01, -2.36842105e-01,  4.68085106e-01, ...,\n",
       "         -6.70463174e-02, -1.00000000e+00,  6.26900000e+01],\n",
       "        [-4.40000000e-01, -2.36842105e-01,  4.68085106e-01, ...,\n",
       "         -1.76841306e-01, -1.00000000e+00,  6.11900000e+01],\n",
       "        [-5.00000000e-01, -2.36842105e-01,  7.23404255e-01, ...,\n",
       "         -3.36218679e-01, -1.00000000e+00,  5.98000000e+01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.00000000e-01,  1.57894737e-01,  4.04255319e-01, ...,\n",
       "         -9.61275626e-02, -1.13241195e-01,  5.02000000e+00],\n",
       "        [-1.00000000e-01,  1.57894737e-01,  4.04255319e-01, ...,\n",
       "         -1.48899013e-01, -9.09496210e-02,  4.52000000e+00],\n",
       "        [-1.40000000e-01,  1.57894737e-01,  4.89361702e-01, ...,\n",
       "         -2.34851936e-01, -4.41373161e-02,  2.30000000e+00],\n",
       "        ...,\n",
       "        [-3.60000000e-01, -1.31578947e-01,  4.89361702e-01, ...,\n",
       "         -2.97342445e-01, -1.00000000e+00,  4.11500000e+01],\n",
       "        [-4.40000000e-01, -1.31578947e-01,  7.23404255e-01, ...,\n",
       "         -3.77524677e-01, -1.00000000e+00,  3.90600000e+01],\n",
       "        [-4.40000000e-01, -1.84210526e-01,  5.95744681e-01, ...,\n",
       "         -4.69096431e-01, -7.53901025e-01,  3.49000000e+01]],\n",
       "\n",
       "       [[-1.40000000e-01, -7.89473684e-02,  2.12765957e-02, ...,\n",
       "         -1.93621868e-01,  3.83860901e-01,  7.00000000e+00],\n",
       "        [-1.80000000e-01, -5.26315789e-02,  1.91489362e-01, ...,\n",
       "         -2.63325740e-01,  4.06152474e-01,  4.62000000e+00],\n",
       "        [-1.80000000e-01, -7.89473684e-02,  1.06382979e-01, ...,\n",
       "         -3.35535308e-01,  5.59518502e-01,  2.30000000e+00],\n",
       "        ...,\n",
       "        [-4.00000000e-01, -1.84210526e-01,  4.68085106e-01, ...,\n",
       "         -2.12148823e-01, -1.00000000e+00,  4.64200000e+01],\n",
       "        [-4.40000000e-01, -1.84210526e-01,  5.95744681e-01, ...,\n",
       "         -3.28170084e-01, -1.00000000e+00,  4.27600000e+01],\n",
       "        [-4.60000000e-01, -1.84210526e-01,  7.23404255e-01, ...,\n",
       "         -4.44419134e-01, -1.00000000e+00,  4.08800000e+01]],\n",
       "\n",
       "       [[-3.60000000e-01, -2.22044605e-16,  8.51063830e-01, ...,\n",
       "         -5.30751708e-01,  1.71199287e-01,  1.22800000e+01],\n",
       "        [-3.60000000e-01, -2.22044605e-16,  8.51063830e-01, ...,\n",
       "         -5.46545178e-01,  3.46411057e-01,  9.99000000e+00],\n",
       "        [-4.00000000e-01, -2.22044605e-16,  1.00000000e+00, ...,\n",
       "         -5.58390281e-01,  6.89701293e-01,  9.32000000e+00],\n",
       "        ...,\n",
       "        [-3.60000000e-01, -7.89473684e-02,  5.95744681e-01, ...,\n",
       "         -4.12604404e-01, -1.00000000e+00,  3.97400000e+01],\n",
       "        [-4.00000000e-01, -7.89473684e-02,  7.23404255e-01, ...,\n",
       "         -4.49962035e-01, -1.00000000e+00,  3.88800000e+01],\n",
       "        [-4.40000000e-01, -1.31578947e-01,  7.23404255e-01, ...,\n",
       "         -5.22019742e-01, -1.00000000e+00,  3.73700000e+01]]])"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.86, 39.15, 36.46, 35.92, 34.98, 37.03, 36.61, 41.8 , 45.29,\n",
       "       47.44, 48.08, 45.47, 45.35, 43.57, 43.81, 45.33, 47.81, 52.21,\n",
       "       56.29, 54.08, 49.67, 46.23, 43.97, 42.44])"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data[1824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019-12-31 00:00:00    41.86\n",
       "2019-12-31 01:00:00    39.15\n",
       "2019-12-31 02:00:00    36.46\n",
       "2019-12-31 03:00:00    35.92\n",
       "2019-12-31 04:00:00    34.98\n",
       "2019-12-31 05:00:00    37.03\n",
       "2019-12-31 06:00:00    36.61\n",
       "2019-12-31 07:00:00    41.80\n",
       "2019-12-31 08:00:00    45.29\n",
       "2019-12-31 09:00:00    47.44\n",
       "2019-12-31 10:00:00    48.08\n",
       "2019-12-31 11:00:00    45.47\n",
       "2019-12-31 12:00:00    45.35\n",
       "2019-12-31 13:00:00    43.57\n",
       "2019-12-31 14:00:00    43.81\n",
       "2019-12-31 15:00:00    45.33\n",
       "2019-12-31 16:00:00    47.81\n",
       "2019-12-31 17:00:00    52.21\n",
       "2019-12-31 18:00:00    56.29\n",
       "2019-12-31 19:00:00    54.08\n",
       "2019-12-31 20:00:00    49.67\n",
       "2019-12-31 21:00:00    46.23\n",
       "2019-12-31 22:00:00    43.97\n",
       "2019-12-31 23:00:00    42.44\n",
       "Name: price_actual, dtype: float64"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.iloc[-24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X_train)/24\n",
    "i = 0 \n",
    "while i < (len(X_train)):\n",
    "    X_train, y_train = next(win)\n",
    "    i += 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 168, 60)"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64.02, 58.46, 54.7 , 54.91, 53.07, 54.23, 58.22, 67.55, 70.33,\n",
       "       71.26, 75.86, 73.65, 74.19, 71.51, 71.04, 71.24, 70.64, 72.85,\n",
       "       82.55, 83.33, 83.23, 79.06, 76.2 , 71.75])"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(60, activation='tanh', input_shape=input_shape))\n",
    "nn.add(layers.RepeatVector(y_train.shape[1]))\n",
    "nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 6s 78ms/step - loss: 36.3296 - SMAPE: 90.4345 - val_loss: 20.1643 - val_SMAPE: 58.4264\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 4s 73ms/step - loss: 34.9471 - SMAPE: 85.1381 - val_loss: 18.8335 - val_SMAPE: 52.9478\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 33.5716 - SMAPE: 80.1440 - val_loss: 17.5436 - val_SMAPE: 47.9317\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 32.1918 - SMAPE: 76.0328 - val_loss: 16.2494 - val_SMAPE: 43.0501\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 4s 70ms/step - loss: 30.8168 - SMAPE: 70.8552 - val_loss: 14.9455 - val_SMAPE: 38.5560\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 29.4629 - SMAPE: 66.4764 - val_loss: 13.7743 - val_SMAPE: 34.5996\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 28.1101 - SMAPE: 62.1691 - val_loss: 12.5838 - val_SMAPE: 30.9997\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 26.7993 - SMAPE: 58.6168 - val_loss: 11.6272 - val_SMAPE: 28.2853\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 25.4909 - SMAPE: 54.1471 - val_loss: 10.6289 - val_SMAPE: 25.4925\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 7s 113ms/step - loss: 24.2145 - SMAPE: 51.2572 - val_loss: 9.7877 - val_SMAPE: 23.4641\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 22.9458 - SMAPE: 47.6889 - val_loss: 9.0522 - val_SMAPE: 21.7260\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 5s 91ms/step - loss: 21.7062 - SMAPE: 44.6531 - val_loss: 8.2370 - val_SMAPE: 19.3217\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 5s 83ms/step - loss: 20.5036 - SMAPE: 41.5608 - val_loss: 7.9664 - val_SMAPE: 19.5234\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 5s 87ms/step - loss: 19.3472 - SMAPE: 38.1283 - val_loss: 7.2955 - val_SMAPE: 17.7339\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 5s 86ms/step - loss: 18.2001 - SMAPE: 35.8352 - val_loss: 7.1749 - val_SMAPE: 17.7569\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 5s 85ms/step - loss: 17.1218 - SMAPE: 33.2693 - val_loss: 6.1403 - val_SMAPE: 14.7220\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 5s 90ms/step - loss: 16.0878 - SMAPE: 30.6669 - val_loss: 6.2023 - val_SMAPE: 15.3830\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 5s 83ms/step - loss: 15.1186 - SMAPE: 28.3553 - val_loss: 5.8238 - val_SMAPE: 14.4083\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 5s 85ms/step - loss: 14.2109 - SMAPE: 26.7502 - val_loss: 6.4864 - val_SMAPE: 16.6333\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 5s 83ms/step - loss: 13.3810 - SMAPE: 24.5053 - val_loss: 5.7515 - val_SMAPE: 14.5098\n",
      "Epoch 21/200\n",
      "58/58 [==============================] - 5s 84ms/step - loss: 12.6633 - SMAPE: 23.2948 - val_loss: 6.1642 - val_SMAPE: 15.3449\n",
      "Epoch 22/200\n",
      "58/58 [==============================] - 5s 85ms/step - loss: 11.9466 - SMAPE: 21.8288 - val_loss: 5.1811 - val_SMAPE: 12.7917\n",
      "Epoch 23/200\n",
      "58/58 [==============================] - 5s 88ms/step - loss: 11.3324 - SMAPE: 20.6364 - val_loss: 5.2374 - val_SMAPE: 13.1742\n",
      "Epoch 24/200\n",
      "58/58 [==============================] - 5s 92ms/step - loss: 10.7694 - SMAPE: 19.5754 - val_loss: 5.2432 - val_SMAPE: 13.1883\n",
      "Epoch 25/200\n",
      "58/58 [==============================] - 5s 84ms/step - loss: 10.2045 - SMAPE: 18.5090 - val_loss: 5.0199 - val_SMAPE: 12.7504\n",
      "Epoch 26/200\n",
      "58/58 [==============================] - 5s 83ms/step - loss: 9.6985 - SMAPE: 17.3355 - val_loss: 4.6316 - val_SMAPE: 11.7536\n",
      "Epoch 27/200\n",
      "58/58 [==============================] - 5s 83ms/step - loss: 9.2750 - SMAPE: 16.6697 - val_loss: 4.5819 - val_SMAPE: 11.8444\n",
      "Epoch 28/200\n",
      "58/58 [==============================] - 5s 83ms/step - loss: 8.8430 - SMAPE: 15.7513 - val_loss: 4.5110 - val_SMAPE: 11.7462\n",
      "Epoch 29/200\n",
      "58/58 [==============================] - 5s 84ms/step - loss: 8.4083 - SMAPE: 15.3966 - val_loss: 4.3351 - val_SMAPE: 11.3069\n",
      "Epoch 30/200\n",
      "58/58 [==============================] - 5s 84ms/step - loss: 8.0438 - SMAPE: 14.4850 - val_loss: 4.2870 - val_SMAPE: 11.3412\n",
      "Epoch 31/200\n",
      "58/58 [==============================] - 5s 83ms/step - loss: 7.6926 - SMAPE: 13.6197 - val_loss: 4.1922 - val_SMAPE: 10.5310\n",
      "Epoch 32/200\n",
      "58/58 [==============================] - 5s 83ms/step - loss: 7.4168 - SMAPE: 13.2188 - val_loss: 3.9462 - val_SMAPE: 10.0497\n",
      "Epoch 33/200\n",
      "58/58 [==============================] - 5s 93ms/step - loss: 7.0256 - SMAPE: 12.5571 - val_loss: 3.9173 - val_SMAPE: 10.1732\n",
      "Epoch 34/200\n",
      "58/58 [==============================] - 5s 84ms/step - loss: 6.7617 - SMAPE: 12.0453 - val_loss: 3.9752 - val_SMAPE: 10.3829\n",
      "Epoch 35/200\n",
      "58/58 [==============================] - 5s 84ms/step - loss: 6.4993 - SMAPE: 11.3824 - val_loss: 3.9560 - val_SMAPE: 10.4440\n",
      "Epoch 36/200\n",
      "58/58 [==============================] - 5s 94ms/step - loss: 6.3445 - SMAPE: 11.1744 - val_loss: 4.0981 - val_SMAPE: 10.4855\n",
      "Epoch 37/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 6.0852 - SMAPE: 10.6714 - val_loss: 3.9435 - val_SMAPE: 10.2956\n",
      "Epoch 38/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 5.8392 - SMAPE: 10.2696 - val_loss: 3.8465 - val_SMAPE: 10.0279\n",
      "Epoch 39/200\n",
      "58/58 [==============================] - 4s 76ms/step - loss: 5.6946 - SMAPE: 10.1608 - val_loss: 3.8238 - val_SMAPE: 10.0479\n",
      "Epoch 40/200\n",
      "58/58 [==============================] - 4s 77ms/step - loss: 5.4624 - SMAPE: 9.8290 - val_loss: 3.7437 - val_SMAPE: 9.6367\n",
      "Epoch 41/200\n",
      "58/58 [==============================] - 4s 77ms/step - loss: 5.5042 - SMAPE: 9.8370 - val_loss: 3.8785 - val_SMAPE: 10.1161\n",
      "Epoch 42/200\n",
      "58/58 [==============================] - 4s 74ms/step - loss: 5.1643 - SMAPE: 9.1470 - val_loss: 3.8785 - val_SMAPE: 10.2917\n",
      "Epoch 43/200\n",
      "58/58 [==============================] - 4s 74ms/step - loss: 4.9844 - SMAPE: 8.9292 - val_loss: 3.7417 - val_SMAPE: 9.8401\n",
      "Epoch 44/200\n",
      "58/58 [==============================] - 4s 73ms/step - loss: 4.8279 - SMAPE: 8.6779 - val_loss: 3.5217 - val_SMAPE: 9.2651\n",
      "Epoch 45/200\n",
      "58/58 [==============================] - 4s 74ms/step - loss: 4.6955 - SMAPE: 8.4078 - val_loss: 3.6450 - val_SMAPE: 9.7455\n",
      "Epoch 46/200\n",
      "58/58 [==============================] - 4s 69ms/step - loss: 4.5510 - SMAPE: 8.0893 - val_loss: 3.6443 - val_SMAPE: 9.6914\n",
      "Epoch 47/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 4.4505 - SMAPE: 7.9745 - val_loss: 3.5805 - val_SMAPE: 9.5203\n",
      "Epoch 48/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 4.3309 - SMAPE: 7.7222 - val_loss: 3.4961 - val_SMAPE: 9.1430\n",
      "Epoch 49/200\n",
      "58/58 [==============================] - 4s 68ms/step - loss: 4.2447 - SMAPE: 7.6016 - val_loss: 3.5958 - val_SMAPE: 9.6854\n",
      "Epoch 50/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 4.2185 - SMAPE: 7.6593 - val_loss: 3.5868 - val_SMAPE: 9.5490\n",
      "Epoch 51/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 4.0614 - SMAPE: 7.3328 - val_loss: 3.5535 - val_SMAPE: 9.5769\n",
      "Epoch 52/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 4.0231 - SMAPE: 7.2556 - val_loss: 3.3835 - val_SMAPE: 8.9074\n",
      "Epoch 53/200\n",
      "58/58 [==============================] - 4s 75ms/step - loss: 4.0079 - SMAPE: 7.3065 - val_loss: 3.4484 - val_SMAPE: 9.2905\n",
      "Epoch 54/200\n",
      "58/58 [==============================] - 4s 73ms/step - loss: 3.7934 - SMAPE: 6.9375 - val_loss: 3.2126 - val_SMAPE: 8.6592\n",
      "Epoch 55/200\n",
      "58/58 [==============================] - 4s 74ms/step - loss: 3.6853 - SMAPE: 6.6446 - val_loss: 3.3727 - val_SMAPE: 9.4113\n",
      "Epoch 56/200\n",
      "58/58 [==============================] - 4s 75ms/step - loss: 3.6430 - SMAPE: 6.6466 - val_loss: 2.9787 - val_SMAPE: 8.3302\n",
      "Epoch 57/200\n",
      "58/58 [==============================] - 4s 76ms/step - loss: 3.5480 - SMAPE: 6.4441 - val_loss: 2.9784 - val_SMAPE: 8.3804\n",
      "Epoch 58/200\n",
      "58/58 [==============================] - 4s 75ms/step - loss: 3.4346 - SMAPE: 6.2547 - val_loss: 2.9409 - val_SMAPE: 8.3915\n",
      "Epoch 59/200\n",
      "58/58 [==============================] - 4s 77ms/step - loss: 3.3901 - SMAPE: 6.1704 - val_loss: 2.7996 - val_SMAPE: 7.8040\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 4s 73ms/step - loss: 3.3489 - SMAPE: 6.0884 - val_loss: 2.7595 - val_SMAPE: 7.9028\n",
      "Epoch 61/200\n",
      "58/58 [==============================] - 4s 77ms/step - loss: 3.2531 - SMAPE: 5.9628 - val_loss: 2.7972 - val_SMAPE: 7.9425\n",
      "Epoch 62/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 3.1685 - SMAPE: 5.7660 - val_loss: 2.8648 - val_SMAPE: 7.7766\n",
      "Epoch 63/200\n",
      "58/58 [==============================] - 5s 79ms/step - loss: 3.1326 - SMAPE: 5.6924 - val_loss: 2.6653 - val_SMAPE: 7.5211\n",
      "Epoch 64/200\n",
      "58/58 [==============================] - 5s 79ms/step - loss: 3.1023 - SMAPE: 6.0401 - val_loss: 2.7468 - val_SMAPE: 7.8495\n",
      "Epoch 65/200\n",
      "58/58 [==============================] - 5s 79ms/step - loss: 2.9881 - SMAPE: 5.4535 - val_loss: 2.6445 - val_SMAPE: 7.3407\n",
      "Epoch 66/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 3.0316 - SMAPE: 5.5991 - val_loss: 3.0784 - val_SMAPE: 8.6580\n",
      "Epoch 67/200\n",
      "58/58 [==============================] - 5s 79ms/step - loss: 2.9393 - SMAPE: 5.3780 - val_loss: 2.6556 - val_SMAPE: 7.4777\n",
      "Epoch 68/200\n",
      "58/58 [==============================] - 5s 80ms/step - loss: 2.9215 - SMAPE: 5.3674 - val_loss: 2.5258 - val_SMAPE: 7.2218\n",
      "Epoch 69/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 2.8906 - SMAPE: 5.3070 - val_loss: 2.6383 - val_SMAPE: 7.4897\n",
      "Epoch 70/200\n",
      "58/58 [==============================] - 5s 84ms/step - loss: 2.8161 - SMAPE: 5.2774 - val_loss: 2.5106 - val_SMAPE: 7.0375\n",
      "Epoch 71/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 2.7927 - SMAPE: 5.1380 - val_loss: 2.9384 - val_SMAPE: 7.7677\n",
      "Epoch 72/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 2.8159 - SMAPE: 5.1340 - val_loss: 2.7020 - val_SMAPE: 7.6519\n",
      "Epoch 73/200\n",
      "58/58 [==============================] - 5s 85ms/step - loss: 2.7110 - SMAPE: 4.9945 - val_loss: 2.4864 - val_SMAPE: 6.8417\n",
      "Epoch 74/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 2.6561 - SMAPE: 4.8635 - val_loss: 2.4430 - val_SMAPE: 6.8712\n",
      "Epoch 75/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 2.6358 - SMAPE: 4.8222 - val_loss: 2.7393 - val_SMAPE: 7.5119\n",
      "Epoch 76/200\n",
      "58/58 [==============================] - 5s 83ms/step - loss: 2.6316 - SMAPE: 4.8335 - val_loss: 2.3846 - val_SMAPE: 6.4674\n",
      "Epoch 77/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 2.5805 - SMAPE: 4.8244 - val_loss: 2.4714 - val_SMAPE: 6.8828\n",
      "Epoch 78/200\n",
      "58/58 [==============================] - 5s 84ms/step - loss: 2.5177 - SMAPE: 4.6703 - val_loss: 2.3910 - val_SMAPE: 6.6921\n",
      "Epoch 79/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 2.5228 - SMAPE: 4.6596 - val_loss: 2.5402 - val_SMAPE: 7.0658\n",
      "Epoch 80/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 2.4659 - SMAPE: 4.5278 - val_loss: 2.5676 - val_SMAPE: 7.1944\n",
      "Epoch 81/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 2.4142 - SMAPE: 4.4368 - val_loss: 2.7748 - val_SMAPE: 7.6615\n",
      "Epoch 82/200\n",
      "58/58 [==============================] - 5s 87ms/step - loss: 2.4520 - SMAPE: 4.5222 - val_loss: 2.5674 - val_SMAPE: 7.0563\n",
      "Epoch 83/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 2.4040 - SMAPE: 4.4202 - val_loss: 2.4844 - val_SMAPE: 6.8274\n",
      "Epoch 84/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 2.3589 - SMAPE: 4.3570 - val_loss: 2.3604 - val_SMAPE: 6.2928\n",
      "Epoch 85/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 2.4180 - SMAPE: 4.4574 - val_loss: 2.4448 - val_SMAPE: 6.9138\n",
      "Epoch 86/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 2.3288 - SMAPE: 4.3199 - val_loss: 2.3005 - val_SMAPE: 6.3539\n",
      "Epoch 87/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 2.3132 - SMAPE: 4.2487 - val_loss: 2.3456 - val_SMAPE: 6.4748\n",
      "Epoch 88/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 2.3181 - SMAPE: 4.2890 - val_loss: 2.3722 - val_SMAPE: 6.5599\n",
      "Epoch 89/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 2.3266 - SMAPE: 4.3351 - val_loss: 2.5687 - val_SMAPE: 7.0551\n",
      "Epoch 90/200\n",
      "58/58 [==============================] - 5s 88ms/step - loss: 2.3216 - SMAPE: 4.2774 - val_loss: 2.4026 - val_SMAPE: 6.4420\n",
      "Epoch 91/200\n",
      "58/58 [==============================] - 5s 90ms/step - loss: 2.2818 - SMAPE: 4.2539 - val_loss: 2.6080 - val_SMAPE: 7.3109\n",
      "Epoch 92/200\n",
      "58/58 [==============================] - 5s 84ms/step - loss: 2.2504 - SMAPE: 4.2074 - val_loss: 2.3660 - val_SMAPE: 6.5268\n",
      "Epoch 93/200\n",
      "58/58 [==============================] - 5s 80ms/step - loss: 2.3048 - SMAPE: 4.2642 - val_loss: 2.6300 - val_SMAPE: 7.2179\n",
      "Epoch 94/200\n",
      "58/58 [==============================] - 4s 77ms/step - loss: 2.2016 - SMAPE: 4.0851 - val_loss: 2.2948 - val_SMAPE: 6.4654\n"
     ]
    }
   ],
   "source": [
    "nn3 = compile_fit(nn, (X_train, y_train), (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 24, 1)"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = nn3.predict(X_val)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSO</th>\n",
       "      <th>nn1</th>\n",
       "      <th>nn2</th>\n",
       "      <th>nn3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sMAPE_train</th>\n",
       "      <td>16.030</td>\n",
       "      <td>2.908806</td>\n",
       "      <td>4.032132</td>\n",
       "      <td>4.819614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sMAPE_val</th>\n",
       "      <td>16.922</td>\n",
       "      <td>4.066236</td>\n",
       "      <td>3.924490</td>\n",
       "      <td>6.263882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_train</th>\n",
       "      <td>0.954</td>\n",
       "      <td>0.984056</td>\n",
       "      <td>0.979786</td>\n",
       "      <td>0.937627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_val</th>\n",
       "      <td>0.971</td>\n",
       "      <td>0.977619</td>\n",
       "      <td>0.980031</td>\n",
       "      <td>0.908647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                TSO       nn1       nn2       nn3\n",
       "sMAPE_train  16.030  2.908806  4.032132  4.819614\n",
       "sMAPE_val    16.922  4.066236  3.924490  6.263882\n",
       "r2_train      0.954  0.984056  0.979786  0.937627\n",
       "r2_val        0.971  0.977619  0.980031  0.908647"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_actual['nn3'] = compute_metrics(nn3,(X_train,y_train), (X_val,y_val))\n",
    "results_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer repeat_vector_16 is incompatible with the layer: expected ndim=2, found ndim=3. Full shape received: (None, 168, 83)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-724-b48ac55f77f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m83\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRepeatVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    215\u001b[0m       \u001b[1;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m       \u001b[1;31m# refresh its output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m       \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    977\u001b[0m                                                 input_list)\n\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[0;32m   1113\u001b[0m       \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[0;32m   1115\u001b[0m           inputs, input_masks, args, kwargs)\n\u001b[0;32m   1116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    884\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 886\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2631\u001b[0m     \u001b[1;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2632\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2633\u001b[1;33m       input_spec.assert_input_compatibility(\n\u001b[0m\u001b[0;32m   2634\u001b[0m           self.input_spec, inputs, self.name)\n\u001b[0;32m   2635\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    212\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0m\u001b[0;32m    215\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer repeat_vector_16 is incompatible with the layer: expected ndim=2, found ndim=3. Full shape received: (None, 168, 83)"
     ]
    }
   ],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(83, activation='tanh', input_shape=input_shape, return_sequences=True))\n",
    "nn.add(layers.RepeatVector(y_train.shape[1]))\n",
    "nn.add(layers.Dense(24, activation='relu'))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn4 = compile_fit(nn, (X_train, y_train), (X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENCODER/DECODER LSTM\n",
    "\n",
    "**ENCODER**:<br>\n",
    "One or more LSTM layers can be used to implement the encoder model. The output is a fixed-size vector that represents the internal represntation of the input sequence. The number of memory cells in this layer defines the length of this fixed-size vector.\n",
    "* Produces a two dimensional matrix of outputs, where the length is defined by the number of memory cells in the layer.\n",
    "\n",
    "**DECODER**:\n",
    "Must transform the learned interal representation of the input sequence into the correct output sequence. The output should be a TimeDistributed layer.\n",
    "* Expects a 3D input of [samples, time steps, features] in order to produce a decoded sequence of some different length define by the problem\n",
    "\n",
    "**Repeat Vector**:\n",
    "* Repeats the provided 2D input multiple times to create a 3D output\n",
    "* Use to transition from ENCODER - DECORDER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 24)                8160      \n",
      "_________________________________________________________________\n",
      "repeat_vector_11 (RepeatVect (None, 24, 24)            0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 24, 24)            4704      \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 24, 1)             25        \n",
      "=================================================================\n",
      "Total params: 12,889\n",
      "Trainable params: 12,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 24)"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for index in X_train4.index:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN- LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the price_drop cols\n",
    "data = df_lag.drop(columns=price_drop)\n",
    "\n",
    "# Split the data into train and validation\n",
    "X_train, y_train, X_val, y_val = split_data(data, 2020, 'price_actual')\n",
    "\n",
    "# Subset dnn cols\n",
    "X_train_dnn = X_train.filter(regex='lag')\n",
    "X_val_dnn = X_val.filter(regex='lag')\n",
    "\n",
    "# Subset lstm cols\n",
    "X_train_lstm = X_train.drop(columns=X_train_dnn.columns)\n",
    "X_val_lstm = X_val.drop(columns=X_val_dnn.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dnn, y_train, X_val_dnn, y_val = create_windows((X_train_dnn, y_train), (X_val_dnn, y_val), 24*7, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 24, 53)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 24)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input_shape\n",
    "input_shape = (X_train_dnn.shape[1], X_train_dnn.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(59, activation='relu', input_shape=input_shape))\n",
    "nn.add(layers.Dense(239, activation='relu'))\n",
    "nn.add(layers.Dense(162, activation='relu'))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 1s 7ms/step - loss: 7.1709 - SMAPE: 13.7999 - val_loss: 15.7936 - val_SMAPE: 36.2453\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 7.2504 - SMAPE: 13.7528 - val_loss: 13.1679 - val_SMAPE: 31.7116\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 7.1908 - SMAPE: 13.6106 - val_loss: 14.4294 - val_SMAPE: 33.9231\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 7.1933 - SMAPE: 13.8220 - val_loss: 12.6163 - val_SMAPE: 30.7177\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 7.0261 - SMAPE: 13.6109 - val_loss: 14.1698 - val_SMAPE: 33.4584\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.9741 - SMAPE: 13.1447 - val_loss: 14.0560 - val_SMAPE: 33.2725\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.8338 - SMAPE: 13.0065 - val_loss: 12.5395 - val_SMAPE: 30.5537\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.8483 - SMAPE: 13.0011 - val_loss: 15.6024 - val_SMAPE: 35.8631\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.8289 - SMAPE: 13.5874 - val_loss: 11.8585 - val_SMAPE: 29.3006\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.8745 - SMAPE: 13.2036 - val_loss: 13.5305 - val_SMAPE: 32.2988\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.8723 - SMAPE: 13.0343 - val_loss: 13.4693 - val_SMAPE: 32.2015\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.6854 - SMAPE: 12.7991 - val_loss: 14.7757 - val_SMAPE: 34.4903\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.7741 - SMAPE: 12.8028 - val_loss: 12.6372 - val_SMAPE: 30.6623\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.6906 - SMAPE: 12.8134 - val_loss: 13.4758 - val_SMAPE: 32.1868\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.6669 - SMAPE: 12.6352 - val_loss: 12.6718 - val_SMAPE: 30.7420\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.8156 - SMAPE: 12.8574 - val_loss: 15.3274 - val_SMAPE: 35.2869\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.7119 - SMAPE: 12.7457 - val_loss: 14.3600 - val_SMAPE: 33.7234\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.5393 - SMAPE: 12.7184 - val_loss: 14.7336 - val_SMAPE: 34.3090\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 6.5423 - SMAPE: 12.3682 - val_loss: 14.1878 - val_SMAPE: 33.3201\n"
     ]
    }
   ],
   "source": [
    "dnn = compile_fit(nn, (X_train_dnn,y_train), (X_val_dnn, y_val), patience=10,\n",
    "                  loss = tf.keras.metrics.mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.258809300502557,\n",
       " 29.424358739029838,\n",
       " 0.5939931779008499,\n",
       " 0.14228398023713856]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(dnn, (X_train_dnn, y_train), (X_val_dnn, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['load_forecast', 'load_actual', 'solar_forecast', 'wind_forecast',\n",
       "       'generation_forecast', 'consumption_forecast', 'price_actual',\n",
       "       'price_day_ahead'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lstm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nn2.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43800,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds.shape)\n",
    "train_shape = y_train.shape[0] * y_train.shape[1]\n",
    "preds = preds.reshape(train_shape)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1825, 24, 59)\n",
      "(366, 24, 59)\n"
     ]
    }
   ],
   "source": [
    "X_train_test = create_windows(train, 24)\n",
    "X_val_test = create_windows(X_val, 24)\n",
    "X_\n",
    "print(X_train_test.shape)\n",
    "print(X_val_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 24, 59)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1824, 24, 59) (1824, 24)\n",
      "(365, 24, 59) (365, 24)\n",
      "(24, 59)\n"
     ]
    }
   ],
   "source": [
    "# Convert sliced arrays into input/output sequences\n",
    "X_train, y_train = to_supervised(train, n_input=window_len, n_out=window_len, stride=24)\n",
    "X_val, y_val = to_supervised(val, n_input=window_len, n_out=window_len, stride=24)\n",
    "\n",
    "input_shape =  (X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40.83, 38.07, 37.14, ..., 35.21, 35.6 , 39.28],\n",
       "       [36.76, 35.14, 33.78, ..., 33.11, 33.61, 35.23],\n",
       "       [41.18, 35.74, 34.93, ..., 33.81, 33.73, 33.75],\n",
       "       ...,\n",
       "       [37.1 , 29.31, 24.83, ..., 24.69, 26.94, 33.29],\n",
       "       [47.76, 44.29, 40.69, ..., 37.92, 40.4 , 47.6 ],\n",
       "       [46.62, 42.43, 39.47, ..., 35.8 , 38.39, 41.38]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43800, 74)\n",
      "(8784, 74)\n",
      "\n",
      "(43800, 75)\n",
      "(8784, 75)\n",
      "\n",
      "(1825, 24, 75)\n",
      "(366, 24, 75)\n"
     ]
    }
   ],
   "source": [
    "train['price_actual'] = df_lag.loc[:'2019', 'price_actual']\n",
    "val['price_actual'] = df_lag.loc['2020', 'price_actual']\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print()\n",
    "\n",
    "train = np.array(np.split(train, len(train)/24))\n",
    "val = np.array(np.split(val, len(val)/24))\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "\n",
    "\n",
    "#X_train, y_train = to_supervised(train, n_input=24, n_out=24, stride=24)\n",
    "#X_val, y_val = to_supervised(val, n_input=24, n_out=24, stride=24)\n",
    "#input_shape=(X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and build layers\n",
    "nn_final = models.Sequential()\n",
    "nn_final.add(layers.Dense(62, activation='relu', input_shape=(shape,)))\n",
    "nn_final.add(layers.Dense(239, activation='relu'))\n",
    "nn_final.add(layers.Dense(162, activation='relu'))\n",
    "nn_final.add(layers.Dense(1, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1644/1644 [==============================] - 3s 1ms/step - loss: 1.9733 - sMAPE: 4.2737 - val_loss: 3.5257 - val_sMAPE: 3.9302\n",
      "Epoch 2/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.5054 - sMAPE: 3.0898 - val_loss: 2.5294 - val_sMAPE: 3.1163\n",
      "Epoch 3/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.4663 - sMAPE: 3.0000 - val_loss: 2.7105 - val_sMAPE: 3.2777\n",
      "Epoch 4/200\n",
      "1644/1644 [==============================] - 3s 2ms/step - loss: 1.4028 - sMAPE: 2.8682 - val_loss: 2.4926 - val_sMAPE: 3.0372\n",
      "Epoch 5/200\n",
      "1644/1644 [==============================] - 3s 2ms/step - loss: 1.3379 - sMAPE: 2.7306 - val_loss: 2.8074 - val_sMAPE: 3.3143\n",
      "Epoch 6/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.3100 - sMAPE: 2.6720 - val_loss: 2.6923 - val_sMAPE: 3.1800\n",
      "Epoch 7/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.2757 - sMAPE: 2.6006 - val_loss: 2.9182 - val_sMAPE: 3.5041\n",
      "Epoch 8/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.2648 - sMAPE: 2.5793 - val_loss: 2.5996 - val_sMAPE: 3.1444\n",
      "Epoch 9/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.2446 - sMAPE: 2.5322 - val_loss: 2.8880 - val_sMAPE: 3.6045\n",
      "Epoch 10/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.2276 - sMAPE: 2.4953 - val_loss: 2.6306 - val_sMAPE: 3.1107\n",
      "Epoch 11/200\n",
      "1644/1644 [==============================] - 3s 2ms/step - loss: 1.2230 - sMAPE: 2.4855 - val_loss: 2.6101 - val_sMAPE: 3.0049\n",
      "Epoch 12/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.2051 - sMAPE: 2.4430 - val_loss: 2.5430 - val_sMAPE: 3.0304\n",
      "Epoch 13/200\n",
      "1644/1644 [==============================] - 2s 2ms/step - loss: 1.1892 - sMAPE: 2.4096 - val_loss: 2.5868 - val_sMAPE: 3.0398\n",
      "Epoch 14/200\n",
      "1644/1644 [==============================] - 3s 2ms/step - loss: 1.1855 - sMAPE: 2.3972 - val_loss: 2.6644 - val_sMAPE: 3.0783\n",
      "Epoch 15/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.1723 - sMAPE: 2.3668 - val_loss: 3.3081 - val_sMAPE: 3.5299\n",
      "Epoch 16/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.1636 - sMAPE: 2.3514 - val_loss: 2.9295 - val_sMAPE: 3.2304\n",
      "Epoch 17/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.1527 - sMAPE: 2.3298 - val_loss: 2.4448 - val_sMAPE: 3.0907\n",
      "Epoch 18/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.1450 - sMAPE: 2.3083 - val_loss: 4.1177 - val_sMAPE: 3.8777\n",
      "Epoch 19/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.1398 - sMAPE: 2.2941 - val_loss: 2.4973 - val_sMAPE: 3.1724\n",
      "Epoch 20/200\n",
      "1644/1644 [==============================] - 2s 2ms/step - loss: 1.1376 - sMAPE: 2.2866 - val_loss: 2.9828 - val_sMAPE: 3.3189\n",
      "Epoch 21/200\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 1.1289 - sMAPE: 2.2669 - val_loss: 2.4456 - val_sMAPE: 3.1116\n",
      "r2: 0.9975249037005292\n",
      "mape: 2.9504125262141203\n",
      "smape: 3.004926558162059\n",
      "smape: 3.004925489425659\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = split_data(df, 2021)\n",
    "\n",
    "bl_final = compile_fit(nn_final, X_train, y_train, (X_val, y_val),\n",
    "                patience=10,\n",
    "                loss = tf.keras.metrics.mean_absolute_error)\n",
    "preds = bl_final.predict(X_val).flatten()\n",
    "r2 = (np.corrcoef(y_val, preds)**2)[0][1]\n",
    "print('r2:', r2)\n",
    "print('mape:', mean_absolute_percentage_error(y_val, preds)*100)\n",
    "print('smape:', SMAPE(y_val, preds))\n",
    "loss, sMAPE_val = bl_final.evaluate(X_val, y_val, verbose=0)\n",
    "print('smape:', sMAPE_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.9714777867123521\n",
      "mape: 14.959628546355278\n",
      "smape: 16.918003049222687\n"
     ]
    }
   ],
   "source": [
    "print('r2:', (np.corrcoef(y_val,TSO_preds.loc['2020'])**2)[0][1])\n",
    "print('mape:', mean_absolute_percentage_error(y_val,TSO_preds.loc['2020'])*100)\n",
    "print('smape:', SMAPE(y_val,TSO_preds.loc['2020']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.9974031129868238\n",
      "mape: 3.210796692591586\n",
      "smape: 3.334922035726982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2021-01-01 00:00:00    -0.131090\n",
       "2021-01-01 01:00:00    -2.204759\n",
       "2021-01-01 02:00:00    -1.673647\n",
       "2021-01-01 03:00:00    -1.716557\n",
       "2021-01-01 04:00:00    -2.175416\n",
       "                         ...    \n",
       "2021-12-30 19:00:00    -7.747296\n",
       "2021-12-30 20:00:00    -5.384048\n",
       "2021-12-30 21:00:00    -9.977943\n",
       "2021-12-30 22:00:00   -13.360194\n",
       "2021-12-30 23:00:00   -17.658956\n",
       "Name: price_tomorrow, Length: 8736, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_data(df, 2021)\n",
    "preds = bl.predict(X_test).flatten()\n",
    "r2 = (np.corrcoef(y_test, preds)**2)[0][1]\n",
    "print('r2:', r2)\n",
    "print('mape:', mean_absolute_percentage_error(y_test, preds)*100)\n",
    "print('smape:', SMAPE(y_test, preds))\n",
    "resid_forecast = preds - y_test\n",
    "resid_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 42.3649 - MAPE: 83.3904 - val_loss: 24.8675 - val_MAPE: 111.7421\n",
      "Epoch 2/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 24.3124 - MAPE: 68.8397 - val_loss: 23.3587 - val_MAPE: 100.0323\n",
      "Epoch 3/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 23.2809 - MAPE: 62.0716 - val_loss: 22.6162 - val_MAPE: 96.2855\n",
      "Epoch 4/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 22.5349 - MAPE: 60.1233 - val_loss: 21.8751 - val_MAPE: 94.0755\n",
      "Epoch 5/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 21.7326 - MAPE: 58.6938 - val_loss: 21.1528 - val_MAPE: 95.9708\n",
      "Epoch 6/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 20.7999 - MAPE: 56.7986 - val_loss: 20.1292 - val_MAPE: 94.3731\n",
      "Epoch 7/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 19.6020 - MAPE: 55.6993 - val_loss: 18.8427 - val_MAPE: 95.4855\n",
      "Epoch 8/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 18.0284 - MAPE: 55.1644 - val_loss: 17.1238 - val_MAPE: 97.9483\n",
      "Epoch 9/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 15.9196 - MAPE: 54.7906 - val_loss: 14.7550 - val_MAPE: 102.0337\n",
      "Epoch 10/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 13.2180 - MAPE: 51.2085 - val_loss: 12.0199 - val_MAPE: 101.4767\n",
      "Epoch 11/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 10.3763 - MAPE: 49.6144 - val_loss: 9.5851 - val_MAPE: 98.7602\n",
      "Epoch 12/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 8.7122 - MAPE: 46.6954 - val_loss: 8.6697 - val_MAPE: 97.0760\n",
      "Epoch 13/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 8.1930 - MAPE: 43.9931 - val_loss: 8.4811 - val_MAPE: 94.9215\n",
      "Epoch 14/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.9654 - MAPE: 43.5136 - val_loss: 8.1589 - val_MAPE: 90.2864\n",
      "Epoch 15/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.8089 - MAPE: 42.7593 - val_loss: 8.0142 - val_MAPE: 89.7980\n",
      "Epoch 16/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.6934 - MAPE: 41.9618 - val_loss: 7.9013 - val_MAPE: 88.4938\n",
      "Epoch 17/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.5989 - MAPE: 41.5502 - val_loss: 7.8481 - val_MAPE: 87.7954\n",
      "Epoch 18/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.5309 - MAPE: 41.0745 - val_loss: 7.7562 - val_MAPE: 86.6834\n",
      "Epoch 19/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.4656 - MAPE: 40.6785 - val_loss: 7.6819 - val_MAPE: 84.7859\n",
      "Epoch 20/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.4101 - MAPE: 39.7905 - val_loss: 7.6691 - val_MAPE: 81.1435\n",
      "Epoch 21/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.3568 - MAPE: 39.2496 - val_loss: 7.5660 - val_MAPE: 80.5407\n",
      "Epoch 22/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.2990 - MAPE: 38.6698 - val_loss: 7.7036 - val_MAPE: 79.6300\n",
      "Epoch 23/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.2576 - MAPE: 38.8286 - val_loss: 7.4625 - val_MAPE: 80.0509\n",
      "Epoch 24/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 7.2288 - MAPE: 38.1678 - val_loss: 7.4232 - val_MAPE: 78.8882\n",
      "Epoch 25/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.1803 - MAPE: 37.4694 - val_loss: 7.4292 - val_MAPE: 77.5186\n",
      "Epoch 26/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.1486 - MAPE: 37.3799 - val_loss: 7.3322 - val_MAPE: 77.5184\n",
      "Epoch 27/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.1084 - MAPE: 36.7988 - val_loss: 7.2989 - val_MAPE: 76.1484\n",
      "Epoch 28/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.0835 - MAPE: 36.3635 - val_loss: 7.3226 - val_MAPE: 75.1726\n",
      "Epoch 29/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.0651 - MAPE: 36.2957 - val_loss: 7.2739 - val_MAPE: 73.4240\n",
      "Epoch 30/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.0198 - MAPE: 35.7371 - val_loss: 7.2384 - val_MAPE: 75.0487\n",
      "Epoch 31/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 7.0082 - MAPE: 35.1625 - val_loss: 7.1809 - val_MAPE: 72.6405\n",
      "Epoch 32/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.9696 - MAPE: 35.1348 - val_loss: 7.1824 - val_MAPE: 71.2904\n",
      "Epoch 33/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.9537 - MAPE: 34.8566 - val_loss: 7.1167 - val_MAPE: 72.5389\n",
      "Epoch 34/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.9284 - MAPE: 34.6932 - val_loss: 7.1074 - val_MAPE: 71.3433\n",
      "Epoch 35/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.9039 - MAPE: 34.0939 - val_loss: 7.1225 - val_MAPE: 70.4945\n",
      "Epoch 36/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.8834 - MAPE: 33.7701 - val_loss: 7.0510 - val_MAPE: 70.5089\n",
      "Epoch 37/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.8546 - MAPE: 33.9122 - val_loss: 7.0219 - val_MAPE: 69.8152\n",
      "Epoch 38/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.8386 - MAPE: 33.7979 - val_loss: 7.0223 - val_MAPE: 69.0464\n",
      "Epoch 39/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.8219 - MAPE: 33.4144 - val_loss: 7.0060 - val_MAPE: 69.2989\n",
      "Epoch 40/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.7946 - MAPE: 33.2060 - val_loss: 6.9942 - val_MAPE: 69.2769\n",
      "Epoch 41/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.7828 - MAPE: 33.3226 - val_loss: 7.0542 - val_MAPE: 65.7990\n",
      "Epoch 42/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.7783 - MAPE: 32.8779 - val_loss: 7.1822 - val_MAPE: 70.3645\n",
      "Epoch 43/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.7420 - MAPE: 32.6554 - val_loss: 6.9402 - val_MAPE: 65.4686\n",
      "Epoch 44/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.7299 - MAPE: 32.4850 - val_loss: 6.9198 - val_MAPE: 66.3519\n",
      "Epoch 45/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.7226 - MAPE: 32.0699 - val_loss: 6.9207 - val_MAPE: 66.7538\n",
      "Epoch 46/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.7057 - MAPE: 31.9391 - val_loss: 6.8919 - val_MAPE: 65.0622\n",
      "Epoch 47/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.6940 - MAPE: 32.2783 - val_loss: 6.8846 - val_MAPE: 64.9862\n",
      "Epoch 48/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.6783 - MAPE: 31.8771 - val_loss: 6.8648 - val_MAPE: 64.4338\n",
      "Epoch 49/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.6574 - MAPE: 31.8197 - val_loss: 6.8577 - val_MAPE: 64.0338\n",
      "Epoch 50/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.6539 - MAPE: 31.3227 - val_loss: 6.8536 - val_MAPE: 63.9932\n",
      "Epoch 51/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.6412 - MAPE: 31.3444 - val_loss: 6.8501 - val_MAPE: 61.9919\n",
      "Epoch 52/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.6313 - MAPE: 31.3233 - val_loss: 6.8431 - val_MAPE: 62.7841\n",
      "Epoch 53/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.6231 - MAPE: 31.1422 - val_loss: 6.8473 - val_MAPE: 63.4425\n",
      "Epoch 54/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.6058 - MAPE: 30.8358 - val_loss: 6.8286 - val_MAPE: 61.0420\n",
      "Epoch 55/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.5982 - MAPE: 30.8334 - val_loss: 6.8026 - val_MAPE: 61.4593\n",
      "Epoch 56/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.5916 - MAPE: 30.9894 - val_loss: 6.8237 - val_MAPE: 61.3918\n",
      "Epoch 57/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.5716 - MAPE: 30.6630 - val_loss: 6.8124 - val_MAPE: 60.9818\n",
      "Epoch 58/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.5660 - MAPE: 30.4998 - val_loss: 6.9284 - val_MAPE: 61.6341\n",
      "Epoch 59/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.5516 - MAPE: 30.5000 - val_loss: 6.7768 - val_MAPE: 60.4217\n",
      "Epoch 60/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.5560 - MAPE: 30.3107 - val_loss: 6.8020 - val_MAPE: 62.3260\n",
      "Epoch 61/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.5424 - MAPE: 30.3945 - val_loss: 6.7678 - val_MAPE: 62.0723\n",
      "Epoch 62/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.5374 - MAPE: 30.6727 - val_loss: 6.7714 - val_MAPE: 59.7517\n",
      "Epoch 63/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.5297 - MAPE: 30.2308 - val_loss: 6.7659 - val_MAPE: 60.9871\n",
      "Epoch 64/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.5278 - MAPE: 29.9674 - val_loss: 6.7834 - val_MAPE: 60.2495\n",
      "Epoch 65/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.5087 - MAPE: 29.9064 - val_loss: 6.8491 - val_MAPE: 62.4007\n",
      "Epoch 66/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4993 - MAPE: 30.2195 - val_loss: 6.8062 - val_MAPE: 59.5190\n",
      "Epoch 67/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.5032 - MAPE: 29.7893 - val_loss: 6.7676 - val_MAPE: 59.4298\n",
      "Epoch 68/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4919 - MAPE: 29.7104 - val_loss: 6.8400 - val_MAPE: 61.5431\n",
      "Epoch 69/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4850 - MAPE: 29.8398 - val_loss: 6.7751 - val_MAPE: 60.6694\n",
      "Epoch 70/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4809 - MAPE: 29.8614 - val_loss: 6.7039 - val_MAPE: 59.1688\n",
      "Epoch 71/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4613 - MAPE: 29.4125 - val_loss: 6.7092 - val_MAPE: 59.6436\n",
      "Epoch 72/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4535 - MAPE: 29.4374 - val_loss: 6.7075 - val_MAPE: 59.5057\n",
      "Epoch 73/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4601 - MAPE: 29.4059 - val_loss: 6.7589 - val_MAPE: 59.0167\n",
      "Epoch 74/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4417 - MAPE: 29.4695 - val_loss: 6.7228 - val_MAPE: 58.3572\n",
      "Epoch 75/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4350 - MAPE: 29.0912 - val_loss: 6.6886 - val_MAPE: 58.5827\n",
      "Epoch 76/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4262 - MAPE: 29.6614 - val_loss: 6.7035 - val_MAPE: 59.3901\n",
      "Epoch 77/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4217 - MAPE: 29.1754 - val_loss: 6.6997 - val_MAPE: 58.0888\n",
      "Epoch 78/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4166 - MAPE: 29.3208 - val_loss: 6.6984 - val_MAPE: 60.0224\n",
      "Epoch 79/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4208 - MAPE: 29.3119 - val_loss: 6.8473 - val_MAPE: 61.9790\n",
      "Epoch 80/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.4007 - MAPE: 29.2533 - val_loss: 6.6630 - val_MAPE: 58.9944\n",
      "Epoch 81/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3976 - MAPE: 28.8626 - val_loss: 6.6561 - val_MAPE: 58.6392\n",
      "Epoch 82/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3905 - MAPE: 28.9708 - val_loss: 6.6758 - val_MAPE: 59.2342\n",
      "Epoch 83/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.3878 - MAPE: 29.0871 - val_loss: 6.6814 - val_MAPE: 57.8697\n",
      "Epoch 84/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3821 - MAPE: 28.8556 - val_loss: 6.6534 - val_MAPE: 58.6794\n",
      "Epoch 85/100\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 6.3681 - MAPE: 29.1229 - val_loss: 6.6986 - val_MAPE: 57.9934\n",
      "Epoch 86/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3773 - MAPE: 28.8967 - val_loss: 6.6810 - val_MAPE: 58.9872\n",
      "Epoch 87/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3687 - MAPE: 28.7754 - val_loss: 6.6315 - val_MAPE: 58.5093\n",
      "Epoch 88/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3585 - MAPE: 28.7532 - val_loss: 6.6602 - val_MAPE: 56.8734\n",
      "Epoch 89/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3566 - MAPE: 28.6196 - val_loss: 6.6946 - val_MAPE: 57.1142\n",
      "Epoch 90/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3500 - MAPE: 28.9529 - val_loss: 6.7473 - val_MAPE: 60.9010\n",
      "Epoch 91/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3485 - MAPE: 28.8765 - val_loss: 6.6696 - val_MAPE: 57.3434\n",
      "Epoch 92/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3348 - MAPE: 28.5160 - val_loss: 6.6542 - val_MAPE: 58.7095\n",
      "Epoch 93/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3291 - MAPE: 28.7331 - val_loss: 6.6715 - val_MAPE: 59.4932\n",
      "Epoch 94/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3255 - MAPE: 28.3685 - val_loss: 6.6056 - val_MAPE: 57.7105\n",
      "Epoch 95/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3149 - MAPE: 28.3867 - val_loss: 6.6400 - val_MAPE: 58.1340\n",
      "Epoch 96/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3123 - MAPE: 28.3650 - val_loss: 6.5967 - val_MAPE: 57.9205\n",
      "Epoch 97/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3117 - MAPE: 28.3174 - val_loss: 6.6021 - val_MAPE: 58.0523\n",
      "Epoch 98/100\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 6.3077 - MAPE: 28.6687 - val_loss: 6.6058 - val_MAPE: 58.6624\n",
      "r2: 0.9276014251521141\n",
      "r2: 0.927929047762599\n",
      "val r2: 0.9198971676998733\n",
      "val r2: 0.9202833278256821\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(239, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "nn.add(layers.Dense(162, activation='relu'))\n",
    "nn.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "# Loss Metric to optimize\n",
    "metric = tf.keras.metrics.MeanAbsolutePercentageError(name='MAPE')\n",
    "\n",
    "# Create early stopping point\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    monitor='val_'+metric.name,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "nn.compile(loss=tf.keras.metrics.mean_absolute_error, \n",
    "           optimizer=keras.optimizers.Adam(learning_rate=.0001),\n",
    "           metrics=metric)\n",
    "\n",
    "# Fit the model\n",
    "history = nn.fit(x= X_train,\n",
    "                 y=y_train,\n",
    "                 batch_size=128,\n",
    "                 epochs = 100,\n",
    "                 callbacks=[callback],\n",
    "                 validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "preds_train = nn.predict(X_train).flatten()\n",
    "preds_val = nn.predict(X_val).flatten()\n",
    "\n",
    "print('r2:',r2_score(y_train, preds_train))\n",
    "print('r2:',(np.corrcoef(y_train, preds_train)**2)[0][1])\n",
    "\n",
    "print('val r2:',r2_score(y_val, preds_val))\n",
    "print('val r2:',(np.corrcoef(y_val, preds_val)**2)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, y_train, X_val, y_val = split_data(df, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 40.7536 - MAPE: 82.2737 - val_loss: 29.4587 - val_MAPE: 112.3087\n",
      "Epoch 2/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 27.4505 - MAPE: 78.2185 - val_loss: 25.6727 - val_MAPE: 95.6277\n",
      "Epoch 3/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 24.9472 - MAPE: 65.1849 - val_loss: 23.9091 - val_MAPE: 81.0519\n",
      "Epoch 4/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 23.1896 - MAPE: 55.6207 - val_loss: 22.2045 - val_MAPE: 68.1717\n",
      "Epoch 5/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 21.1731 - MAPE: 47.3178 - val_loss: 20.1566 - val_MAPE: 54.0342\n",
      "Epoch 6/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 18.9979 - MAPE: 40.0940 - val_loss: 18.0588 - val_MAPE: 41.6691\n",
      "Epoch 7/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 16.8930 - MAPE: 35.7444 - val_loss: 16.0439 - val_MAPE: 28.6478\n",
      "Epoch 8/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 15.1438 - MAPE: 30.5226 - val_loss: 14.5786 - val_MAPE: 27.8606\n",
      "Epoch 9/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 13.9179 - MAPE: 26.9165 - val_loss: 13.5892 - val_MAPE: 27.8606\n",
      "Epoch 10/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 12.9888 - MAPE: 25.1765 - val_loss: 12.8144 - val_MAPE: 27.8122\n",
      "Epoch 11/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 12.2912 - MAPE: 24.2078 - val_loss: 12.2748 - val_MAPE: 28.4628\n",
      "Epoch 12/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 11.7065 - MAPE: 23.6774 - val_loss: 11.6196 - val_MAPE: 27.6600\n",
      "Epoch 13/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 11.1763 - MAPE: 23.1183 - val_loss: 11.1782 - val_MAPE: 27.5312\n",
      "Epoch 14/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 10.7344 - MAPE: 22.6043 - val_loss: 10.7316 - val_MAPE: 26.9204\n",
      "Epoch 15/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 10.3438 - MAPE: 22.1033 - val_loss: 10.3997 - val_MAPE: 26.3158\n",
      "Epoch 16/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 10.0517 - MAPE: 21.6522 - val_loss: 10.1462 - val_MAPE: 25.7484\n",
      "Epoch 17/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 9.8160 - MAPE: 21.3405 - val_loss: 9.9062 - val_MAPE: 25.3580\n",
      "Epoch 18/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 9.6133 - MAPE: 21.0099 - val_loss: 9.7292 - val_MAPE: 24.9279\n",
      "Epoch 19/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 9.4583 - MAPE: 20.7242 - val_loss: 9.6942 - val_MAPE: 25.4181\n",
      "Epoch 20/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 9.3152 - MAPE: 20.5124 - val_loss: 9.4351 - val_MAPE: 24.4504\n",
      "Epoch 21/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 9.1801 - MAPE: 20.3550 - val_loss: 9.3490 - val_MAPE: 24.3628\n",
      "Epoch 22/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 9.0684 - MAPE: 20.1993 - val_loss: 9.2047 - val_MAPE: 23.9875\n",
      "Epoch 23/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 8.9724 - MAPE: 20.0312 - val_loss: 9.1108 - val_MAPE: 23.6877\n",
      "Epoch 24/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 8.8656 - MAPE: 19.9131 - val_loss: 9.2001 - val_MAPE: 24.3516\n",
      "Epoch 25/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 8.7769 - MAPE: 19.7466 - val_loss: 8.9469 - val_MAPE: 23.2960\n",
      "Epoch 26/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 8.7080 - MAPE: 19.7666 - val_loss: 8.8771 - val_MAPE: 22.9268\n",
      "Epoch 27/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 8.6240 - MAPE: 19.6435 - val_loss: 8.8044 - val_MAPE: 22.7795\n",
      "Epoch 28/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 8.5434 - MAPE: 19.5744 - val_loss: 8.7291 - val_MAPE: 22.7590\n",
      "Epoch 29/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 8.4950 - MAPE: 19.5380 - val_loss: 8.7017 - val_MAPE: 22.2686\n",
      "Epoch 30/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 8.4232 - MAPE: 19.4365 - val_loss: 8.6345 - val_MAPE: 22.6142\n",
      "Epoch 31/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 8.3491 - MAPE: 19.3261 - val_loss: 8.5958 - val_MAPE: 21.9382\n",
      "Epoch 32/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 8.3103 - MAPE: 19.3082 - val_loss: 8.5487 - val_MAPE: 21.9117\n",
      "Epoch 33/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 8.2492 - MAPE: 19.2034 - val_loss: 8.4698 - val_MAPE: 21.7320\n",
      "Epoch 34/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 8.1977 - MAPE: 19.1609 - val_loss: 8.4323 - val_MAPE: 21.8003\n",
      "Epoch 35/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 8.1484 - MAPE: 19.1129 - val_loss: 8.3870 - val_MAPE: 21.4005\n",
      "Epoch 36/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 8.0963 - MAPE: 19.0505 - val_loss: 8.3454 - val_MAPE: 21.3163\n",
      "Epoch 37/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 8.0437 - MAPE: 19.0040 - val_loss: 8.3047 - val_MAPE: 21.6043\n",
      "Epoch 38/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.9958 - MAPE: 18.9216 - val_loss: 8.3014 - val_MAPE: 21.5781\n",
      "Epoch 39/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 7.9561 - MAPE: 18.8510 - val_loss: 8.2580 - val_MAPE: 21.4213\n",
      "Epoch 40/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 7.9145 - MAPE: 18.8321 - val_loss: 8.1935 - val_MAPE: 21.0726\n",
      "Epoch 41/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.8928 - MAPE: 18.7315 - val_loss: 8.1438 - val_MAPE: 21.0992\n",
      "Epoch 42/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.8325 - MAPE: 18.7069 - val_loss: 8.1229 - val_MAPE: 20.7696\n",
      "Epoch 43/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 7.7965 - MAPE: 18.6477 - val_loss: 8.0787 - val_MAPE: 20.7263\n",
      "Epoch 44/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.7698 - MAPE: 18.5658 - val_loss: 8.0547 - val_MAPE: 20.5772\n",
      "Epoch 45/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.7230 - MAPE: 18.5042 - val_loss: 8.0491 - val_MAPE: 21.0018\n",
      "Epoch 46/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.6850 - MAPE: 18.4188 - val_loss: 8.0274 - val_MAPE: 20.9373\n",
      "Epoch 47/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.6603 - MAPE: 18.4194 - val_loss: 7.9705 - val_MAPE: 20.6897\n",
      "Epoch 48/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.6240 - MAPE: 18.3446 - val_loss: 7.9530 - val_MAPE: 20.4681\n",
      "Epoch 49/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.6067 - MAPE: 18.3013 - val_loss: 7.9335 - val_MAPE: 20.3875\n",
      "Epoch 50/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.5485 - MAPE: 18.1828 - val_loss: 7.9223 - val_MAPE: 20.6084\n",
      "Epoch 51/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.5266 - MAPE: 18.1456 - val_loss: 7.8869 - val_MAPE: 20.2596\n",
      "Epoch 52/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 7.4931 - MAPE: 18.0600 - val_loss: 7.8346 - val_MAPE: 20.2826\n",
      "Epoch 53/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 7.4582 - MAPE: 18.0342 - val_loss: 7.9018 - val_MAPE: 20.7868\n",
      "Epoch 54/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.4358 - MAPE: 18.0795 - val_loss: 7.8161 - val_MAPE: 20.3873\n",
      "Epoch 55/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.4071 - MAPE: 18.0717 - val_loss: 7.7940 - val_MAPE: 19.8725\n",
      "Epoch 56/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.3895 - MAPE: 18.0416 - val_loss: 7.8774 - val_MAPE: 20.6360\n",
      "Epoch 57/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.3646 - MAPE: 18.0279 - val_loss: 7.7230 - val_MAPE: 20.0570\n",
      "Epoch 58/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.3152 - MAPE: 18.2198 - val_loss: 7.7175 - val_MAPE: 19.9138\n",
      "Epoch 59/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.2891 - MAPE: 18.1002 - val_loss: 7.6955 - val_MAPE: 20.1433\n",
      "Epoch 60/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.2682 - MAPE: 18.5793 - val_loss: 7.6573 - val_MAPE: 19.8791\n",
      "Epoch 61/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.2467 - MAPE: 18.6973 - val_loss: 7.6629 - val_MAPE: 20.2029\n",
      "Epoch 62/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.2197 - MAPE: 18.8502 - val_loss: 7.6726 - val_MAPE: 20.4380\n",
      "Epoch 63/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.2109 - MAPE: 18.7489 - val_loss: 7.5994 - val_MAPE: 19.9233\n",
      "Epoch 64/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.1774 - MAPE: 19.1197 - val_loss: 7.5866 - val_MAPE: 19.6452\n",
      "Epoch 65/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.1480 - MAPE: 19.1100 - val_loss: 7.5547 - val_MAPE: 20.4095\n",
      "Epoch 66/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.1294 - MAPE: 19.3869 - val_loss: 7.5422 - val_MAPE: 21.3500\n",
      "Epoch 67/100\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 7.0959 - MAPE: 19.4419 - val_loss: 7.5143 - val_MAPE: 22.1001\n",
      "Epoch 68/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.0748 - MAPE: 19.7706 - val_loss: 7.5100 - val_MAPE: 22.6776\n",
      "Epoch 69/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.0520 - MAPE: 19.8156 - val_loss: 7.5079 - val_MAPE: 23.9820\n",
      "Epoch 70/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.0295 - MAPE: 20.0480 - val_loss: 7.5602 - val_MAPE: 25.2349\n",
      "Epoch 71/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.0250 - MAPE: 20.0263 - val_loss: 7.4748 - val_MAPE: 26.1673\n",
      "Epoch 72/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 7.0008 - MAPE: 20.3853 - val_loss: 7.4440 - val_MAPE: 26.4878\n",
      "Epoch 73/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 6.9779 - MAPE: 20.4908 - val_loss: 7.4201 - val_MAPE: 27.1134\n",
      "Epoch 74/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 6.9821 - MAPE: 20.6863 - val_loss: 7.4667 - val_MAPE: 27.2669\n",
      "r2: 0.9269175386415136\n",
      "r2: 0.9271853803859837\n",
      "val r2: 0.9165620880865135\n",
      "val r2: 0.916804032657446\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(239, activation='relu', input_shape=(X_train.shape[1],),\n",
    "                    kernel_regularizer=regularizers.L1(.0001)))\n",
    "nn.add(layers.Dense(162, activation='relu', kernel_regularizer=regularizers.L1(.0001)))\n",
    "nn.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "# Loss Metric to optimize\n",
    "metric = tf.keras.metrics.MeanAbsolutePercentageError(name='MAPE')\n",
    "\n",
    "# Create early stopping point\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    monitor='val_'+metric.name,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "nn.compile(loss=tf.keras.metrics.mean_absolute_error, \n",
    "           optimizer=keras.optimizers.Adam(learning_rate=.0001),\n",
    "           metrics=metric)\n",
    "\n",
    "# Fit the model\n",
    "history = nn.fit(x= X_train,\n",
    "                 y=y_train,\n",
    "                 batch_size=256,\n",
    "                 epochs = 100,\n",
    "                 callbacks=[callback],\n",
    "                 validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "preds_train = nn.predict(X_train).flatten()\n",
    "preds_val = nn.predict(X_val).flatten()\n",
    "\n",
    "print('r2:',r2_score(y_train, preds_train))\n",
    "print('r2:',(np.corrcoef(y_train, preds_train)**2)[0][1])\n",
    "\n",
    "print('val r2:',r2_score(y_val, preds_val))\n",
    "print('val r2:',(np.corrcoef(y_val, preds_val)**2)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network - 24 Hour Prediction \n",
    "Two hidden layers\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(train, n_input, n_out=7, stride=1):\n",
    "    # flatten data\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            X.append(data[in_start:in_end, :-1])\n",
    "            y.append(data[in_end:out_end, -1])\n",
    "        # move along one time step\n",
    "        in_start += stride\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1826, 24, 62)\n",
      "(366, 24, 62)\n"
     ]
    }
   ],
   "source": [
    "train = df.loc[:'2019'].drop(columns='price_tomorrow')\n",
    "val = df.loc['2020'].drop(columns='price_tomorrow')\n",
    "\n",
    "train['price_tomorrow'] = df.loc[:'2019', 'price_tomorrow']\n",
    "val['price_tomorrow'] = df.loc['2020', 'price_tomorrow']\n",
    "\n",
    "train = np.array(np.split(train, len(train)/24))\n",
    "val = np.array(np.split(val, len(val)/24))\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "\n",
    "\n",
    "X_train, y_train = to_supervised(train, n_input=24, n_out=24, stride=24)\n",
    "X_val, y_val = to_supervised(val, n_input=24, n_out=24, stride=24)\n",
    "input_shape=(X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and build layers\n",
    "bm = models.Sequential()\n",
    "bm.add(layers.Dense(62, activation='relu', input_shape=input_shape))\n",
    "bm.add(layers.Dense(239, activation='relu'))\n",
    "bm.add(layers.Dense(162, activation='relu'))\n",
    "bm.add(TimeDistributed(layers.Dense(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* <u>monitor_metric & metric (MAPE)</u>:  chosen because sMAPE not available. Chance to hard code sMAPE and implement?\n",
    "* <u>patience (10)</u>: chosen to prevent reaching local minimum\n",
    "* <u>loss function (mean_absolute_error)</u>: chosen because as the electricity prices have large spikes, the Euclidean norm would put too much importance on the spiky prices\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "58/58 [==============================] - 1s 6ms/step - loss: 12.8354 - MAPE: 23.8156 - val_loss: 6.6339 - val_MAPE: 20.9957\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 6.2299 - MAPE: 12.9851 - val_loss: 6.3519 - val_MAPE: 20.7919\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 5.9734 - MAPE: 12.6865 - val_loss: 7.3727 - val_MAPE: 24.7458\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 5.9424 - MAPE: 12.5946 - val_loss: 8.1275 - val_MAPE: 27.1244\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 5.9886 - MAPE: 12.7219 - val_loss: 7.2862 - val_MAPE: 24.4366\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 5.7854 - MAPE: 12.3688 - val_loss: 5.4870 - val_MAPE: 18.0086\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 5.7725 - MAPE: 12.2448 - val_loss: 6.0984 - val_MAPE: 20.8754\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 5.7683 - MAPE: 12.3319 - val_loss: 7.2579 - val_MAPE: 24.3321\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 5.7905 - MAPE: 12.2621 - val_loss: 6.1431 - val_MAPE: 21.0078\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 5.6658 - MAPE: 12.0444 - val_loss: 8.5846 - val_MAPE: 29.3594\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 5.7717 - MAPE: 12.2813 - val_loss: 6.1409 - val_MAPE: 21.3960\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 5.7693 - MAPE: 12.3194 - val_loss: 6.4291 - val_MAPE: 22.0588\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 5.6331 - MAPE: 11.9967 - val_loss: 5.7046 - val_MAPE: 19.3274\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 5.6766 - MAPE: 12.0926 - val_loss: 6.1183 - val_MAPE: 21.0723\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 5.5682 - MAPE: 11.8354 - val_loss: 6.9164 - val_MAPE: 24.0128\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 5.6465 - MAPE: 11.9898 - val_loss: 5.8647 - val_MAPE: 20.0411\n",
      "train r2: 0.652412158831897\n",
      "train r2: 0.672250990806971\n",
      "val r2: 0.5840653925835957\n",
      "val r2: 0.6189845842045377\n"
     ]
    }
   ],
   "source": [
    "# Loss Metric to optimize\n",
    "metric = tf.keras.metrics.MeanAbsolutePercentageError(name='MAPE')\n",
    "\n",
    "# Create early stopping point\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    monitor='val_'+metric.name,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "bm.compile(loss=tf.keras.metrics.mean_absolute_error, \n",
    "           optimizer=keras.optimizers.Adam(),\n",
    "           metrics=metric)\n",
    "\n",
    "# Fit the model\n",
    "history = bm.fit(x= X_train,\n",
    "                 y=y_train,\n",
    "                 epochs = 100,\n",
    "                 callbacks=[callback],\n",
    "                 batch_size=32,\n",
    "                 validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "preds_train = bm.predict(X_train).flatten()\n",
    "preds_val = bm.predict(X_val).flatten()\n",
    "\n",
    "\n",
    "print('train r2:',r2_score(y_train.flatten(), preds_train))\n",
    "print('train r2:',(np.corrcoef(y_train.flatten(), preds_train)**2)[0][1])\n",
    "\n",
    "print('val r2:',r2_score(y_val.flatten(), preds_val))\n",
    "print('val r2:',(np.corrcoef(y_val.flatten(), preds_val)**2)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and build layers\n",
    "bm = models.Sequential()\n",
    "bm.add(layers.Dense(239, activation='relu', input_shape=input_shape, kernel_regularizer=regularizers.l1(0.9)))\n",
    "bm.add(layers.Dropout(.9))\n",
    "bm.add(layers.Dense(162, activation='relu', kernel_regularizer=regularizers.l1(0.9)))\n",
    "bm.add(layers.Dropout(.9))\n",
    "bm.add(TimeDistributed(layers.Dense(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 1s 123ms/step - loss: 3070.5112 - MAPE: 101.8340 - val_loss: 2922.1626 - val_MAPE: 101.8104\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2883.5691 - MAPE: 101.3586 - val_loss: 2739.7991 - val_MAPE: 101.1662\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 2703.2878 - MAPE: 100.7100 - val_loss: 2563.8469 - val_MAPE: 100.5940\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 2529.2197 - MAPE: 100.2305 - val_loss: 2394.1724 - val_MAPE: 100.0971\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 2361.3186 - MAPE: 99.6174 - val_loss: 2230.2959 - val_MAPE: 99.6700\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 2199.3940 - MAPE: 99.2076 - val_loss: 2072.3784 - val_MAPE: 99.3002\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 2043.2935 - MAPE: 98.9438 - val_loss: 1920.2661 - val_MAPE: 98.9757\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1893.2133 - MAPE: 98.7547 - val_loss: 1774.1318 - val_MAPE: 98.6878\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 1748.9343 - MAPE: 98.3810 - val_loss: 1633.9053 - val_MAPE: 98.4256\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1610.6371 - MAPE: 98.4842 - val_loss: 1499.4052 - val_MAPE: 98.1748\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1478.0415 - MAPE: 98.2872 - val_loss: 1370.6615 - val_MAPE: 97.9251\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1351.2557 - MAPE: 98.1702 - val_loss: 1247.8281 - val_MAPE: 97.6728\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1230.6328 - MAPE: 98.1689 - val_loss: 1131.7712 - val_MAPE: 97.3980\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 1116.6366 - MAPE: 98.3509 - val_loss: 1021.4200 - val_MAPE: 97.1105\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 1008.1313 - MAPE: 98.1647 - val_loss: 916.8488 - val_MAPE: 96.8032\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 905.6412 - MAPE: 97.9495 - val_loss: 818.3315 - val_MAPE: 96.4854\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 809.2230 - MAPE: 97.9108 - val_loss: 726.0428 - val_MAPE: 96.1590\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 718.9461 - MAPE: 97.7211 - val_loss: 639.6510 - val_MAPE: 95.8245\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 634.5608 - MAPE: 97.7203 - val_loss: 559.3539 - val_MAPE: 95.4793\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 556.3351 - MAPE: 97.6449 - val_loss: 485.1843 - val_MAPE: 95.1362\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 484.1218 - MAPE: 97.4643 - val_loss: 417.0025 - val_MAPE: 94.7975\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 417.9449 - MAPE: 97.3441 - val_loss: 354.6152 - val_MAPE: 94.4575\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 357.5350 - MAPE: 97.1297 - val_loss: 298.4630 - val_MAPE: 94.1157\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 303.4090 - MAPE: 96.9418 - val_loss: 248.3436 - val_MAPE: 93.7662\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 255.1970 - MAPE: 96.8274 - val_loss: 204.0850 - val_MAPE: 93.4024\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 212.9058 - MAPE: 96.5873 - val_loss: 165.9777 - val_MAPE: 93.0263\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 176.9557 - MAPE: 96.4500 - val_loss: 134.2539 - val_MAPE: 92.6519\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 147.2464 - MAPE: 96.1606 - val_loss: 108.5689 - val_MAPE: 92.2712\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 123.3676 - MAPE: 96.1113 - val_loss: 88.4558 - val_MAPE: 91.8709\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 105.2078 - MAPE: 95.7352 - val_loss: 74.3406 - val_MAPE: 91.4313\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 93.2576 - MAPE: 95.6107 - val_loss: 66.2203 - val_MAPE: 90.9620\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 85.4565 - MAPE: 95.3221 - val_loss: 58.0610 - val_MAPE: 90.4629\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 77.2989 - MAPE: 95.0797 - val_loss: 50.5414 - val_MAPE: 89.9553\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 70.5703 - MAPE: 94.8668 - val_loss: 45.8749 - val_MAPE: 89.4376\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 67.0440 - MAPE: 94.6063 - val_loss: 44.0402 - val_MAPE: 88.9118\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 65.3835 - MAPE: 94.3281 - val_loss: 42.3485 - val_MAPE: 88.3640\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 63.5915 - MAPE: 94.0118 - val_loss: 40.6740 - val_MAPE: 87.7953\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 62.0418 - MAPE: 93.6722 - val_loss: 39.1720 - val_MAPE: 87.2053\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 60.6109 - MAPE: 93.3461 - val_loss: 38.2111 - val_MAPE: 86.5951\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 59.8516 - MAPE: 93.2418 - val_loss: 37.5240 - val_MAPE: 85.9676\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 59.1376 - MAPE: 92.6926 - val_loss: 36.8305 - val_MAPE: 85.3210\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 58.5182 - MAPE: 92.3554 - val_loss: 36.3935 - val_MAPE: 84.6574\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 58.0795 - MAPE: 91.9815 - val_loss: 35.9096 - val_MAPE: 83.9672\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 57.6413 - MAPE: 91.5778 - val_loss: 35.5592 - val_MAPE: 83.2627\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 57.2624 - MAPE: 91.4097 - val_loss: 35.2185 - val_MAPE: 82.5348\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 56.9508 - MAPE: 90.6920 - val_loss: 34.8961 - val_MAPE: 81.7915\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 56.6276 - MAPE: 90.2962 - val_loss: 34.6231 - val_MAPE: 81.0348\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 56.3596 - MAPE: 90.0040 - val_loss: 34.3343 - val_MAPE: 80.2678\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 56.0657 - MAPE: 89.5408 - val_loss: 34.0391 - val_MAPE: 79.4894\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 55.7842 - MAPE: 89.1671 - val_loss: 33.7588 - val_MAPE: 78.6932\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 55.4660 - MAPE: 88.6304 - val_loss: 33.4589 - val_MAPE: 77.8754\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 55.1833 - MAPE: 88.1970 - val_loss: 33.1964 - val_MAPE: 77.0372\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 54.9318 - MAPE: 87.7673 - val_loss: 32.9369 - val_MAPE: 76.1730\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 54.6246 - MAPE: 87.0243 - val_loss: 32.6261 - val_MAPE: 75.2930\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 54.2980 - MAPE: 86.4256 - val_loss: 32.3087 - val_MAPE: 74.3915\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 53.9765 - MAPE: 86.1213 - val_loss: 32.0312 - val_MAPE: 73.4817\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 53.6644 - MAPE: 85.5315 - val_loss: 31.6758 - val_MAPE: 72.5666\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 53.3329 - MAPE: 84.8407 - val_loss: 31.3528 - val_MAPE: 71.6389\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 52.9928 - MAPE: 84.5516 - val_loss: 31.0637 - val_MAPE: 70.6916\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 52.7073 - MAPE: 84.0037 - val_loss: 30.7029 - val_MAPE: 69.7326\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 52.3521 - MAPE: 83.3592 - val_loss: 30.3735 - val_MAPE: 68.7571\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 52.0184 - MAPE: 82.4622 - val_loss: 30.0646 - val_MAPE: 67.7744\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 51.6151 - MAPE: 82.2272 - val_loss: 29.6854 - val_MAPE: 66.7883\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 51.2844 - MAPE: 81.8191 - val_loss: 29.3288 - val_MAPE: 65.7996\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 50.8381 - MAPE: 81.3949 - val_loss: 28.9842 - val_MAPE: 64.8038\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 50.4919 - MAPE: 80.5984 - val_loss: 28.5981 - val_MAPE: 63.8076\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 50.1133 - MAPE: 80.1506 - val_loss: 28.2086 - val_MAPE: 62.8011\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 49.6828 - MAPE: 79.0360 - val_loss: 27.8396 - val_MAPE: 61.8073\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 49.2524 - MAPE: 78.2626 - val_loss: 27.4651 - val_MAPE: 60.8033\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 48.8870 - MAPE: 77.8844 - val_loss: 27.0765 - val_MAPE: 59.7998\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 48.4693 - MAPE: 77.1284 - val_loss: 26.6980 - val_MAPE: 58.7920\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 48.0389 - MAPE: 76.2972 - val_loss: 26.2912 - val_MAPE: 57.7905\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 47.5532 - MAPE: 76.1460 - val_loss: 25.8877 - val_MAPE: 56.8103\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 47.1546 - MAPE: 74.3006 - val_loss: 25.4753 - val_MAPE: 55.8415\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 46.6481 - MAPE: 73.9055 - val_loss: 25.0691 - val_MAPE: 54.8920\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 46.1783 - MAPE: 73.3536 - val_loss: 24.6545 - val_MAPE: 53.9529\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 45.7243 - MAPE: 71.8932 - val_loss: 24.2511 - val_MAPE: 53.0348\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 45.2647 - MAPE: 71.9921 - val_loss: 23.8389 - val_MAPE: 52.1266\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 44.7612 - MAPE: 71.6764 - val_loss: 23.4418 - val_MAPE: 51.2343\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 44.3328 - MAPE: 70.8911 - val_loss: 23.0515 - val_MAPE: 50.3533\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 43.8195 - MAPE: 69.2191 - val_loss: 22.6027 - val_MAPE: 49.5063\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 43.3195 - MAPE: 68.5830 - val_loss: 22.1779 - val_MAPE: 48.6810\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 42.8454 - MAPE: 67.3169 - val_loss: 21.7952 - val_MAPE: 47.8751\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 42.3722 - MAPE: 66.7351 - val_loss: 21.4054 - val_MAPE: 47.1062\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 41.8458 - MAPE: 66.1223 - val_loss: 20.9735 - val_MAPE: 46.3861\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 41.2872 - MAPE: 67.2034 - val_loss: 20.5829 - val_MAPE: 45.7022\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 40.7530 - MAPE: 64.0700 - val_loss: 20.1794 - val_MAPE: 45.0491\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 40.2697 - MAPE: 63.6177 - val_loss: 19.8060 - val_MAPE: 44.4307\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 39.7965 - MAPE: 62.7548 - val_loss: 19.4524 - val_MAPE: 43.8499\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 39.2625 - MAPE: 61.8676 - val_loss: 19.0681 - val_MAPE: 43.3128\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 111ms/step - loss: 38.7156 - MAPE: 61.2312 - val_loss: 18.6883 - val_MAPE: 42.8096\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 38.1349 - MAPE: 60.6650 - val_loss: 18.3723 - val_MAPE: 42.3706\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 112ms/step - loss: 37.6446 - MAPE: 60.6070 - val_loss: 18.0148 - val_MAPE: 41.9905\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 37.1455 - MAPE: 59.7977 - val_loss: 17.6306 - val_MAPE: 41.6775\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 36.5901 - MAPE: 57.3449 - val_loss: 17.3491 - val_MAPE: 41.4389\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 36.0132 - MAPE: 57.4680 - val_loss: 17.0463 - val_MAPE: 41.2731\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 35.4711 - MAPE: 56.8420 - val_loss: 16.7897 - val_MAPE: 41.1870\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 35.0287 - MAPE: 56.7273 - val_loss: 16.5770 - val_MAPE: 41.1788\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 34.4528 - MAPE: 56.7084 - val_loss: 16.3296 - val_MAPE: 41.2555\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 34.0179 - MAPE: 55.0572 - val_loss: 16.1079 - val_MAPE: 41.4030\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 33.4417 - MAPE: 54.5309 - val_loss: 15.9427 - val_MAPE: 41.6154\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 33.0139 - MAPE: 53.7295 - val_loss: 15.7722 - val_MAPE: 41.9236\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 32.4909 - MAPE: 53.1394 - val_loss: 15.5873 - val_MAPE: 42.3062\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 31.8857 - MAPE: 52.4532 - val_loss: 15.5152 - val_MAPE: 42.7513\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 31.5689 - MAPE: 50.7270 - val_loss: 15.4028 - val_MAPE: 43.2538\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 31.0200 - MAPE: 52.1089 - val_loss: 15.3370 - val_MAPE: 43.8172\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 30.6218 - MAPE: 50.6093 - val_loss: 15.2963 - val_MAPE: 44.4408\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 30.1660 - MAPE: 48.5617 - val_loss: 15.2438 - val_MAPE: 45.1244\n",
      "train r2: -3.798560230924428\n",
      "train r2: 0.0023237040319962713\n",
      "val r2: -0.26172456004227174\n",
      "val r2: 0.001139147004360362\n"
     ]
    }
   ],
   "source": [
    "# Loss Metric to optimize\n",
    "metric = tf.keras.metrics.MeanAbsolutePercentageError(name='MAPE')\n",
    "\n",
    "# Create early stopping point\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    monitor='val_'+metric.name,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "bm.compile(loss=tf.keras.metrics.mean_absolute_error, \n",
    "           optimizer=keras.optimizers.Adam(),\n",
    "           metrics=metric)\n",
    "\n",
    "# Fit the model\n",
    "history = bm.fit(x= X_train,\n",
    "                 y=y_train,\n",
    "                 epochs = 200,\n",
    "                 callbacks=[callback],\n",
    "                 batch_size=512,\n",
    "                 validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "preds_train = bm.predict(X_train).flatten()\n",
    "preds_val = bm.predict(X_val).flatten()\n",
    "\n",
    "print('train r2:',r2_score(y_train.flatten(), preds_train))\n",
    "print('train r2:',(np.corrcoef(y_train.flatten(), preds_train)**2)[0][1])\n",
    "\n",
    "print('val r2:',r2_score(y_val.flatten(), preds_val))\n",
    "print('val r2:',(np.corrcoef(y_val.flatten(), preds_val)**2)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 45.4839 - MAPE: 91.9167 - val_loss: 14.1024 - val_MAPE: 66.8014\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 33.7040 - MAPE: 79.5775 - val_loss: 11.1353 - val_MAPE: 52.1070\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 29.3823 - MAPE: 66.4293 - val_loss: 10.0158 - val_MAPE: 47.2930\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 27.3341 - MAPE: 62.4996 - val_loss: 9.8122 - val_MAPE: 45.6371\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 25.9432 - MAPE: 55.5520 - val_loss: 9.7036 - val_MAPE: 45.2370\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 25.4551 - MAPE: 56.4819 - val_loss: 9.4428 - val_MAPE: 42.3645\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 24.9479 - MAPE: 53.2063 - val_loss: 9.4181 - val_MAPE: 42.3927\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 24.3422 - MAPE: 52.0987 - val_loss: 9.3754 - val_MAPE: 42.5363\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 24.0473 - MAPE: 51.0788 - val_loss: 9.2533 - val_MAPE: 40.9980\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 23.6397 - MAPE: 50.6882 - val_loss: 9.2193 - val_MAPE: 40.3316\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 23.2921 - MAPE: 49.9936 - val_loss: 9.2629 - val_MAPE: 41.8958\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 23.0707 - MAPE: 46.5365 - val_loss: 9.1623 - val_MAPE: 40.4954\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 22.8354 - MAPE: 46.6125 - val_loss: 9.1190 - val_MAPE: 39.7095\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 22.3515 - MAPE: 45.7213 - val_loss: 9.0785 - val_MAPE: 39.2843\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 22.3956 - MAPE: 48.1304 - val_loss: 9.0428 - val_MAPE: 39.1185\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 22.1809 - MAPE: 46.0905 - val_loss: 9.0216 - val_MAPE: 38.5208\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 21.9118 - MAPE: 44.6431 - val_loss: 9.0170 - val_MAPE: 40.2121\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 21.7377 - MAPE: 46.2671 - val_loss: 9.0063 - val_MAPE: 39.5202\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 21.7625 - MAPE: 43.9639 - val_loss: 8.9208 - val_MAPE: 38.6830\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 21.5076 - MAPE: 42.9237 - val_loss: 8.8986 - val_MAPE: 38.6378\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 21.3242 - MAPE: 44.0305 - val_loss: 8.8690 - val_MAPE: 38.7422\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 21.3454 - MAPE: 42.4029 - val_loss: 8.8659 - val_MAPE: 38.0820\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 20.9969 - MAPE: 42.1361 - val_loss: 8.9083 - val_MAPE: 37.5172\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 21.0572 - MAPE: 43.6731 - val_loss: 8.8250 - val_MAPE: 37.8923\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 20.8666 - MAPE: 41.5298 - val_loss: 8.8288 - val_MAPE: 38.5777\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 20.7811 - MAPE: 42.1265 - val_loss: 8.7830 - val_MAPE: 38.1982\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 20.6739 - MAPE: 41.4457 - val_loss: 8.7718 - val_MAPE: 36.8796\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 20.6952 - MAPE: 42.0688 - val_loss: 8.7443 - val_MAPE: 36.9349\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 20.3558 - MAPE: 40.6507 - val_loss: 8.8324 - val_MAPE: 36.2733\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 20.3514 - MAPE: 40.5410 - val_loss: 8.7840 - val_MAPE: 35.9863\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 20.2374 - MAPE: 40.1925 - val_loss: 8.6909 - val_MAPE: 36.3489\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 20.2333 - MAPE: 39.8813 - val_loss: 8.6420 - val_MAPE: 36.6056\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 20.1378 - MAPE: 39.6157 - val_loss: 8.5957 - val_MAPE: 36.6552\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 20.1125 - MAPE: 39.8018 - val_loss: 8.5850 - val_MAPE: 36.9059\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 19.9909 - MAPE: 39.3319 - val_loss: 8.5770 - val_MAPE: 35.9629\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 20.0022 - MAPE: 39.7829 - val_loss: 8.6040 - val_MAPE: 35.2682\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 19.8526 - MAPE: 38.9964 - val_loss: 8.5531 - val_MAPE: 35.3803\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 19.8400 - MAPE: 39.3522 - val_loss: 8.6589 - val_MAPE: 34.7255\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 19.7754 - MAPE: 38.7735 - val_loss: 8.6974 - val_MAPE: 34.7072\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 19.6844 - MAPE: 38.4480 - val_loss: 8.5807 - val_MAPE: 34.8082\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 19.6017 - MAPE: 39.4322 - val_loss: 8.5335 - val_MAPE: 35.0207\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 19.4891 - MAPE: 38.2527 - val_loss: 8.4842 - val_MAPE: 34.6113\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 19.4479 - MAPE: 38.7884 - val_loss: 8.4638 - val_MAPE: 35.0119\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 19.4746 - MAPE: 38.3143 - val_loss: 8.4718 - val_MAPE: 34.8330\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 19.2792 - MAPE: 38.3545 - val_loss: 8.5121 - val_MAPE: 34.3161\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 19.2484 - MAPE: 38.0443 - val_loss: 8.5863 - val_MAPE: 33.9277\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 19.1521 - MAPE: 37.4525 - val_loss: 8.4140 - val_MAPE: 34.2565\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 19.1177 - MAPE: 37.4676 - val_loss: 8.4995 - val_MAPE: 33.7899\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 19.1438 - MAPE: 37.7525 - val_loss: 8.3766 - val_MAPE: 33.8194\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 18.9886 - MAPE: 37.4557 - val_loss: 8.2952 - val_MAPE: 33.9716\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 19.0681 - MAPE: 37.7154 - val_loss: 8.4804 - val_MAPE: 33.5882\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.9513 - MAPE: 37.5494 - val_loss: 8.4582 - val_MAPE: 33.5547\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 18.8805 - MAPE: 37.4023 - val_loss: 8.3575 - val_MAPE: 33.4285\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.9249 - MAPE: 37.2534 - val_loss: 8.2476 - val_MAPE: 33.6547\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 18.9120 - MAPE: 37.8716 - val_loss: 8.2982 - val_MAPE: 33.5370\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 18.8576 - MAPE: 37.2636 - val_loss: 8.3283 - val_MAPE: 33.0091\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 18.7524 - MAPE: 36.8340 - val_loss: 8.3108 - val_MAPE: 33.3630\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 18.7709 - MAPE: 37.5859 - val_loss: 8.2463 - val_MAPE: 33.3528\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 18.7797 - MAPE: 37.2460 - val_loss: 8.6045 - val_MAPE: 33.0218\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 18.5755 - MAPE: 36.9172 - val_loss: 8.5266 - val_MAPE: 32.8886\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.6008 - MAPE: 36.5179 - val_loss: 8.1034 - val_MAPE: 33.0864\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 18.6852 - MAPE: 37.0578 - val_loss: 8.3589 - val_MAPE: 32.6310\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 18.5311 - MAPE: 36.3787 - val_loss: 8.5077 - val_MAPE: 32.6189\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 18.5476 - MAPE: 36.5327 - val_loss: 8.3984 - val_MAPE: 32.9950\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 18.5580 - MAPE: 36.6507 - val_loss: 7.9578 - val_MAPE: 33.0020\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 18.4697 - MAPE: 36.5952 - val_loss: 8.1333 - val_MAPE: 32.7666\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.4045 - MAPE: 36.4965 - val_loss: 7.9441 - val_MAPE: 32.7988\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.3543 - MAPE: 36.5069 - val_loss: 8.0991 - val_MAPE: 32.7114\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.3464 - MAPE: 36.4945 - val_loss: 7.8694 - val_MAPE: 32.7246\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 18.2138 - MAPE: 36.0222 - val_loss: 8.0624 - val_MAPE: 32.3888\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.2502 - MAPE: 36.6249 - val_loss: 7.8695 - val_MAPE: 32.8269\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.1688 - MAPE: 36.2928 - val_loss: 7.7560 - val_MAPE: 32.8123\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.2410 - MAPE: 36.4444 - val_loss: 8.2014 - val_MAPE: 32.3137\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 18.2033 - MAPE: 36.3074 - val_loss: 8.2645 - val_MAPE: 32.2400\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.1750 - MAPE: 36.5138 - val_loss: 8.2422 - val_MAPE: 32.2745\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.1855 - MAPE: 35.6706 - val_loss: 8.2020 - val_MAPE: 32.1751\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.2600 - MAPE: 36.3906 - val_loss: 7.9891 - val_MAPE: 32.3484\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.0267 - MAPE: 35.8398 - val_loss: 8.1301 - val_MAPE: 32.1363\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.9469 - MAPE: 35.5857 - val_loss: 8.3243 - val_MAPE: 31.8383\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 18.0443 - MAPE: 35.8290 - val_loss: 8.0765 - val_MAPE: 31.6829\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.8908 - MAPE: 35.6668 - val_loss: 7.7388 - val_MAPE: 32.1153\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 17.9460 - MAPE: 35.9380 - val_loss: 7.6684 - val_MAPE: 31.9590\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.8359 - MAPE: 35.9512 - val_loss: 7.7792 - val_MAPE: 31.9658\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.9721 - MAPE: 35.6439 - val_loss: 7.6486 - val_MAPE: 31.7528\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 17.7749 - MAPE: 35.6758 - val_loss: 7.7249 - val_MAPE: 31.9330\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.7206 - MAPE: 35.6135 - val_loss: 7.6885 - val_MAPE: 31.8242\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 17.7115 - MAPE: 35.7434 - val_loss: 7.5279 - val_MAPE: 31.8900\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 17.6427 - MAPE: 35.6455 - val_loss: 7.7854 - val_MAPE: 31.7369\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.5177 - MAPE: 35.2903 - val_loss: 7.8251 - val_MAPE: 31.8169\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.5483 - MAPE: 35.4319 - val_loss: 7.4370 - val_MAPE: 31.6514\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.5670 - MAPE: 35.7224 - val_loss: 7.4067 - val_MAPE: 31.7734\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.5235 - MAPE: 35.6095 - val_loss: 7.2520 - val_MAPE: 31.8806\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 17.5627 - MAPE: 35.4442 - val_loss: 7.3330 - val_MAPE: 31.6100\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.5064 - MAPE: 35.5763 - val_loss: 7.6556 - val_MAPE: 31.3353\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 17.4801 - MAPE: 35.4719 - val_loss: 7.3190 - val_MAPE: 32.2328\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.4845 - MAPE: 35.3909 - val_loss: 7.4410 - val_MAPE: 31.2997\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 17.4585 - MAPE: 35.9041 - val_loss: 7.2267 - val_MAPE: 31.6369\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 17.5199 - MAPE: 35.5587 - val_loss: 7.0610 - val_MAPE: 32.7466\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 17.2687 - MAPE: 35.7612 - val_loss: 7.1998 - val_MAPE: 31.7047\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 17.4063 - MAPE: 35.2550 - val_loss: 7.1500 - val_MAPE: 31.8188\n",
      "train r2: 0.4610921866254599\n",
      "train r2: 0.5979128031454339\n",
      "val r2: 0.353535359011678\n",
      "val r2: 0.3696322500886179\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and build layers\n",
    "bm = models.Sequential()\n",
    "bm.add(layers.Dense(239, activation='relu', input_shape=input_shape, kernel_regularizer=regularizers.l1(0)))\n",
    "bm.add(layers.Dropout(.9))\n",
    "bm.add(layers.Dense(162, activation='relu', kernel_regularizer=regularizers.l1(0)))\n",
    "bm.add(layers.Dropout(.9))\n",
    "bm.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Loss Metric to optimize\n",
    "metric = tf.keras.metrics.MeanAbsolutePercentageError(name='MAPE')\n",
    "\n",
    "# Create early stopping point\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    monitor='val_'+metric.name,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "bm.compile(loss=tf.keras.metrics.mean_absolute_error, \n",
    "           optimizer=keras.optimizers.Adam(),\n",
    "           metrics=metric)\n",
    "\n",
    "# Fit the model\n",
    "history = bm.fit(x= X_train,\n",
    "                 y=y_train,\n",
    "                 epochs = 100,\n",
    "                 callbacks=[callback],\n",
    "                 batch_size=128,\n",
    "                 validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "preds_train = bm.predict(X_train).flatten()\n",
    "preds_val = bm.predict(X_val).flatten()\n",
    "\n",
    "print('train r2:',r2_score(y_train.flatten(), preds_train))\n",
    "print('train r2:',(np.corrcoef(y_train.flatten(), preds_train)**2)[0][1])\n",
    "\n",
    "print('val r2:',r2_score(y_val.flatten(), preds_val))\n",
    "print('val r2:',(np.corrcoef(y_val.flatten(), preds_val)**2)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 46.8585 - MAPE: 90.4892 - val_loss: 17.5185 - val_MAPE: 67.1698\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 36.7941 - MAPE: 79.6642 - val_loss: 13.9950 - val_MAPE: 47.4856\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 32.5062 - MAPE: 66.7438 - val_loss: 13.3872 - val_MAPE: 48.3413\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 30.4398 - MAPE: 60.3333 - val_loss: 12.8435 - val_MAPE: 44.3023\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 28.9885 - MAPE: 53.8950 - val_loss: 12.7383 - val_MAPE: 45.2990\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 28.0770 - MAPE: 54.0073 - val_loss: 12.3631 - val_MAPE: 42.5779\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 27.3943 - MAPE: 52.5591 - val_loss: 12.2777 - val_MAPE: 43.1760\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 26.8713 - MAPE: 49.6120 - val_loss: 12.1268 - val_MAPE: 42.0520\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 26.4406 - MAPE: 50.0308 - val_loss: 12.0917 - val_MAPE: 42.5025\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 25.8199 - MAPE: 51.5100 - val_loss: 12.0949 - val_MAPE: 43.1972\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 25.4520 - MAPE: 50.1542 - val_loss: 11.8539 - val_MAPE: 41.2658\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 25.2384 - MAPE: 47.5307 - val_loss: 11.8252 - val_MAPE: 41.5656\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 24.6753 - MAPE: 45.7127 - val_loss: 11.8169 - val_MAPE: 41.9496\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 24.5108 - MAPE: 46.0155 - val_loss: 11.8391 - val_MAPE: 42.5885\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 24.3157 - MAPE: 46.8670 - val_loss: 11.7782 - val_MAPE: 42.4243\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 23.9825 - MAPE: 44.6938 - val_loss: 11.6753 - val_MAPE: 41.6844\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 23.6821 - MAPE: 44.1249 - val_loss: 11.5649 - val_MAPE: 40.5917\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 23.4603 - MAPE: 43.3977 - val_loss: 11.4548 - val_MAPE: 40.0415\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 23.2167 - MAPE: 44.3024 - val_loss: 11.4355 - val_MAPE: 40.6646\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 1s 32ms/step - loss: 23.0283 - MAPE: 42.2585 - val_loss: 11.3685 - val_MAPE: 40.5959\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 22.9126 - MAPE: 41.8642 - val_loss: 11.3311 - val_MAPE: 40.6177\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 22.6122 - MAPE: 41.0221 - val_loss: 11.2637 - val_MAPE: 39.8533\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 22.4158 - MAPE: 42.7012 - val_loss: 11.1475 - val_MAPE: 39.2528\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 22.2767 - MAPE: 41.8864 - val_loss: 11.0885 - val_MAPE: 38.0643\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 22.0394 - MAPE: 40.4060 - val_loss: 11.1012 - val_MAPE: 40.0724\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 21.9976 - MAPE: 40.0452 - val_loss: 10.9727 - val_MAPE: 39.5755\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 21.7782 - MAPE: 39.7691 - val_loss: 10.8323 - val_MAPE: 38.1905\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 21.6059 - MAPE: 39.1647 - val_loss: 10.7318 - val_MAPE: 37.7934\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 21.6026 - MAPE: 39.2336 - val_loss: 10.6472 - val_MAPE: 37.4700\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 21.5648 - MAPE: 40.3317 - val_loss: 10.6320 - val_MAPE: 38.3403\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 21.4654 - MAPE: 40.5226 - val_loss: 10.5256 - val_MAPE: 37.2975\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 21.2257 - MAPE: 38.3845 - val_loss: 10.4716 - val_MAPE: 36.6790\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 21.2068 - MAPE: 38.7801 - val_loss: 10.4532 - val_MAPE: 35.9604\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 20.8609 - MAPE: 38.3634 - val_loss: 10.3342 - val_MAPE: 35.9680\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 20.9420 - MAPE: 38.6888 - val_loss: 10.2864 - val_MAPE: 36.3373\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 20.7632 - MAPE: 38.2931 - val_loss: 10.1736 - val_MAPE: 35.5457\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 20.6980 - MAPE: 37.8806 - val_loss: 10.1388 - val_MAPE: 36.5768\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 20.3912 - MAPE: 37.7008 - val_loss: 10.0505 - val_MAPE: 35.3015\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 20.4018 - MAPE: 37.5925 - val_loss: 10.0090 - val_MAPE: 34.9178\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 20.3237 - MAPE: 37.6566 - val_loss: 9.8850 - val_MAPE: 35.3036\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 20.1901 - MAPE: 37.4577 - val_loss: 9.8413 - val_MAPE: 35.9091\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 20.0246 - MAPE: 37.7131 - val_loss: 9.8514 - val_MAPE: 34.7337\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 19.9137 - MAPE: 36.7482 - val_loss: 9.8767 - val_MAPE: 34.1464\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 19.8612 - MAPE: 36.8493 - val_loss: 9.8456 - val_MAPE: 33.9662\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 19.8848 - MAPE: 37.1028 - val_loss: 9.6822 - val_MAPE: 34.0572\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 19.6033 - MAPE: 36.6189 - val_loss: 9.6171 - val_MAPE: 34.4817\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 19.5054 - MAPE: 36.4242 - val_loss: 9.4383 - val_MAPE: 34.2507\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 19.3983 - MAPE: 36.0171 - val_loss: 9.3938 - val_MAPE: 34.4396\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 19.3778 - MAPE: 36.7130 - val_loss: 9.3176 - val_MAPE: 34.2469\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 19.2108 - MAPE: 36.0077 - val_loss: 9.4776 - val_MAPE: 33.2315\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 19.2557 - MAPE: 36.2149 - val_loss: 9.2172 - val_MAPE: 34.0222\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 19.0724 - MAPE: 37.6366 - val_loss: 9.1762 - val_MAPE: 35.0906\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 18.9517 - MAPE: 35.5982 - val_loss: 9.2054 - val_MAPE: 33.4425\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 18.9457 - MAPE: 36.2887 - val_loss: 9.2993 - val_MAPE: 32.8170\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 18.8230 - MAPE: 36.1336 - val_loss: 9.0407 - val_MAPE: 33.1689\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 18.6749 - MAPE: 35.8504 - val_loss: 9.1151 - val_MAPE: 32.9470\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 18.7349 - MAPE: 36.5722 - val_loss: 8.8462 - val_MAPE: 33.5661\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 18.4936 - MAPE: 35.2042 - val_loss: 8.8058 - val_MAPE: 34.0467\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 18.4369 - MAPE: 36.0447 - val_loss: 8.8421 - val_MAPE: 32.8735\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 18.3069 - MAPE: 35.3579 - val_loss: 8.7690 - val_MAPE: 32.8914\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 18.3334 - MAPE: 35.4016 - val_loss: 8.8897 - val_MAPE: 32.7688\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 18.3135 - MAPE: 35.1414 - val_loss: 8.5723 - val_MAPE: 32.9527\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 18.1084 - MAPE: 35.8728 - val_loss: 8.4773 - val_MAPE: 32.9152\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 34ms/step - loss: 18.0752 - MAPE: 35.8489 - val_loss: 8.4673 - val_MAPE: 33.3615\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 18.0607 - MAPE: 35.2130 - val_loss: 8.3936 - val_MAPE: 33.0119\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 17.9383 - MAPE: 35.7968 - val_loss: 8.3814 - val_MAPE: 32.5619\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 17.8687 - MAPE: 35.1157 - val_loss: 8.2518 - val_MAPE: 33.1781\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 17.7131 - MAPE: 35.4420 - val_loss: 8.4944 - val_MAPE: 32.3860\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 17.6829 - MAPE: 35.7597 - val_loss: 8.2107 - val_MAPE: 34.5354\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 17.5788 - MAPE: 35.2279 - val_loss: 8.3350 - val_MAPE: 32.1661\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 17.6899 - MAPE: 35.6847 - val_loss: 8.1076 - val_MAPE: 32.9836\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 17.4741 - MAPE: 35.4262 - val_loss: 8.0580 - val_MAPE: 33.0912\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 17.4704 - MAPE: 35.0676 - val_loss: 8.0116 - val_MAPE: 33.6608\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 17.3831 - MAPE: 35.6198 - val_loss: 7.9612 - val_MAPE: 34.0324\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 17.2468 - MAPE: 34.8990 - val_loss: 7.9839 - val_MAPE: 32.9814\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 17.1634 - MAPE: 34.6106 - val_loss: 7.9744 - val_MAPE: 33.1294\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 17.0217 - MAPE: 34.6463 - val_loss: 7.9142 - val_MAPE: 33.2601\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 17.0436 - MAPE: 34.8329 - val_loss: 7.9117 - val_MAPE: 33.1419\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 16.9383 - MAPE: 34.8096 - val_loss: 7.8790 - val_MAPE: 34.5030\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 16.8202 - MAPE: 34.9099 - val_loss: 7.8428 - val_MAPE: 33.8084\n",
      "train r2: 0.3598204798881486\n",
      "train r2: 0.5818973967622155\n",
      "val r2: 0.33874439352623664\n",
      "val r2: 0.36775351892582664\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and build layers\n",
    "bm = models.Sequential()\n",
    "bm.add(layers.Dense(239, activation='relu', input_shape=input_shape, kernel_regularizer=regularizers.l1(0.001)))\n",
    "bm.add(layers.Dropout(.9))\n",
    "bm.add(layers.Dense(162, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "bm.add(layers.Dropout(.9))\n",
    "bm.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Loss Metric to optimize\n",
    "metric = tf.keras.metrics.MeanAbsolutePercentageError(name='MAPE')\n",
    "\n",
    "# Create early stopping point\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    monitor='val_'+metric.name,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "bm.compile(loss=tf.keras.metrics.mean_absolute_error, \n",
    "           optimizer=keras.optimizers.Adam(),\n",
    "           metrics=metric)\n",
    "\n",
    "# Fit the model\n",
    "history = bm.fit(x= X_train,\n",
    "                 y=y_train,\n",
    "                 epochs = 100,\n",
    "                 callbacks=[callback],\n",
    "                 batch_size=128,\n",
    "                 validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "preds_train = bm.predict(X_train).flatten()\n",
    "preds_val = bm.predict(X_val).flatten()\n",
    "\n",
    "print('train r2:',r2_score(y_train.flatten(), preds_train))\n",
    "print('train r2:',(np.corrcoef(y_train.flatten(), preds_train)**2)[0][1])\n",
    "\n",
    "print('val r2:',r2_score(y_val.flatten(), preds_val))\n",
    "print('val r2:',(np.corrcoef(y_val.flatten(), preds_val)**2)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit Price Spikes\n",
    "Limit price spikes to +- 3*STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loc[:'2019', 'price_tomorrow'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train data, move price column to the end of set\n",
    "train = df.loc[:'2019'].drop(columns='price_tomorrow')\n",
    "train['price_tomorrow'] = df.loc[:'2019', 'price_tomorrow']\n",
    "\n",
    "# Get price_tomorrow std, mean\n",
    "price_std = df.loc[:'2019', 'price_tomorrow'].std()\n",
    "price_mean = df.loc[:'2019', 'price_tomorrow'].mean()\n",
    "\n",
    "# Copy price data and trim\n",
    "train_trimmed = train.copy()\n",
    "train_trimmed.loc[train_trimmed.price_tomorrow>(price_mean + price_std*3), 'price_tomorrow'] = price_mean + price_std*3\n",
    "\n",
    "# Prep trimmed set for modeling\n",
    "train_trimmed = np.array(np.split(train_trimmed, len(train_trimmed)/24))\n",
    "X_train_trimmed, y_train_trimmed = to_supervised(train_trimmed, n_input=24, n_out=24, stride=24)\n",
    "input_shape=(X_train_trimmed.shape[1], X_train_trimmed.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 19ms/step - loss: 27.5423 - MAPE: 48.7608 - val_loss: 11.3260 - val_MAPE: 32.1205\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 9.4534 - MAPE: 18.0859 - val_loss: 5.8631 - val_MAPE: 17.3613\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 6.9641 - MAPE: 13.8306 - val_loss: 5.6631 - val_MAPE: 16.9238\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 6.3103 - MAPE: 12.8144 - val_loss: 6.1048 - val_MAPE: 19.4528\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 6.0803 - MAPE: 12.8012 - val_loss: 6.6318 - val_MAPE: 22.0523\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 5.9934 - MAPE: 12.6876 - val_loss: 6.1971 - val_MAPE: 20.4445\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 5.9179 - MAPE: 12.5849 - val_loss: 6.8428 - val_MAPE: 22.7620\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 5.9300 - MAPE: 12.5031 - val_loss: 6.0804 - val_MAPE: 20.0912\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 5.8487 - MAPE: 12.4032 - val_loss: 6.7355 - val_MAPE: 22.5357\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 5.8436 - MAPE: 12.4487 - val_loss: 6.1224 - val_MAPE: 20.5202\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 5.7836 - MAPE: 12.3336 - val_loss: 6.3260 - val_MAPE: 21.3017\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 5.7525 - MAPE: 12.3092 - val_loss: 6.4322 - val_MAPE: 21.7429\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 5.7353 - MAPE: 12.2160 - val_loss: 6.4720 - val_MAPE: 21.8296\n",
      "train r2: 0.575173580473701\n",
      "train r2: 0.6355498754335003\n",
      "val r2: 0.5383384396682895\n",
      "val r2: 0.5968869626809818\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and build layers\n",
    "bm_1 = models.Sequential()\n",
    "bm_1.add(layers.Dense(62, activation='relu', input_shape=input_shape))\n",
    "bm_1.add(layers.Dense(239, activation='relu'))\n",
    "bm_1.add(layers.Dense(162, activation='relu'))\n",
    "bm_1.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Loss Metric to optimize\n",
    "metric = tf.keras.metrics.MeanAbsolutePercentageError(name='MAPE')\n",
    "\n",
    "# Create early stopping point\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    monitor='val_'+metric.name,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "bm_1.compile(loss=tf.keras.metrics.mean_absolute_error, \n",
    "           optimizer=keras.optimizers.Adam(),\n",
    "           metrics=metric)\n",
    "\n",
    "# Fit the model\n",
    "history = bm_1.fit(x= X_train_trimmed,\n",
    "                 y=y_train_trimmed,\n",
    "                 epochs = 100,\n",
    "                 callbacks=[callback],\n",
    "                 batch_size=128,\n",
    "                 validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "preds_train = bm_1.predict(X_train_trimmed).flatten()\n",
    "preds_val = bm_1.predict(X_val).flatten()\n",
    "\n",
    "print('train r2:',r2_score(y_train_trimmed.flatten(), preds_train))\n",
    "print('train r2:',(np.corrcoef(y_train.flatten(), preds_train)**2)[0][1])\n",
    "\n",
    "print('val r2:',r2_score(y_val.flatten(), preds_val))\n",
    "print('val r2:',(np.corrcoef(y_val.flatten(), preds_val)**2)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trimming the training response variable did not improve model performance much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - DNN\n",
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1826, 24, 7)\n",
      "(366, 24, 7)\n",
      "(1825, 24, 6) (1825, 24)\n",
      "(365, 24, 6) (365, 24)\n"
     ]
    }
   ],
   "source": [
    "# Get columns representing future information \n",
    "DNN_cols = df.filter(regex='forecast').columns\n",
    "\n",
    "train = df.loc[:'2019', DNN_cols].copy()\n",
    "val = df.loc['2020', DNN_cols].copy()\n",
    "\n",
    "train['price_tomorrow'] = df.loc[:'2019', 'price_tomorrow']\n",
    "val['price_tomorrow'] = df.loc['2020', 'price_tomorrow']\n",
    "\n",
    "train = np.array(np.split(train, len(train)/24))\n",
    "val = np.array(np.split(val, len(val)/24))\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "\n",
    "\n",
    "X_train, y_train = to_supervised(train, n_input=24, n_out=24, stride=24)\n",
    "X_val, y_val = to_supervised(val, n_input=24, n_out=24, stride=24)\n",
    "input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and build layers\n",
    "bm_1 = models.Sequential()\n",
    "bm_1.add(layers.Dense(62, activation='relu', input_shape=input_shape))\n",
    "bm_1.add(layers.Dense(239, activation='relu'))\n",
    "bm_1.add(layers.Dense(162, activation='relu'))\n",
    "bm_1.add(TimeDistributed(layers.Dense(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 22ms/step - loss: 31.1012 - MAPE: 54.8837 - val_loss: 7.6221 - val_MAPE: 22.0542\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 9.5633 - MAPE: 18.6024 - val_loss: 6.4179 - val_MAPE: 18.1986\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 6.9690 - MAPE: 13.9584 - val_loss: 6.0370 - val_MAPE: 17.4881\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 6.7030 - MAPE: 13.4970 - val_loss: 5.9364 - val_MAPE: 17.4142\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 6.6260 - MAPE: 13.3519 - val_loss: 5.8679 - val_MAPE: 17.1270\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 6.5688 - MAPE: 13.1972 - val_loss: 5.7908 - val_MAPE: 16.9660\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 6.4767 - MAPE: 13.0076 - val_loss: 5.6648 - val_MAPE: 16.5560\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 6.3765 - MAPE: 12.8074 - val_loss: 5.6211 - val_MAPE: 16.8038\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 6.2441 - MAPE: 12.6377 - val_loss: 5.6271 - val_MAPE: 17.0610\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 6.1414 - MAPE: 12.5432 - val_loss: 5.7092 - val_MAPE: 17.6546\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 6.1120 - MAPE: 12.6177 - val_loss: 5.5993 - val_MAPE: 17.4925\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 6.0496 - MAPE: 12.6041 - val_loss: 5.6319 - val_MAPE: 17.7280\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 6.0232 - MAPE: 12.6004 - val_loss: 6.2680 - val_MAPE: 19.9830\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 6.0250 - MAPE: 12.6530 - val_loss: 6.7330 - val_MAPE: 21.3585\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 6.2306 - MAPE: 12.9499 - val_loss: 5.7442 - val_MAPE: 18.3345\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 6.0014 - MAPE: 12.6014 - val_loss: 5.8728 - val_MAPE: 18.8382\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 5.9794 - MAPE: 12.5993 - val_loss: 5.9262 - val_MAPE: 19.0980\n",
      "train r2: 0.5552289382714086\n",
      "train r2: 0.6389792939446287\n",
      "val r2: 0.5290725929659816\n",
      "val r2: 0.5979660285450905\n"
     ]
    }
   ],
   "source": [
    "dnn = models.Sequential()\n",
    "dnn.add(layers.Dense(62, activation='relu', input_shape=input_shape))\n",
    "dnn.add(layers.Dense(239, activation='relu'))\n",
    "dnn.add(layers.Dense(162, activation='relu'))\n",
    "dnn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Loss Metric to optimize\n",
    "metric = tf.keras.metrics.MeanAbsolutePercentageError(name='MAPE')\n",
    "\n",
    "# Create early stopping point\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    monitor='val_'+metric.name,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "dnn.compile(loss=tf.keras.metrics.mean_absolute_error, \n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=metric)\n",
    "\n",
    "# Fit the model\n",
    "history = dnn.fit(x= X_train,\n",
    "                  y=y_train,\n",
    "                  epochs = 100,\n",
    "                  callbacks=[callback],\n",
    "                  batch_size=128,\n",
    "                  validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "preds_train = dnn.predict(X_train).flatten()\n",
    "preds_val = dnn.predict(X_val).flatten()\n",
    "\n",
    "print('train r2:',r2_score(y_train.flatten(), preds_train))\n",
    "print('train r2:',(np.corrcoef(y_train.flatten(), preds_train)**2)[0][1])\n",
    "\n",
    "print('val r2:',r2_score(y_val.flatten(), preds_val))\n",
    "print('val r2:',(np.corrcoef(y_val.flatten(), preds_val)**2)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1825, 24, 57)\n",
      "(366, 24, 57)\n",
      "(1824, 24, 6) (1824, 24)\n",
      "(365, 24, 6) (365, 24)\n"
     ]
    }
   ],
   "source": [
    "# Get columns representing past information \n",
    "LSTM_cols = list(set(df.columns) - set(DNN_cols))\n",
    "\n",
    "train_LSTM = df.loc[:'2019', LSTM_cols].copy()\n",
    "val_LSTM = df.loc['2020', LSTM_cols].copy()\n",
    "\n",
    "train_LSTM['price_tomorrow'] = df.loc[:'2019', 'price_tomorrow']\n",
    "val_LSTM['price_tomorrow'] = df.loc['2020', 'price_tomorrow']\n",
    "\n",
    "train_LSTM = np.array(np.split(train_LSTM, len(train_LSTM)/24))\n",
    "val_LSTM = np.array(np.split(val_LSTM, len(val_LSTM)/24))\n",
    "print(train_LSTM.shape)\n",
    "print(val_LSTM.shape)\n",
    "\n",
    "\n",
    "X_train_LSTM, y_train_LSTM = to_supervised(train, n_input=24, n_out=24, stride=24)\n",
    "X_val_LSTM, y_val_LSTM = to_supervised(val, n_input=24, n_out=24, stride=24)\n",
    "input_shape=(X_train_LSTM.shape[1], X_train_LSTM.shape[2])\n",
    "print(X_train_LSTM.shape, y_train_LSTM.shape)\n",
    "print(X_val_LSTM.shape, y_val_LSTM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_LSTM.shape[1]*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 26.6459 - MAPE: 48.2963 - val_loss: 14.2966 - val_MAPE: 51.7783\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 25.3086 - MAPE: 45.2933 - val_loss: 13.4928 - val_MAPE: 53.2092\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 24.1907 - MAPE: 46.2159 - val_loss: 13.4061 - val_MAPE: 55.4275\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 23.3549 - MAPE: 46.3062 - val_loss: 13.2973 - val_MAPE: 57.4276\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 22.4916 - MAPE: 46.2195 - val_loss: 13.4286 - val_MAPE: 59.3083\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 21.9316 - MAPE: 45.3119 - val_loss: 13.7680 - val_MAPE: 61.9331\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 21.4020 - MAPE: 44.5079 - val_loss: 13.7720 - val_MAPE: 63.9574\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 20.9852 - MAPE: 46.8672 - val_loss: 13.8392 - val_MAPE: 66.1354\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 20.3976 - MAPE: 46.2649 - val_loss: 13.9022 - val_MAPE: 68.1763\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 19.9114 - MAPE: 45.0588 - val_loss: 14.1110 - val_MAPE: 69.8624\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 19.5399 - MAPE: 44.7417 - val_loss: 14.5458 - val_MAPE: 71.8341\n",
      "train r2: -1.2335523211196078\n",
      "train r2: 0.20860366672203412\n",
      "val r2: -0.12525477272566876\n",
      "val r2: 0.26045952495646585\n"
     ]
    }
   ],
   "source": [
    "lstm = keras.Sequential()\n",
    "lstm.add(layers.LSTM(83, activation='tanh', input_shape=input_shape))\n",
    "lstm.add(RepeatVector(y_train_LSTM.shape[1]))\n",
    "lstm.add(layers.Dense(184, activation='relu', kernel_regularizer=regularizers.l1(0.1)))\n",
    "lstm.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Loss Metric to optimize\n",
    "metric = tf.keras.metrics.MeanAbsolutePercentageError(name='MAPE')\n",
    "\n",
    "# Create early stopping point\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    monitor='val_'+metric.name,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "dnn.compile(loss=tf.keras.metrics.mean_absolute_error, \n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=metric)\n",
    "\n",
    "# Fit the model\n",
    "history = dnn.fit(x= X_train_LSTM,\n",
    "                  y=y_train_LSTM,\n",
    "                  epochs = 100,\n",
    "                  callbacks=[callback],\n",
    "                  batch_size=128,\n",
    "                  validation_data=(X_val_LSTM, y_val_LSTM),\n",
    ")\n",
    "\n",
    "preds_train = dnn.predict(X_train_LSTM).flatten()\n",
    "preds_val = dnn.predict(X_val_LSTM).flatten()\n",
    "\n",
    "print('train r2:',r2_score(y_train_LSTM.flatten(), preds_train))\n",
    "print('train r2:',(np.corrcoef(y_train_LSTM.flatten(), preds_train)**2)[0][1])\n",
    "\n",
    "print('val r2:',r2_score(y_val_LSTM.flatten(), preds_val))\n",
    "print('val r2:',(np.corrcoef(y_val_LSTM.flatten(), preds_val)**2)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.48064142],\n",
       "       [0.48064142, 1.        ]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df['price_forecast_tomorrow'].values, df.price_tomorrow)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble LSTM-DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU - DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
