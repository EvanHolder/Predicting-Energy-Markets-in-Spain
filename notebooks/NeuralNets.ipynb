{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Models\n",
    "____\n",
    "GOALS:\n",
    "* Model `price_actual` using a univariate XGBRegressor <br>\n",
    "* Model all price components EXCEPT `price_day_ahead` with a multivariate XGBRegressor\n",
    "___\n",
    "OUTLINE:<br>\n",
    "1. Import Libraries\n",
    "2. Read in Data\n",
    "3. Modeling `price_actual`<br>\n",
    "    3.1 Prepare Data (Categoricals)<br>\n",
    "    3.2 Split Data<br>\n",
    "    3.3 Model one-to-one<br>\n",
    "    3.4 Model 24-to-24<br>\n",
    "    3.5 LSTM <br>\n",
    "    3.6 LSTM-DNN <br>\n",
    "4. Modeling Price Components (excluding `price_day_ahead`)<br>\n",
    "    4.1 Model one-to-one<br>\n",
    "    4.2 Model 24-to-24<br>\n",
    "    4.3 LSTM <br>\n",
    "    4.4 LSTM-DNN <br>\n",
    "    \n",
    "5. Final Model <br>\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_gen(data, input_window, output_window, stride):\n",
    "    '''\n",
    "    Yields train and test samples of the given provided datasets, at specified input and out lengths, and specified strides\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    train: tuple,\n",
    "        Tuple of length 2, which provides the training features and training targets respectively\n",
    "    test: tuple,\n",
    "        Tuple of length 2, which provides the testing features and testing targets respectively\n",
    "    input_window: int,\n",
    "        Length of the sequence of input data\n",
    "    output_window: int,\n",
    "        Length of the sequence of output data\n",
    "    stride, int\n",
    "        Number of steps to move between first sample and second sample\n",
    "    '''\n",
    "    # Define X_train, y_train, X_test, y_test\n",
    "    X, y = data[0], data[1]\n",
    "    \n",
    "    # Compute number of samples \n",
    "    n = len(X)/stride\n",
    "    \n",
    "    # If the input_window is greater than a day, X_train\n",
    "    if input_window > 24:\n",
    "        n_add = input_window - 24\n",
    "        X = X.iloc[:n_add].append(X)\n",
    "    else:\n",
    "        n_add = 0\n",
    "    for i in range(0, len(X)-n_add, stride):\n",
    "        yield X.iloc[i:i+input_window].to_numpy(), y.iloc[i:i+output_window].to_numpy()\n",
    "        \n",
    "def resample(data, input_window, output_window, stride):\n",
    "    win = window_gen((data[0], data[1]), input_window=input_window, output_window=output_window, stride=stride)\n",
    "    \n",
    "    n = int(len(data[0])/stride)\n",
    "    X_data = np.array([])\n",
    "    y_data = np.array([])\n",
    "    \n",
    "    for i in range(n):\n",
    "        X_sample, y_sample = next(win)\n",
    "        X_data = np.append(X_data, X_sample)\n",
    "        y_data = np.append(y_data, y_sample)\n",
    "        \n",
    "    # Reshape\n",
    "    X_data = X_data.reshape(n,input_window,len(data[0].columns))\n",
    "    y_data = y_data.reshape(n, output_window)\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_nn(models):\n",
    "    '''\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    models: list,\n",
    "        List containing trained models to use in ensemble\n",
    "    RETURNS\n",
    "    ----------\n",
    "    ensemble: keras model,\n",
    "        Trained model combining all input models into a single output model.\n",
    "    '''\n",
    "    # Get models in list\n",
    "    models = [dnn, lstm]\n",
    "\n",
    "    # Rename layers \n",
    "    for i, model in enumerate(models):\n",
    "        for i2, layer in enumerate(model.layers):\n",
    "            layer.trainable = False\n",
    "            layer._name = f'ensemble_{i}_{i2}_{layer.name}'\n",
    "    \n",
    "    # Define multi-headed input\n",
    "    ensemble_visible = [model.input for model in models]\n",
    "    \n",
    "    # Concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in models]\n",
    "    merge = layers.merge.concatenate(ensemble_outputs)\n",
    "    hidden = layers.Dense(24, activation='relu')(merge)\n",
    "    output = TimeDistributed(layers.Dense(1))(hidden)\n",
    "    ensemble = keras.Model(inputs=ensemble_visible, outputs=output)   \n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, train, test):\n",
    "    \n",
    "    if type(train[0]) is not list:\n",
    "        X_train = [train[0]]\n",
    "        X_test = [test[0]]\n",
    "    else:\n",
    "        X_train = train[0]\n",
    "        X_test = test[0]\n",
    "    \n",
    "    # Convert data into arrays if not already\n",
    "    X_train = [np.array(x_data) for x_data in X_train]\n",
    "    X_test = [np.array(x_data) for x_data in X_test]\n",
    "    y_train, y_test = np.array(train[1]), np.array(test[1])\n",
    "        \n",
    "    # Convert data into arrays if not already\n",
    "    #X_train, y_train = np.array(train[0]), np.array(train[1])\n",
    "    #X_test, y_test = np.array(test[0]), np.array(test[1])\n",
    "    \n",
    "    # Predict \n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    \n",
    "    # Compute sMAPE\n",
    "    sMAPE_train = sMAPE(y_train.flatten(), preds_train.flatten())\n",
    "    sMAPE_val = sMAPE(y_test.flatten(), preds_test.flatten())\n",
    "\n",
    "    # Compute r2\n",
    "    r2_train = r2(y_train.flatten(), preds_train.flatten())\n",
    "    r2_val = r2(y_test.flatten(), preds_test.flatten())\n",
    "    \n",
    "    return [sMAPE_train, sMAPE_val, r2_train, r2_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from keras import layers, models, regularizers\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import os\n",
    "import winsound\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime as dt\n",
    "\n",
    "os.chdir('../scripts')\n",
    "from functions import split_data, sMAPE, SMAPE, compute_metrics, r2,impute_immediate_mean\n",
    "from functions import resample, plot_metric_range, compile_fit, ensemble_nn\n",
    "os.chdir('../notebooks')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag =pd.read_csv('../data/clean/df_clean_lag.csv', index_col=0, parse_dates=True)\n",
    "TSO_preds = df_lag.price_day_ahead.copy()\n",
    "y_true_train = df_lag.loc[:'2019', 'price_actual'].copy()\n",
    "y_true_val = df_lag.loc['2020', 'price_actual'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modeling\n",
    "### 3.1 Prepare Data\n",
    "Scale Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = df_lag.select_dtypes(exclude='object').drop(columns=['price_actual', 'price_day_ahead']).columns\n",
    "\n",
    "# Get rid of negatives\n",
    "time = dt.datetime(2021,3,24,22)\n",
    "df_lag.loc[time, 'dew_point_bilbao_lag'] = impute_immediate_mean(df_lag['dew_point_bilbao_lag'], time)\n",
    "\n",
    "# Rescale data [-1,1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_lag[continuous] = scaler.fit_transform(df_lag[continuous])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode and scale categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get catergorical \n",
    "categorical = df_lag.select_dtypes(include='object').columns\n",
    "\n",
    "# Get wind direction cols\n",
    "wind_dirs = df_lag.filter(regex='wind(?!_speeds|_forecast)').columns\n",
    "\n",
    "# Instantiate encoder and transfrom wind_dir cols\n",
    "wind_dir_coder = LabelEncoder()\n",
    "wind_dir_coder.fit(df_lag['wind_madrid_lag'])\n",
    "for col in wind_dirs:\n",
    "    df_lag[col] = wind_dir_coder.transform(df_lag[col])\n",
    "    \n",
    "# Stack condition columns into single col\n",
    "stacked_conditions = df_lag.filter(regex='condition').stack()\n",
    "\n",
    "# Instantiate Label encoder, fit and transform on condition cols\n",
    "condition_coder = LabelEncoder()\n",
    "condition_coder.fit(stacked_conditions)\n",
    "for col in df_lag.filter(regex='condition').columns:\n",
    "    df_lag[col] = condition_coder.transform(df_lag[col])\n",
    "\n",
    "# Rescale data [-1,1]\n",
    "df_lag[categorical] = scaler.fit_transform(df_lag[categorical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get price components to drop\n",
    "price_drop = df_lag.filter(regex='price_(?!actual|day)').columns\n",
    "\n",
    "# Split data\n",
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create results_actual dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSO_metrics = [\n",
    "    'None',\n",
    "    round(sMAPE(y_true_train, TSO_preds.loc[:'2019']),3),\n",
    "    round(sMAPE(y_true_val, TSO_preds.loc['2020']),3), \n",
    "    round(r2(y_true_train, TSO_preds.loc[:'2019']),3),\n",
    "    round(r2(y_true_val, TSO_preds.loc['2020']),3)\n",
    "]\n",
    "results_actual = pd.DataFrame({'TSO':TSO_metrics}, \n",
    "                              index=['Parameters','sMAPE_train', 'sMAPE_val', 'r2_train', 'r2_val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Modeling (one-to-one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 2.0533 - SMAPE: 4.2205 - val_loss: 1.7293 - val_SMAPE: 4.9556\n",
      "Epoch 2/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.4609 - SMAPE: 2.7884 - val_loss: 1.6846 - val_SMAPE: 4.9711\n",
      "Epoch 3/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.3903 - SMAPE: 2.6555 - val_loss: 1.7918 - val_SMAPE: 5.1921\n",
      "Epoch 4/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.3304 - SMAPE: 2.5408 - val_loss: 1.7195 - val_SMAPE: 5.0397\n",
      "Epoch 5/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.3153 - SMAPE: 2.4981 - val_loss: 1.6497 - val_SMAPE: 4.9044\n",
      "Epoch 6/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.2606 - SMAPE: 2.3966 - val_loss: 1.6181 - val_SMAPE: 4.7836\n",
      "Epoch 7/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.2427 - SMAPE: 2.3587 - val_loss: 2.0592 - val_SMAPE: 6.0709\n",
      "Epoch 8/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.2356 - SMAPE: 2.3378 - val_loss: 1.3215 - val_SMAPE: 3.8655\n",
      "Epoch 9/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.2002 - SMAPE: 2.2699 - val_loss: 1.8049 - val_SMAPE: 5.2800\n",
      "Epoch 10/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.2044 - SMAPE: 2.2724 - val_loss: 2.0846 - val_SMAPE: 6.0810\n",
      "Epoch 11/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.1657 - SMAPE: 2.2008 - val_loss: 1.7320 - val_SMAPE: 5.0850\n",
      "Epoch 12/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.1724 - SMAPE: 2.2092 - val_loss: 1.6744 - val_SMAPE: 4.9661\n",
      "Epoch 13/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.1325 - SMAPE: 2.1384 - val_loss: 1.6880 - val_SMAPE: 5.0164\n",
      "Epoch 14/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.1195 - SMAPE: 2.1174 - val_loss: 1.7013 - val_SMAPE: 4.9958\n",
      "Epoch 15/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.1351 - SMAPE: 2.1439 - val_loss: 1.9240 - val_SMAPE: 5.6303\n",
      "Epoch 16/200\n",
      "1369/1369 [==============================] - 2s 1ms/step - loss: 1.1055 - SMAPE: 2.0842 - val_loss: 2.0682 - val_SMAPE: 5.9921\n",
      "Epoch 17/200\n",
      "1369/1369 [==============================] - 2s 2ms/step - loss: 1.1046 - SMAPE: 2.0792 - val_loss: 1.9050 - val_SMAPE: 5.5039\n",
      "Epoch 18/200\n",
      "1369/1369 [==============================] - 2s 2ms/step - loss: 1.0855 - SMAPE: 2.0473 - val_loss: 1.9612 - val_SMAPE: 5.6699\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameters</th>\n",
       "      <th>sMAPE_train</th>\n",
       "      <th>sMAPE_val</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSO</th>\n",
       "      <td>None</td>\n",
       "      <td>16.03</td>\n",
       "      <td>16.922</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn1</th>\n",
       "      <td>{'Dense1': 59, 'Dense2': 239, 'Dense3': 1}</td>\n",
       "      <td>3.283</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Parameters sMAPE_train sMAPE_val  \\\n",
       "TSO                                        None       16.03    16.922   \n",
       "nn1  {'Dense1': 59, 'Dense2': 239, 'Dense3': 1}       3.283      3.87   \n",
       "\n",
       "    r2_train r2_val  \n",
       "TSO    0.954  0.971  \n",
       "nn1    0.986   0.98  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input_shape\n",
    "input_shape = (X_train.shape[1],)\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(59, activation='relu', input_shape=input_shape))\n",
    "nn.add(layers.Dense(239, activation='relu'))\n",
    "nn.add(layers.Dense(162, activation='relu'))\n",
    "nn.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "# Compile and fit\n",
    "nn1 = compile_fit(nn, (X_train,y_train), (X_val, y_val), patience=10,\n",
    "                  loss = tf.keras.metrics.mean_absolute_error)\n",
    "\n",
    "# Define params, compute metrics, add to table\n",
    "params = {'Dense1':59,\n",
    "          'Dense2':239,\n",
    "          'Dense3':162,\n",
    "          'Dense3':1}\n",
    "results_actual['nn1'] = compute_metrics(nn1, params ,(X_train,y_train), (X_val, y_val))\n",
    "results_actual.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1-to-1 model performed well, beating the TSO model in both SMAPE and r-squared on the validation set.  Let's remove `price_day_ahead` and see how it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Modeling (24-to-24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restructure data for 24 hour input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 1s 6ms/step - loss: 11.5294 - SMAPE: 29.7080 - val_loss: 2.6049 - val_SMAPE: 7.2521\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.0511 - SMAPE: 3.9545 - val_loss: 1.7437 - val_SMAPE: 4.7946\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.1425 - SMAPE: 4.0857 - val_loss: 1.6637 - val_SMAPE: 4.8153\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 1.7983 - SMAPE: 3.4619 - val_loss: 1.9711 - val_SMAPE: 5.6140\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 1.6712 - SMAPE: 3.2338 - val_loss: 3.0562 - val_SMAPE: 8.3119\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 1.8203 - SMAPE: 3.4523 - val_loss: 1.5975 - val_SMAPE: 4.5826\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 1.6656 - SMAPE: 3.2238 - val_loss: 2.4966 - val_SMAPE: 7.0237\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 1.5754 - SMAPE: 2.9847 - val_loss: 1.5430 - val_SMAPE: 4.2392\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 1.8248 - SMAPE: 3.4372 - val_loss: 1.7854 - val_SMAPE: 5.1773\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 1.5352 - SMAPE: 2.9186 - val_loss: 1.3658 - val_SMAPE: 3.9536\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.5243 - SMAPE: 2.9026 - val_loss: 1.4125 - val_SMAPE: 4.1020\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.7295 - SMAPE: 3.2565 - val_loss: 1.7263 - val_SMAPE: 4.8999\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.6398 - SMAPE: 3.1389 - val_loss: 2.5740 - val_SMAPE: 7.1890\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.4576 - SMAPE: 2.7840 - val_loss: 2.0089 - val_SMAPE: 5.7207\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.4258 - SMAPE: 2.7347 - val_loss: 1.6405 - val_SMAPE: 4.7547\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.4914 - SMAPE: 2.8474 - val_loss: 1.3860 - val_SMAPE: 4.0721\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.4339 - SMAPE: 2.7247 - val_loss: 2.5531 - val_SMAPE: 7.1543\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 1.4318 - SMAPE: 2.7608 - val_loss: 1.4822 - val_SMAPE: 4.2132\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 1.7340 - SMAPE: 3.2222 - val_loss: 2.1282 - val_SMAPE: 6.0493\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 1.4327 - SMAPE: 2.7223 - val_loss: 1.6900 - val_SMAPE: 4.9786\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameters</th>\n",
       "      <th>sMAPE_train</th>\n",
       "      <th>sMAPE_val</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSO</th>\n",
       "      <td>None</td>\n",
       "      <td>16.03</td>\n",
       "      <td>16.922</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn1</th>\n",
       "      <td>{'Dense1': 59, 'Dense2': 239, 'Dense3': 1}</td>\n",
       "      <td>2.744</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn2</th>\n",
       "      <td>{'Dense1': 59, 'Dense2': 239, 'Dense3': 162, '...</td>\n",
       "      <td>3.721</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Parameters sMAPE_train sMAPE_val  \\\n",
       "TSO                                               None       16.03    16.922   \n",
       "nn1         {'Dense1': 59, 'Dense2': 239, 'Dense3': 1}       2.744      4.35   \n",
       "nn2  {'Dense1': 59, 'Dense2': 239, 'Dense3': 162, '...       3.721      3.97   \n",
       "\n",
       "    r2_train r2_val  \n",
       "TSO    0.954  0.971  \n",
       "nn1    0.982  0.978  \n",
       "nn2    0.979  0.977  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and validation\n",
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "\n",
    "# Reorganize the training and testing data into batches\n",
    "X_train, y_train = resample((X_train, y_train), 24, 24, 24)\n",
    "X_val, y_val = resample((X_val,y_val), 24, 24, 24)\n",
    "\n",
    "# Define input_shape\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(59, activation='relu', input_shape=input_shape))\n",
    "nn.add(layers.Dense(239, activation='relu'))\n",
    "nn.add(layers.Dense(162, activation='relu'))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and Fit\n",
    "nn2 = compile_fit(nn, (X_train, y_train), (X_val, y_val))\n",
    "\n",
    "# Define parameters, compute metrics, add to table\n",
    "params = {'Dense1':59,\n",
    "          'Dense2':239,\n",
    "          'Dense3':162,\n",
    "          'TimeDistributed':1}\n",
    "results_actual['nn2'] = compute_metrics(nn2, params, (X_train, y_train), (X_val, y_val))\n",
    "results_actual.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 4s 29ms/step - loss: 52.4747 - SMAPE: 170.0770 - val_loss: 31.4742 - val_SMAPE: 130.5466\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 47.9186 - SMAPE: 142.8279 - val_loss: 29.2232 - val_SMAPE: 114.7994\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 46.0895 - SMAPE: 133.4357 - val_loss: 27.5843 - val_SMAPE: 104.3519\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 44.4898 - SMAPE: 125.1387 - val_loss: 26.0365 - val_SMAPE: 95.1562\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 42.9533 - SMAPE: 117.9142 - val_loss: 24.5477 - val_SMAPE: 86.8701\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 41.4575 - SMAPE: 111.5837 - val_loss: 23.0996 - val_SMAPE: 79.2912\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 39.9884 - SMAPE: 105.2001 - val_loss: 21.6942 - val_SMAPE: 72.3484\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 38.5435 - SMAPE: 98.9628 - val_loss: 20.3235 - val_SMAPE: 65.9396\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 37.1194 - SMAPE: 93.3035 - val_loss: 18.9978 - val_SMAPE: 60.0624\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 35.7135 - SMAPE: 88.3350 - val_loss: 17.7197 - val_SMAPE: 54.6857\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 34.3229 - SMAPE: 83.0750 - val_loss: 16.4922 - val_SMAPE: 49.7709\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 32.9465 - SMAPE: 77.8133 - val_loss: 15.3199 - val_SMAPE: 45.2894\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 1s 23ms/step - loss: 31.5717 - SMAPE: 73.2410 - val_loss: 14.1070 - val_SMAPE: 40.8459\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 30.1814 - SMAPE: 69.0256 - val_loss: 13.0291 - val_SMAPE: 37.0568\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 28.8381 - SMAPE: 65.0393 - val_loss: 12.0458 - val_SMAPE: 33.7847\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 27.4814 - SMAPE: 60.8678 - val_loss: 11.0184 - val_SMAPE: 30.4011\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 26.1694 - SMAPE: 56.2587 - val_loss: 10.3991 - val_SMAPE: 28.5515\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 24.8949 - SMAPE: 52.3525 - val_loss: 9.4578 - val_SMAPE: 25.6247\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 23.6273 - SMAPE: 49.4844 - val_loss: 9.1386 - val_SMAPE: 24.8488\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 22.3885 - SMAPE: 46.3203 - val_loss: 7.9856 - val_SMAPE: 21.4283\n",
      "Epoch 21/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 21.1547 - SMAPE: 43.3839 - val_loss: 7.7850 - val_SMAPE: 20.9509\n",
      "Epoch 22/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 19.9656 - SMAPE: 39.7775 - val_loss: 7.1560 - val_SMAPE: 19.1507\n",
      "Epoch 23/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 18.8332 - SMAPE: 37.2647 - val_loss: 6.9077 - val_SMAPE: 18.3577\n",
      "Epoch 24/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 17.7267 - SMAPE: 34.3072 - val_loss: 6.3861 - val_SMAPE: 17.1532\n",
      "Epoch 25/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 16.6777 - SMAPE: 31.8486 - val_loss: 6.2980 - val_SMAPE: 16.9256\n",
      "Epoch 26/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 15.6989 - SMAPE: 30.0428 - val_loss: 6.1877 - val_SMAPE: 16.7460\n",
      "Epoch 27/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 14.7652 - SMAPE: 27.6378 - val_loss: 5.7606 - val_SMAPE: 15.6243\n",
      "Epoch 28/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 13.9316 - SMAPE: 26.3637 - val_loss: 6.0738 - val_SMAPE: 16.2153\n",
      "Epoch 29/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 13.1689 - SMAPE: 24.6404 - val_loss: 6.3486 - val_SMAPE: 17.1443\n",
      "Epoch 30/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 12.4528 - SMAPE: 22.7412 - val_loss: 5.9777 - val_SMAPE: 16.2153\n",
      "Epoch 31/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 11.7827 - SMAPE: 21.5411 - val_loss: 6.1438 - val_SMAPE: 16.4868\n",
      "Epoch 32/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 11.1578 - SMAPE: 20.1891 - val_loss: 5.9096 - val_SMAPE: 16.1262\n",
      "Epoch 33/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 10.6091 - SMAPE: 19.3008 - val_loss: 4.8673 - val_SMAPE: 13.4747\n",
      "Epoch 34/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 10.0684 - SMAPE: 18.0472 - val_loss: 4.9692 - val_SMAPE: 13.7058\n",
      "Epoch 35/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 9.6240 - SMAPE: 17.1674 - val_loss: 5.0969 - val_SMAPE: 14.0602\n",
      "Epoch 36/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 9.1649 - SMAPE: 16.3297 - val_loss: 5.4808 - val_SMAPE: 14.8486\n",
      "Epoch 37/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 8.7987 - SMAPE: 15.7524 - val_loss: 5.0077 - val_SMAPE: 13.7552\n",
      "Epoch 38/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 8.4007 - SMAPE: 14.9600 - val_loss: 4.4047 - val_SMAPE: 12.2930\n",
      "Epoch 39/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 8.0891 - SMAPE: 14.6353 - val_loss: 4.5218 - val_SMAPE: 12.6678\n",
      "Epoch 40/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 7.7663 - SMAPE: 13.8081 - val_loss: 4.2575 - val_SMAPE: 11.8213\n",
      "Epoch 41/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 7.7673 - SMAPE: 13.9587 - val_loss: 5.3118 - val_SMAPE: 14.4962\n",
      "Epoch 42/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 7.4126 - SMAPE: 13.2437 - val_loss: 5.8645 - val_SMAPE: 15.5310\n",
      "Epoch 43/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 7.1269 - SMAPE: 12.6842 - val_loss: 4.5707 - val_SMAPE: 12.6966\n",
      "Epoch 44/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 6.8070 - SMAPE: 12.1010 - val_loss: 4.2130 - val_SMAPE: 11.8235\n",
      "Epoch 45/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 6.5779 - SMAPE: 11.7792 - val_loss: 4.0654 - val_SMAPE: 11.4169\n",
      "Epoch 46/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 6.3068 - SMAPE: 11.2507 - val_loss: 4.0468 - val_SMAPE: 11.3346\n",
      "Epoch 47/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 6.0580 - SMAPE: 10.7200 - val_loss: 3.8686 - val_SMAPE: 10.7735\n",
      "Epoch 48/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 5.9042 - SMAPE: 10.5696 - val_loss: 4.0178 - val_SMAPE: 11.2714\n",
      "Epoch 49/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 5.6777 - SMAPE: 10.1187 - val_loss: 3.9252 - val_SMAPE: 11.0879\n",
      "Epoch 50/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 5.4636 - SMAPE: 9.8055 - val_loss: 4.3095 - val_SMAPE: 12.0057\n",
      "Epoch 51/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 5.2708 - SMAPE: 9.6038 - val_loss: 3.7706 - val_SMAPE: 10.6794\n",
      "Epoch 52/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 5.0797 - SMAPE: 9.0718 - val_loss: 4.0164 - val_SMAPE: 11.2698\n",
      "Epoch 53/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 4.9411 - SMAPE: 8.8108 - val_loss: 3.8453 - val_SMAPE: 10.8511\n",
      "Epoch 54/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 4.7838 - SMAPE: 8.5440 - val_loss: 3.8461 - val_SMAPE: 10.8672\n",
      "Epoch 55/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 4.7805 - SMAPE: 8.5732 - val_loss: 4.4696 - val_SMAPE: 12.5376\n",
      "Epoch 56/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 4.5804 - SMAPE: 8.1955 - val_loss: 3.9185 - val_SMAPE: 11.0148\n",
      "Epoch 57/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 4.4528 - SMAPE: 7.9480 - val_loss: 3.8511 - val_SMAPE: 10.8758\n",
      "Epoch 58/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 4.3298 - SMAPE: 7.7454 - val_loss: 3.6878 - val_SMAPE: 10.4584\n",
      "Epoch 59/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 4.2862 - SMAPE: 7.6535 - val_loss: 4.0158 - val_SMAPE: 11.1967\n",
      "Epoch 60/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 4.1813 - SMAPE: 7.4794 - val_loss: 3.8589 - val_SMAPE: 10.9616\n",
      "Epoch 61/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 4.0843 - SMAPE: 7.3957 - val_loss: 3.6912 - val_SMAPE: 10.4208\n",
      "Epoch 62/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 4.0365 - SMAPE: 7.2787 - val_loss: 3.5511 - val_SMAPE: 10.0408\n",
      "Epoch 63/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 4.0336 - SMAPE: 7.4106 - val_loss: 3.6843 - val_SMAPE: 10.3503\n",
      "Epoch 64/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 3.9059 - SMAPE: 7.0640 - val_loss: 3.5545 - val_SMAPE: 10.1708\n",
      "Epoch 65/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 3.8577 - SMAPE: 7.0022 - val_loss: 3.5949 - val_SMAPE: 10.1453\n",
      "Epoch 66/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 3.7933 - SMAPE: 6.8809 - val_loss: 3.3497 - val_SMAPE: 9.5481\n",
      "Epoch 67/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 3.6732 - SMAPE: 6.6409 - val_loss: 3.5101 - val_SMAPE: 10.0627\n",
      "Epoch 68/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 3.6249 - SMAPE: 6.5742 - val_loss: 3.3229 - val_SMAPE: 9.5723\n",
      "Epoch 69/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 3.5486 - SMAPE: 6.4657 - val_loss: 3.7543 - val_SMAPE: 10.7940\n",
      "Epoch 70/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 3.4795 - SMAPE: 6.3628 - val_loss: 3.2328 - val_SMAPE: 9.2737\n",
      "Epoch 71/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 3.4484 - SMAPE: 6.2699 - val_loss: 3.2613 - val_SMAPE: 9.3703\n",
      "Epoch 72/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 3.4537 - SMAPE: 6.2689 - val_loss: 3.1921 - val_SMAPE: 9.1663\n",
      "Epoch 73/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 3.3474 - SMAPE: 6.1920 - val_loss: 3.1543 - val_SMAPE: 9.0529\n",
      "Epoch 74/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 3.2645 - SMAPE: 6.0062 - val_loss: 3.3270 - val_SMAPE: 9.5271\n",
      "Epoch 75/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 3.3128 - SMAPE: 6.0681 - val_loss: 3.4767 - val_SMAPE: 9.8700\n",
      "Epoch 76/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 3.2418 - SMAPE: 5.9066 - val_loss: 3.3124 - val_SMAPE: 9.5503\n",
      "Epoch 77/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 3.1905 - SMAPE: 5.8381 - val_loss: 3.2883 - val_SMAPE: 9.4875\n",
      "Epoch 78/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 3.1208 - SMAPE: 5.7307 - val_loss: 3.2633 - val_SMAPE: 9.3838\n",
      "Epoch 79/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 3.0749 - SMAPE: 5.6860 - val_loss: 3.1809 - val_SMAPE: 9.1923\n",
      "Epoch 80/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 3.0365 - SMAPE: 5.7427 - val_loss: 3.4722 - val_SMAPE: 9.8295\n",
      "Epoch 81/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 3.1246 - SMAPE: 5.7970 - val_loss: 3.0703 - val_SMAPE: 8.8385\n",
      "Epoch 82/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.9845 - SMAPE: 5.5019 - val_loss: 3.6474 - val_SMAPE: 10.2279\n",
      "Epoch 83/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 3.5014 - SMAPE: 6.3434 - val_loss: 3.2329 - val_SMAPE: 9.3500\n",
      "Epoch 84/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 3.0147 - SMAPE: 5.5737 - val_loss: 3.1690 - val_SMAPE: 9.1624\n",
      "Epoch 85/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.9502 - SMAPE: 5.4528 - val_loss: 3.0504 - val_SMAPE: 8.8177\n",
      "Epoch 86/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.9064 - SMAPE: 5.3277 - val_loss: 3.0285 - val_SMAPE: 8.7267\n",
      "Epoch 87/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.8498 - SMAPE: 5.2342 - val_loss: 3.1317 - val_SMAPE: 9.0841\n",
      "Epoch 88/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 2.8692 - SMAPE: 5.2882 - val_loss: 3.1857 - val_SMAPE: 9.1489\n",
      "Epoch 89/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.8346 - SMAPE: 5.2298 - val_loss: 3.1433 - val_SMAPE: 9.0354\n",
      "Epoch 90/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 2.8239 - SMAPE: 5.2212 - val_loss: 2.9751 - val_SMAPE: 8.5575\n",
      "Epoch 91/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 2.7777 - SMAPE: 5.1515 - val_loss: 2.9930 - val_SMAPE: 8.6243\n",
      "Epoch 92/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.7410 - SMAPE: 5.0384 - val_loss: 3.1004 - val_SMAPE: 8.9909\n",
      "Epoch 93/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.6935 - SMAPE: 4.9548 - val_loss: 3.0871 - val_SMAPE: 8.8389\n",
      "Epoch 94/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 2.7140 - SMAPE: 5.0764 - val_loss: 3.0327 - val_SMAPE: 8.6295\n",
      "Epoch 95/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 2.8000 - SMAPE: 5.1628 - val_loss: 3.0558 - val_SMAPE: 8.8485\n",
      "Epoch 96/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 2.7040 - SMAPE: 5.0104 - val_loss: 2.9630 - val_SMAPE: 8.5683\n",
      "Epoch 97/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 2.6836 - SMAPE: 4.9995 - val_loss: 2.9618 - val_SMAPE: 8.5132\n",
      "Epoch 98/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.6300 - SMAPE: 5.0019 - val_loss: 3.0980 - val_SMAPE: 8.8713\n",
      "Epoch 99/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 2.6393 - SMAPE: 4.8834 - val_loss: 2.9185 - val_SMAPE: 8.4376\n",
      "Epoch 100/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.5701 - SMAPE: 4.7545 - val_loss: 2.9711 - val_SMAPE: 8.5890\n",
      "Epoch 101/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.5693 - SMAPE: 4.8756 - val_loss: 2.8935 - val_SMAPE: 8.3492\n",
      "Epoch 102/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.5541 - SMAPE: 4.7304 - val_loss: 2.9800 - val_SMAPE: 8.5794\n",
      "Epoch 103/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 2.5334 - SMAPE: 4.7609 - val_loss: 2.9050 - val_SMAPE: 8.3814\n",
      "Epoch 104/200\n",
      "58/58 [==============================] - 1s 19ms/step - loss: 2.5228 - SMAPE: 4.7681 - val_loss: 2.8592 - val_SMAPE: 8.2748\n",
      "Epoch 105/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 2.4811 - SMAPE: 4.6153 - val_loss: 2.8636 - val_SMAPE: 8.2436\n",
      "Epoch 106/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 2.5004 - SMAPE: 4.7343 - val_loss: 3.1296 - val_SMAPE: 9.0663\n",
      "Epoch 107/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 2.5040 - SMAPE: 4.7262 - val_loss: 2.8510 - val_SMAPE: 8.2271\n",
      "Epoch 108/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 2.4826 - SMAPE: 4.6442 - val_loss: 3.5653 - val_SMAPE: 9.9795\n",
      "Epoch 109/200\n",
      "58/58 [==============================] - 1s 24ms/step - loss: 2.6699 - SMAPE: 4.9719 - val_loss: 2.9305 - val_SMAPE: 8.4202\n",
      "Epoch 110/200\n",
      "58/58 [==============================] - 1s 25ms/step - loss: 2.4686 - SMAPE: 4.5963 - val_loss: 2.9192 - val_SMAPE: 8.3404\n",
      "Epoch 111/200\n",
      "58/58 [==============================] - 1s 24ms/step - loss: 2.5072 - SMAPE: 4.6390 - val_loss: 2.7708 - val_SMAPE: 7.9893\n",
      "Epoch 112/200\n",
      "58/58 [==============================] - 1s 23ms/step - loss: 2.4846 - SMAPE: 4.5912 - val_loss: 2.9561 - val_SMAPE: 8.5352\n",
      "Epoch 113/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 2.5206 - SMAPE: 4.7122 - val_loss: 2.8204 - val_SMAPE: 8.0649\n",
      "Epoch 114/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 2.4181 - SMAPE: 4.5446 - val_loss: 3.0484 - val_SMAPE: 8.7882\n",
      "Epoch 115/200\n",
      "58/58 [==============================] - 1s 25ms/step - loss: 2.5088 - SMAPE: 4.7234 - val_loss: 2.7896 - val_SMAPE: 8.0263\n",
      "Epoch 116/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 2.3679 - SMAPE: 4.4218 - val_loss: 2.7924 - val_SMAPE: 8.1284\n",
      "Epoch 117/200\n",
      "58/58 [==============================] - 1s 23ms/step - loss: 2.3769 - SMAPE: 4.4300 - val_loss: 2.9014 - val_SMAPE: 8.3117\n",
      "Epoch 118/200\n",
      "58/58 [==============================] - 1s 24ms/step - loss: 2.4419 - SMAPE: 4.5444 - val_loss: 2.8096 - val_SMAPE: 8.2033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200\n",
      "58/58 [==============================] - 1s 26ms/step - loss: 2.3354 - SMAPE: 4.3878 - val_loss: 2.7967 - val_SMAPE: 8.1557\n",
      "Epoch 120/200\n",
      "58/58 [==============================] - 2s 26ms/step - loss: 2.3179 - SMAPE: 4.3534 - val_loss: 2.8402 - val_SMAPE: 8.0956\n",
      "Epoch 121/200\n",
      "58/58 [==============================] - 1s 23ms/step - loss: 2.3461 - SMAPE: 4.4008 - val_loss: 2.7958 - val_SMAPE: 8.0258\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameters</th>\n",
       "      <th>sMAPE_train</th>\n",
       "      <th>sMAPE_val</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSO</th>\n",
       "      <td>None</td>\n",
       "      <td>16.03</td>\n",
       "      <td>16.922</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn1</th>\n",
       "      <td>{'Dense1': 59, 'Dense2': 239, 'Dense3': 1}</td>\n",
       "      <td>2.744</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn2</th>\n",
       "      <td>{'Dense1': 59, 'Dense2': 239, 'Dense3': 162, '...</td>\n",
       "      <td>3.721</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn3</th>\n",
       "      <td>{'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...</td>\n",
       "      <td>4.557</td>\n",
       "      <td>7.967</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Parameters sMAPE_train sMAPE_val  \\\n",
       "TSO                                               None       16.03    16.922   \n",
       "nn1         {'Dense1': 59, 'Dense2': 239, 'Dense3': 1}       2.744      4.35   \n",
       "nn2  {'Dense1': 59, 'Dense2': 239, 'Dense3': 162, '...       3.721      3.97   \n",
       "nn3  {'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...       4.557     7.967   \n",
       "\n",
       "    r2_train r2_val  \n",
       "TSO    0.954  0.971  \n",
       "nn1    0.982  0.978  \n",
       "nn2    0.979  0.977  \n",
       "nn3    0.933  0.895  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and validation\n",
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "\n",
    "# Reorganize the training and testing data into batches\n",
    "X_train, y_train = resample((X_train, y_train), 24, 24, 24)\n",
    "X_val, y_val = resample((X_val,y_val), 24, 24, 24)\n",
    "\n",
    "# Input Shape\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(60, activation='tanh', input_shape=input_shape))\n",
    "nn.add(layers.RepeatVector(y_train.shape[1]))\n",
    "nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile Fit\n",
    "nn3 = compile_fit(nn, (X_train, y_train), (X_val, y_val))\n",
    "\n",
    "# Append results\n",
    "params = {'LSTM1':60,\n",
    "          'LSTM2':24,\n",
    "          'TimeDistributed':1,\n",
    "          'input_win':'1-day'}\n",
    "\n",
    "# Compute metrics and add to table\n",
    "results_actual['nn3'] = compute_metrics(nn3, params, (X_train,y_train), (X_val,y_val))\n",
    "results_actual.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameters</th>\n",
       "      <th>sMAPE_train</th>\n",
       "      <th>sMAPE_val</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSO</th>\n",
       "      <td>None</td>\n",
       "      <td>16.03</td>\n",
       "      <td>16.922</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn1</th>\n",
       "      <td>{'Dense1': 59, 'Dense2': 239, 'Dense3': 1}</td>\n",
       "      <td>2.744</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn2</th>\n",
       "      <td>{'Dense1': 59, 'Dense2': 239, 'Dense3': 162, '...</td>\n",
       "      <td>3.721</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn3</th>\n",
       "      <td>{'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...</td>\n",
       "      <td>4.557</td>\n",
       "      <td>7.967</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn4</th>\n",
       "      <td>{'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...</td>\n",
       "      <td>6.24</td>\n",
       "      <td>9.812</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Parameters sMAPE_train sMAPE_val  \\\n",
       "TSO                                               None       16.03    16.922   \n",
       "nn1         {'Dense1': 59, 'Dense2': 239, 'Dense3': 1}       2.744      4.35   \n",
       "nn2  {'Dense1': 59, 'Dense2': 239, 'Dense3': 162, '...       3.721      3.97   \n",
       "nn3  {'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...       4.557     7.967   \n",
       "nn4  {'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...        6.24     9.812   \n",
       "\n",
       "    r2_train r2_val  \n",
       "TSO    0.954  0.971  \n",
       "nn1    0.982  0.978  \n",
       "nn2    0.979  0.977  \n",
       "nn3    0.933  0.895  \n",
       "nn4    0.883  0.843  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and validation\n",
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "\n",
    "# Reorganize the training and testing data into batches\n",
    "X_train, y_train = resample((X_train, y_train), 24*7, 24, 24)\n",
    "X_val, y_val = resample((X_val,y_val), 24*7, 24, 24)\n",
    "\n",
    "# Input Shape\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(60, activation='tanh', input_shape=input_shape))\n",
    "nn.add(layers.RepeatVector(y_train.shape[1]))\n",
    "nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and Fit\n",
    "nn4 = compile_fit(nn, (X_train, y_train), (X_val, y_val))\n",
    "\n",
    "# Append results\n",
    "params = {'LSTM1':60,\n",
    "          'LSTM2':24,\n",
    "          'TimeDistributed':1,\n",
    "          'input_win':'7-days'}\n",
    "\n",
    "# Compute metrics and add to table\n",
    "results_actual['nn4'] = compute_metrics(nn4, params, (X_train,y_train), (X_val,y_val))\n",
    "results_actual.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 8s 99ms/step - loss: 53.0665 - SMAPE: 173.6172 - val_loss: 32.2764 - val_SMAPE: 136.5677\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 5s 80ms/step - loss: 48.9403 - SMAPE: 148.5013 - val_loss: 30.2828 - val_SMAPE: 121.9762\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 47.1511 - SMAPE: 138.7440 - val_loss: 28.6390 - val_SMAPE: 110.9816\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 5s 80ms/step - loss: 45.5500 - SMAPE: 130.4701 - val_loss: 27.0860 - val_SMAPE: 101.3201\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 44.0100 - SMAPE: 123.1193 - val_loss: 25.5841 - val_SMAPE: 92.5810\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 5s 95ms/step - loss: 42.5075 - SMAPE: 115.5932 - val_loss: 24.1213 - val_SMAPE: 84.5892\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 6s 101ms/step - loss: 41.0329 - SMAPE: 109.6595 - val_loss: 22.6976 - val_SMAPE: 77.2637\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 6s 97ms/step - loss: 39.5806 - SMAPE: 103.2418 - val_loss: 21.3081 - val_SMAPE: 70.5066\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 6s 96ms/step - loss: 38.1480 - SMAPE: 97.7285 - val_loss: 19.9558 - val_SMAPE: 64.2781\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 6s 97ms/step - loss: 36.7354 - SMAPE: 92.4041 - val_loss: 18.6458 - val_SMAPE: 58.5531\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 6s 99ms/step - loss: 35.3374 - SMAPE: 86.4753 - val_loss: 17.3873 - val_SMAPE: 53.3318\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 6s 99ms/step - loss: 33.9560 - SMAPE: 81.2740 - val_loss: 16.1770 - val_SMAPE: 48.5454\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 32.5876 - SMAPE: 76.5830 - val_loss: 15.0316 - val_SMAPE: 44.2184\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 6s 100ms/step - loss: 31.2332 - SMAPE: 72.2590 - val_loss: 13.9540 - val_SMAPE: 40.3259\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 29.8931 - SMAPE: 67.9718 - val_loss: 12.8796 - val_SMAPE: 36.5985\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 28.5270 - SMAPE: 63.8488 - val_loss: 11.7292 - val_SMAPE: 32.7455\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 27.1918 - SMAPE: 59.4349 - val_loss: 10.7206 - val_SMAPE: 29.4785\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 7s 113ms/step - loss: 25.8951 - SMAPE: 56.0022 - val_loss: 10.1619 - val_SMAPE: 27.7366\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 8s 130ms/step - loss: 24.5993 - SMAPE: 52.5301 - val_loss: 9.3571 - val_SMAPE: 25.3098\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 8s 141ms/step - loss: 23.3270 - SMAPE: 48.7757 - val_loss: 8.6132 - val_SMAPE: 23.0885\n",
      "Epoch 21/200\n",
      "58/58 [==============================] - 8s 129ms/step - loss: 22.0915 - SMAPE: 45.4107 - val_loss: 7.8288 - val_SMAPE: 20.8997\n",
      "Epoch 22/200\n",
      "58/58 [==============================] - 7s 128ms/step - loss: 20.9041 - SMAPE: 42.4929 - val_loss: 7.8342 - val_SMAPE: 20.9879\n",
      "Epoch 23/200\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 19.7047 - SMAPE: 39.3117 - val_loss: 7.9741 - val_SMAPE: 21.3821\n",
      "Epoch 24/200\n",
      "58/58 [==============================] - 7s 122ms/step - loss: 18.5745 - SMAPE: 36.1678 - val_loss: 7.2535 - val_SMAPE: 19.3346\n",
      "Epoch 25/200\n",
      "58/58 [==============================] - 8s 134ms/step - loss: 17.4757 - SMAPE: 33.8253 - val_loss: 7.6775 - val_SMAPE: 20.2341\n",
      "Epoch 26/200\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 16.4795 - SMAPE: 31.5697 - val_loss: 6.1929 - val_SMAPE: 16.5186\n",
      "Epoch 27/200\n",
      "58/58 [==============================] - 8s 139ms/step - loss: 15.5525 - SMAPE: 29.2501 - val_loss: 7.2839 - val_SMAPE: 19.1469\n",
      "Epoch 28/200\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 14.6499 - SMAPE: 27.5058 - val_loss: 5.7215 - val_SMAPE: 15.3237\n",
      "Epoch 29/200\n",
      "58/58 [==============================] - 6s 111ms/step - loss: 13.8069 - SMAPE: 25.7281 - val_loss: 6.6410 - val_SMAPE: 18.0809\n",
      "Epoch 30/200\n",
      "58/58 [==============================] - 7s 120ms/step - loss: 13.0633 - SMAPE: 24.5325 - val_loss: 5.6348 - val_SMAPE: 15.2972\n",
      "Epoch 31/200\n",
      "58/58 [==============================] - 7s 118ms/step - loss: 12.3447 - SMAPE: 22.5202 - val_loss: 6.2115 - val_SMAPE: 16.2317\n",
      "Epoch 32/200\n",
      "58/58 [==============================] - 7s 114ms/step - loss: 11.8253 - SMAPE: 21.3816 - val_loss: 6.1832 - val_SMAPE: 16.4032\n",
      "Epoch 33/200\n",
      "58/58 [==============================] - 6s 111ms/step - loss: 11.3259 - SMAPE: 20.6968 - val_loss: 7.4899 - val_SMAPE: 19.3858\n",
      "Epoch 34/200\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 10.8381 - SMAPE: 19.5522 - val_loss: 6.9027 - val_SMAPE: 17.7017\n",
      "Epoch 35/200\n",
      "58/58 [==============================] - 6s 95ms/step - loss: 10.3198 - SMAPE: 18.4758 - val_loss: 5.5296 - val_SMAPE: 14.7520\n",
      "Epoch 36/200\n",
      "58/58 [==============================] - 5s 95ms/step - loss: 9.9249 - SMAPE: 18.1274 - val_loss: 5.2915 - val_SMAPE: 14.1649\n",
      "Epoch 37/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 9.5109 - SMAPE: 17.4711 - val_loss: 5.5039 - val_SMAPE: 14.5369\n",
      "Epoch 38/200\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 9.1310 - SMAPE: 16.4446 - val_loss: 5.5673 - val_SMAPE: 14.9671\n",
      "Epoch 39/200\n",
      "58/58 [==============================] - 7s 115ms/step - loss: 8.7745 - SMAPE: 15.9666 - val_loss: 4.8321 - val_SMAPE: 13.1969\n",
      "Epoch 40/200\n",
      "58/58 [==============================] - 6s 99ms/step - loss: 8.4172 - SMAPE: 15.0324 - val_loss: 5.1671 - val_SMAPE: 14.0821\n",
      "Epoch 41/200\n",
      "58/58 [==============================] - 6s 100ms/step - loss: 8.0520 - SMAPE: 14.8064 - val_loss: 4.7479 - val_SMAPE: 13.1833\n",
      "Epoch 42/200\n",
      "58/58 [==============================] - 6s 99ms/step - loss: 7.7479 - SMAPE: 13.7444 - val_loss: 4.9701 - val_SMAPE: 13.6933\n",
      "Epoch 43/200\n",
      "58/58 [==============================] - 6s 95ms/step - loss: 7.4598 - SMAPE: 13.4918 - val_loss: 4.3868 - val_SMAPE: 12.1555\n",
      "Epoch 44/200\n",
      "58/58 [==============================] - 5s 95ms/step - loss: 7.1631 - SMAPE: 12.8378 - val_loss: 4.4515 - val_SMAPE: 12.2453\n",
      "Epoch 45/200\n",
      "58/58 [==============================] - 5s 94ms/step - loss: 7.0304 - SMAPE: 12.5379 - val_loss: 4.4390 - val_SMAPE: 12.1599\n",
      "Epoch 46/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 6.7438 - SMAPE: 12.0433 - val_loss: 4.2591 - val_SMAPE: 12.0282\n",
      "Epoch 47/200\n",
      "58/58 [==============================] - 6s 102ms/step - loss: 6.5204 - SMAPE: 11.8732 - val_loss: 4.1201 - val_SMAPE: 11.5711\n",
      "Epoch 48/200\n",
      "58/58 [==============================] - 6s 107ms/step - loss: 6.3200 - SMAPE: 11.4840 - val_loss: 4.3558 - val_SMAPE: 12.2241\n",
      "Epoch 49/200\n",
      "58/58 [==============================] - 6s 103ms/step - loss: 6.1316 - SMAPE: 11.0270 - val_loss: 4.1940 - val_SMAPE: 11.9205\n",
      "Epoch 50/200\n",
      "58/58 [==============================] - 6s 103ms/step - loss: 5.9567 - SMAPE: 10.6189 - val_loss: 4.6312 - val_SMAPE: 12.7513\n",
      "Epoch 51/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 5.8211 - SMAPE: 10.4086 - val_loss: 4.2071 - val_SMAPE: 11.6948\n",
      "Epoch 52/200\n",
      "58/58 [==============================] - 6s 107ms/step - loss: 5.6492 - SMAPE: 10.1216 - val_loss: 4.1713 - val_SMAPE: 11.5881\n",
      "Epoch 53/200\n",
      "58/58 [==============================] - 6s 97ms/step - loss: 5.3872 - SMAPE: 9.8624 - val_loss: 4.3068 - val_SMAPE: 12.1114\n",
      "Epoch 54/200\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 5.3011 - SMAPE: 9.5011 - val_loss: 4.0840 - val_SMAPE: 11.3563\n",
      "Epoch 55/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 5.0753 - SMAPE: 9.2035 - val_loss: 4.0236 - val_SMAPE: 11.2253\n",
      "Epoch 56/200\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 4.8951 - SMAPE: 8.8828 - val_loss: 4.2010 - val_SMAPE: 11.7090\n",
      "Epoch 57/200\n",
      "58/58 [==============================] - 6s 112ms/step - loss: 4.8391 - SMAPE: 8.7163 - val_loss: 4.0070 - val_SMAPE: 11.1696\n",
      "Epoch 58/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 4.6627 - SMAPE: 8.3867 - val_loss: 3.8476 - val_SMAPE: 10.7850\n",
      "Epoch 59/200\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 4.5920 - SMAPE: 8.2698 - val_loss: 3.6962 - val_SMAPE: 10.3027\n",
      "Epoch 60/200\n",
      "58/58 [==============================] - 6s 108ms/step - loss: 4.3854 - SMAPE: 8.0712 - val_loss: 3.7771 - val_SMAPE: 10.5361\n",
      "Epoch 61/200\n",
      "58/58 [==============================] - 6s 112ms/step - loss: 4.2762 - SMAPE: 7.7391 - val_loss: 3.5935 - val_SMAPE: 10.0497\n",
      "Epoch 62/200\n",
      "58/58 [==============================] - 7s 120ms/step - loss: 4.2167 - SMAPE: 7.6670 - val_loss: 3.8018 - val_SMAPE: 10.6309\n",
      "Epoch 63/200\n",
      "58/58 [==============================] - 7s 119ms/step - loss: 4.0943 - SMAPE: 7.5540 - val_loss: 3.6699 - val_SMAPE: 10.4064\n",
      "Epoch 64/200\n",
      "58/58 [==============================] - 7s 122ms/step - loss: 3.9936 - SMAPE: 7.2998 - val_loss: 3.5828 - val_SMAPE: 10.1607\n",
      "Epoch 65/200\n",
      "58/58 [==============================] - 8s 132ms/step - loss: 4.0594 - SMAPE: 7.5274 - val_loss: 3.7015 - val_SMAPE: 10.5652\n",
      "Epoch 66/200\n",
      "58/58 [==============================] - 7s 115ms/step - loss: 3.8717 - SMAPE: 7.3705 - val_loss: 3.6821 - val_SMAPE: 10.4239\n",
      "Epoch 67/200\n",
      "58/58 [==============================] - 6s 110ms/step - loss: 3.8097 - SMAPE: 7.0050 - val_loss: 3.5055 - val_SMAPE: 9.8813\n",
      "Epoch 68/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 3.7586 - SMAPE: 7.0435 - val_loss: 3.6695 - val_SMAPE: 10.3506\n",
      "Epoch 69/200\n",
      "58/58 [==============================] - 6s 102ms/step - loss: 3.6861 - SMAPE: 6.7836 - val_loss: 3.4554 - val_SMAPE: 9.8564\n",
      "Epoch 70/200\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 3.5705 - SMAPE: 6.6976 - val_loss: 3.4670 - val_SMAPE: 9.8209\n",
      "Epoch 71/200\n",
      "58/58 [==============================] - 6s 102ms/step - loss: 3.6345 - SMAPE: 6.9067 - val_loss: 4.0295 - val_SMAPE: 11.3273\n",
      "Epoch 72/200\n",
      "58/58 [==============================] - 5s 93ms/step - loss: 3.6658 - SMAPE: 6.9181 - val_loss: 3.5442 - val_SMAPE: 10.0914\n",
      "Epoch 73/200\n",
      "58/58 [==============================] - 5s 95ms/step - loss: 3.5084 - SMAPE: 6.5031 - val_loss: 3.5712 - val_SMAPE: 10.2155\n",
      "Epoch 74/200\n",
      "58/58 [==============================] - 6s 97ms/step - loss: 3.4514 - SMAPE: 6.4240 - val_loss: 3.4344 - val_SMAPE: 9.7981\n",
      "Epoch 75/200\n",
      "58/58 [==============================] - 6s 96ms/step - loss: 3.3570 - SMAPE: 6.2620 - val_loss: 3.5798 - val_SMAPE: 10.0800\n",
      "Epoch 76/200\n",
      "58/58 [==============================] - 6s 97ms/step - loss: 3.4430 - SMAPE: 6.4801 - val_loss: 3.3354 - val_SMAPE: 9.5197\n",
      "Epoch 77/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 3.2959 - SMAPE: 6.1200 - val_loss: 3.5329 - val_SMAPE: 10.1008\n",
      "Epoch 78/200\n",
      "58/58 [==============================] - 6s 100ms/step - loss: 3.2760 - SMAPE: 6.0872 - val_loss: 3.3410 - val_SMAPE: 9.5398\n",
      "Epoch 79/200\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 3.2653 - SMAPE: 6.2224 - val_loss: 3.3656 - val_SMAPE: 9.6080\n",
      "Epoch 80/200\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 3.2224 - SMAPE: 6.0051 - val_loss: 3.3298 - val_SMAPE: 9.4936\n",
      "Epoch 81/200\n",
      "58/58 [==============================] - 6s 96ms/step - loss: 3.1466 - SMAPE: 5.9057 - val_loss: 3.3934 - val_SMAPE: 9.6542\n",
      "Epoch 82/200\n",
      "58/58 [==============================] - 6s 95ms/step - loss: 3.1427 - SMAPE: 6.0790 - val_loss: 3.2861 - val_SMAPE: 9.3920\n",
      "Epoch 83/200\n",
      "58/58 [==============================] - 6s 96ms/step - loss: 3.0640 - SMAPE: 5.6987 - val_loss: 3.2467 - val_SMAPE: 9.2894\n",
      "Epoch 84/200\n",
      "58/58 [==============================] - 6s 97ms/step - loss: 3.0302 - SMAPE: 5.7110 - val_loss: 3.1771 - val_SMAPE: 9.0334\n",
      "Epoch 85/200\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 3.0123 - SMAPE: 5.6252 - val_loss: 3.2555 - val_SMAPE: 9.3445\n",
      "Epoch 86/200\n",
      "58/58 [==============================] - 6s 95ms/step - loss: 2.9980 - SMAPE: 5.6953 - val_loss: 3.2040 - val_SMAPE: 9.2245\n",
      "Epoch 87/200\n",
      "58/58 [==============================] - 6s 96ms/step - loss: 3.0019 - SMAPE: 5.6145 - val_loss: 3.1282 - val_SMAPE: 8.8901\n",
      "Epoch 88/200\n",
      "58/58 [==============================] - 6s 100ms/step - loss: 2.9621 - SMAPE: 5.5480 - val_loss: 3.3800 - val_SMAPE: 9.6567\n",
      "Epoch 89/200\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 2.9297 - SMAPE: 5.4941 - val_loss: 3.2648 - val_SMAPE: 9.2931\n",
      "Epoch 90/200\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 2.8910 - SMAPE: 5.3957 - val_loss: 3.0747 - val_SMAPE: 8.7661\n",
      "Epoch 91/200\n",
      "58/58 [==============================] - 6s 100ms/step - loss: 2.8726 - SMAPE: 5.3610 - val_loss: 3.2126 - val_SMAPE: 9.1675\n",
      "Epoch 92/200\n",
      "58/58 [==============================] - 6s 100ms/step - loss: 2.8398 - SMAPE: 5.3568 - val_loss: 3.1545 - val_SMAPE: 9.0194\n",
      "Epoch 93/200\n",
      "58/58 [==============================] - 6s 99ms/step - loss: 2.8288 - SMAPE: 5.2746 - val_loss: 3.0803 - val_SMAPE: 8.7840\n",
      "Epoch 94/200\n",
      "58/58 [==============================] - 6s 103ms/step - loss: 2.8161 - SMAPE: 5.3328 - val_loss: 3.0870 - val_SMAPE: 8.8488\n",
      "Epoch 95/200\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 2.8108 - SMAPE: 5.3246 - val_loss: 3.0277 - val_SMAPE: 8.6531\n",
      "Epoch 96/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 2.7618 - SMAPE: 5.1562 - val_loss: 3.0987 - val_SMAPE: 8.7565\n",
      "Epoch 97/200\n",
      "58/58 [==============================] - 6s 112ms/step - loss: 2.7700 - SMAPE: 5.2650 - val_loss: 3.0055 - val_SMAPE: 8.5921\n",
      "Epoch 98/200\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 2.7187 - SMAPE: 5.1273 - val_loss: 3.0232 - val_SMAPE: 8.6482\n",
      "Epoch 99/200\n",
      "58/58 [==============================] - 6s 108ms/step - loss: 2.7326 - SMAPE: 5.1526 - val_loss: 3.1044 - val_SMAPE: 8.7419\n",
      "Epoch 100/200\n",
      "58/58 [==============================] - 7s 112ms/step - loss: 2.7081 - SMAPE: 5.1004 - val_loss: 3.0580 - val_SMAPE: 8.7108\n",
      "Epoch 101/200\n",
      "58/58 [==============================] - 8s 130ms/step - loss: 2.6708 - SMAPE: 5.0035 - val_loss: 2.9787 - val_SMAPE: 8.4221\n",
      "Epoch 102/200\n",
      "58/58 [==============================] - 7s 115ms/step - loss: 2.6681 - SMAPE: 5.0878 - val_loss: 3.0124 - val_SMAPE: 8.5636\n",
      "Epoch 103/200\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 2.7182 - SMAPE: 5.0974 - val_loss: 3.0334 - val_SMAPE: 8.5336\n",
      "Epoch 104/200\n",
      "58/58 [==============================] - 6s 102ms/step - loss: 2.6202 - SMAPE: 4.9046 - val_loss: 3.0219 - val_SMAPE: 8.5092\n",
      "Epoch 105/200\n",
      "58/58 [==============================] - 6s 107ms/step - loss: 2.6224 - SMAPE: 4.9415 - val_loss: 3.0985 - val_SMAPE: 8.7431\n",
      "Epoch 106/200\n",
      "58/58 [==============================] - 6s 103ms/step - loss: 2.6301 - SMAPE: 5.2098 - val_loss: 2.9402 - val_SMAPE: 8.3535\n",
      "Epoch 107/200\n",
      "58/58 [==============================] - 6s 99ms/step - loss: 2.6265 - SMAPE: 4.9707 - val_loss: 2.9849 - val_SMAPE: 8.5815\n",
      "Epoch 108/200\n",
      "58/58 [==============================] - 6s 101ms/step - loss: 2.5584 - SMAPE: 4.8263 - val_loss: 2.9868 - val_SMAPE: 8.4901\n",
      "Epoch 109/200\n",
      "58/58 [==============================] - 6s 100ms/step - loss: 2.5489 - SMAPE: 4.8253 - val_loss: 2.9597 - val_SMAPE: 8.4234\n",
      "Epoch 110/200\n",
      "58/58 [==============================] - 6s 103ms/step - loss: 2.5459 - SMAPE: 4.7740 - val_loss: 3.0058 - val_SMAPE: 8.5509\n",
      "Epoch 111/200\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 2.5296 - SMAPE: 4.7689 - val_loss: 3.0652 - val_SMAPE: 8.6415\n",
      "Epoch 112/200\n",
      "58/58 [==============================] - 6s 102ms/step - loss: 2.5248 - SMAPE: 4.8040 - val_loss: 2.8582 - val_SMAPE: 8.1387\n",
      "Epoch 113/200\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 2.5332 - SMAPE: 4.7739 - val_loss: 2.9445 - val_SMAPE: 8.3522\n",
      "Epoch 114/200\n",
      "58/58 [==============================] - 6s 101ms/step - loss: 2.6053 - SMAPE: 4.9213 - val_loss: 3.1743 - val_SMAPE: 9.0742\n",
      "Epoch 115/200\n",
      "58/58 [==============================] - 6s 102ms/step - loss: 2.5268 - SMAPE: 4.7887 - val_loss: 2.9471 - val_SMAPE: 8.3449\n",
      "Epoch 116/200\n",
      "58/58 [==============================] - 6s 100ms/step - loss: 2.5584 - SMAPE: 4.8055 - val_loss: 3.0824 - val_SMAPE: 8.7495\n",
      "Epoch 117/200\n",
      "58/58 [==============================] - 6s 107ms/step - loss: 2.4875 - SMAPE: 4.6825 - val_loss: 2.8341 - val_SMAPE: 8.1619\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 6s 100ms/step - loss: 2.4860 - SMAPE: 4.6744 - val_loss: 2.8137 - val_SMAPE: 8.0911\n",
      "Epoch 119/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 2.4431 - SMAPE: 4.5597 - val_loss: 2.8170 - val_SMAPE: 8.0761\n",
      "Epoch 120/200\n",
      "58/58 [==============================] - 6s 101ms/step - loss: 2.4311 - SMAPE: 4.5569 - val_loss: 2.9772 - val_SMAPE: 8.4211\n",
      "Epoch 121/200\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 2.4351 - SMAPE: 4.7070 - val_loss: 2.8617 - val_SMAPE: 8.1654\n",
      "Epoch 122/200\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 2.4556 - SMAPE: 4.6422 - val_loss: 2.9756 - val_SMAPE: 8.4351\n",
      "Epoch 123/200\n",
      "58/58 [==============================] - 6s 97ms/step - loss: 2.4051 - SMAPE: 4.5632 - val_loss: 2.8037 - val_SMAPE: 7.9759\n",
      "Epoch 124/200\n",
      "58/58 [==============================] - 6s 102ms/step - loss: 2.3796 - SMAPE: 4.4703 - val_loss: 2.8493 - val_SMAPE: 8.1519\n",
      "Epoch 125/200\n",
      "58/58 [==============================] - 6s 102ms/step - loss: 2.3735 - SMAPE: 4.4579 - val_loss: 2.9639 - val_SMAPE: 8.4262\n",
      "Epoch 126/200\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 2.3579 - SMAPE: 4.4338 - val_loss: 3.1588 - val_SMAPE: 8.8902\n",
      "Epoch 127/200\n",
      "58/58 [==============================] - 6s 102ms/step - loss: 2.4169 - SMAPE: 4.5625 - val_loss: 3.0542 - val_SMAPE: 8.6362\n",
      "Epoch 128/200\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 2.4420 - SMAPE: 4.7071 - val_loss: 3.0559 - val_SMAPE: 8.6901\n",
      "Epoch 129/200\n",
      "58/58 [==============================] - 6s 100ms/step - loss: 2.4352 - SMAPE: 4.5672 - val_loss: 2.9165 - val_SMAPE: 8.3813\n",
      "Epoch 130/200\n",
      "58/58 [==============================] - 6s 107ms/step - loss: 2.3119 - SMAPE: 4.3264 - val_loss: 2.8702 - val_SMAPE: 8.1795\n",
      "Epoch 131/200\n",
      "58/58 [==============================] - 6s 103ms/step - loss: 2.3311 - SMAPE: 4.3810 - val_loss: 2.7758 - val_SMAPE: 8.0161\n",
      "Epoch 132/200\n",
      "58/58 [==============================] - 6s 99ms/step - loss: 2.2989 - SMAPE: 4.3198 - val_loss: 2.8133 - val_SMAPE: 8.1077\n",
      "Epoch 133/200\n",
      "58/58 [==============================] - 6s 101ms/step - loss: 2.2801 - SMAPE: 4.2813 - val_loss: 2.8522 - val_SMAPE: 8.1528\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameters</th>\n",
       "      <th>sMAPE_train</th>\n",
       "      <th>sMAPE_val</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSO</th>\n",
       "      <td>None</td>\n",
       "      <td>16.03</td>\n",
       "      <td>16.922</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn1</th>\n",
       "      <td>{'Dense1': 59, 'Dense2': 239, 'Dense3': 1}</td>\n",
       "      <td>2.744</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn2</th>\n",
       "      <td>{'Dense1': 59, 'Dense2': 239, 'Dense3': 162, '...</td>\n",
       "      <td>3.721</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn3</th>\n",
       "      <td>{'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...</td>\n",
       "      <td>4.557</td>\n",
       "      <td>7.967</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn4</th>\n",
       "      <td>{'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...</td>\n",
       "      <td>6.24</td>\n",
       "      <td>9.812</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn5</th>\n",
       "      <td>{'LSTM1': 83, 'LSTM2': 24, 'TimeDistributed': ...</td>\n",
       "      <td>4.413</td>\n",
       "      <td>7.982</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Parameters sMAPE_train sMAPE_val  \\\n",
       "TSO                                               None       16.03    16.922   \n",
       "nn1         {'Dense1': 59, 'Dense2': 239, 'Dense3': 1}       2.744      4.35   \n",
       "nn2  {'Dense1': 59, 'Dense2': 239, 'Dense3': 162, '...       3.721      3.97   \n",
       "nn3  {'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...       4.557     7.967   \n",
       "nn4  {'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...        6.24     9.812   \n",
       "nn5  {'LSTM1': 83, 'LSTM2': 24, 'TimeDistributed': ...       4.413     7.982   \n",
       "\n",
       "    r2_train r2_val  \n",
       "TSO    0.954  0.971  \n",
       "nn1    0.982  0.978  \n",
       "nn2    0.979  0.977  \n",
       "nn3    0.933  0.895  \n",
       "nn4    0.883  0.843  \n",
       "nn5    0.941  0.894  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Shape\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(83, activation='tanh', input_shape=input_shape))\n",
    "nn.add(layers.RepeatVector(y_train.shape[1]))\n",
    "nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and Fit\n",
    "nn5 = compile_fit(nn, (X_train, y_train), (X_val, y_val))\n",
    "\n",
    "# Append results\n",
    "params = {'LSTM1':83,\n",
    "          'LSTM2':24,\n",
    "          'TimeDistributed':1,\n",
    "          'input_win':'7-days'}\n",
    "\n",
    "# Compute metrics and add to table\n",
    "results_actual['nn5'] = compute_metrics(nn5, params, (X_train,y_train), (X_val,y_val))\n",
    "results_actual.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Shape\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(60, activation='tanh', input_shape=input_shape))\n",
    "nn.add(layers.RepeatVector(y_train.shape[1]))\n",
    "nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 7s 80ms/step - loss: 53.9754 - SMAPE: 179.1431 - val_loss: 35.6774 - val_SMAPE: 148.6349\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 4s 69ms/step - loss: 50.0152 - SMAPE: 154.3059 - val_loss: 33.4253 - val_SMAPE: 131.5672\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 4s 68ms/step - loss: 48.1394 - SMAPE: 143.7396 - val_loss: 31.7212 - val_SMAPE: 119.7844\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 4s 68ms/step - loss: 46.5005 - SMAPE: 135.2807 - val_loss: 30.1331 - val_SMAPE: 109.5757\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 4s 69ms/step - loss: 44.9406 - SMAPE: 127.1814 - val_loss: 28.5983 - val_SMAPE: 100.3414\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 4s 69ms/step - loss: 43.4204 - SMAPE: 120.0789 - val_loss: 27.0997 - val_SMAPE: 91.8631\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 4s 68ms/step - loss: 41.9306 - SMAPE: 113.1782 - val_loss: 25.6314 - val_SMAPE: 84.0301\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 4s 69ms/step - loss: 40.4648 - SMAPE: 106.8161 - val_loss: 24.1879 - val_SMAPE: 76.7509\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 4s 69ms/step - loss: 39.0203 - SMAPE: 100.7677 - val_loss: 22.7687 - val_SMAPE: 69.9666\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 4s 69ms/step - loss: 37.5950 - SMAPE: 95.1417 - val_loss: 21.3802 - val_SMAPE: 63.6783\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 4s 70ms/step - loss: 36.1882 - SMAPE: 89.5534 - val_loss: 20.0186 - val_SMAPE: 57.8136\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 4s 70ms/step - loss: 34.7951 - SMAPE: 84.2749 - val_loss: 18.6875 - val_SMAPE: 52.3639\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 4s 74ms/step - loss: 33.4012 - SMAPE: 79.9382 - val_loss: 17.3988 - val_SMAPE: 47.3470\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 32.0207 - SMAPE: 74.8949 - val_loss: 16.1012 - val_SMAPE: 42.5165\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 4s 74ms/step - loss: 30.6694 - SMAPE: 70.8481 - val_loss: 14.8577 - val_SMAPE: 38.1486\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 5s 80ms/step - loss: 29.2950 - SMAPE: 66.0192 - val_loss: 13.6653 - val_SMAPE: 34.2500\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 4s 76ms/step - loss: 27.9753 - SMAPE: 62.3832 - val_loss: 12.5472 - val_SMAPE: 30.8526\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 4s 76ms/step - loss: 26.6434 - SMAPE: 58.0426 - val_loss: 11.6078 - val_SMAPE: 28.2248\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 6s 96ms/step - loss: 25.3438 - SMAPE: 53.7323 - val_loss: 10.5280 - val_SMAPE: 25.3163\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 5s 79ms/step - loss: 24.0971 - SMAPE: 50.7502 - val_loss: 9.8963 - val_SMAPE: 23.9156\n",
      "Epoch 21/200\n",
      "58/58 [==============================] - 5s 79ms/step - loss: 22.8379 - SMAPE: 47.7222 - val_loss: 9.0475 - val_SMAPE: 21.8182\n",
      "Epoch 22/200\n",
      "58/58 [==============================] - 5s 79ms/step - loss: 21.6201 - SMAPE: 43.9020 - val_loss: 8.2560 - val_SMAPE: 19.6096\n",
      "Epoch 23/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 20.4638 - SMAPE: 41.0260 - val_loss: 7.6544 - val_SMAPE: 18.1519\n",
      "Epoch 24/200\n",
      "58/58 [==============================] - 5s 79ms/step - loss: 19.2611 - SMAPE: 38.2743 - val_loss: 7.2607 - val_SMAPE: 17.6677\n",
      "Epoch 25/200\n",
      "58/58 [==============================] - 5s 79ms/step - loss: 18.1403 - SMAPE: 35.8664 - val_loss: 6.9418 - val_SMAPE: 17.0745\n",
      "Epoch 26/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 17.0538 - SMAPE: 32.7484 - val_loss: 6.2830 - val_SMAPE: 15.3524\n",
      "Epoch 27/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 16.0570 - SMAPE: 30.5464 - val_loss: 6.0597 - val_SMAPE: 14.8805\n",
      "Epoch 28/200\n",
      "58/58 [==============================] - 5s 79ms/step - loss: 15.1045 - SMAPE: 28.3802 - val_loss: 6.1894 - val_SMAPE: 15.7214\n",
      "Epoch 29/200\n",
      "58/58 [==============================] - 5s 83ms/step - loss: 14.2331 - SMAPE: 26.8976 - val_loss: 5.8281 - val_SMAPE: 14.6439\n",
      "Epoch 30/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 13.4388 - SMAPE: 25.1791 - val_loss: 6.1443 - val_SMAPE: 15.2454\n",
      "Epoch 31/200\n",
      "58/58 [==============================] - 4s 78ms/step - loss: 12.7155 - SMAPE: 23.7848 - val_loss: 6.5157 - val_SMAPE: 16.3340\n",
      "Epoch 32/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 12.0354 - SMAPE: 21.9511 - val_loss: 5.8845 - val_SMAPE: 14.2741\n",
      "Epoch 33/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 11.5271 - SMAPE: 21.1159 - val_loss: 5.7602 - val_SMAPE: 14.4704\n",
      "Epoch 34/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 11.0460 - SMAPE: 20.1463 - val_loss: 6.3706 - val_SMAPE: 16.0944\n",
      "Epoch 35/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 10.4260 - SMAPE: 18.8354 - val_loss: 6.9513 - val_SMAPE: 16.7232\n",
      "Epoch 36/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 10.0501 - SMAPE: 18.4467 - val_loss: 6.7669 - val_SMAPE: 16.3920\n",
      "Epoch 37/200\n",
      "58/58 [==============================] - 5s 80ms/step - loss: 9.6106 - SMAPE: 17.3889 - val_loss: 6.2549 - val_SMAPE: 15.3314\n",
      "Epoch 38/200\n",
      "58/58 [==============================] - 5s 82ms/step - loss: 9.1800 - SMAPE: 16.3592 - val_loss: 6.0509 - val_SMAPE: 14.8547\n",
      "Epoch 39/200\n",
      "58/58 [==============================] - 5s 89ms/step - loss: 8.8092 - SMAPE: 15.6813 - val_loss: 4.9344 - val_SMAPE: 12.5177\n",
      "Epoch 40/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 8.4294 - SMAPE: 14.9869 - val_loss: 4.4892 - val_SMAPE: 11.5279\n",
      "Epoch 41/200\n",
      "58/58 [==============================] - 5s 81ms/step - loss: 8.1055 - SMAPE: 14.8161 - val_loss: 4.9154 - val_SMAPE: 12.6447\n",
      "Epoch 42/200\n",
      "58/58 [==============================] - 5s 90ms/step - loss: 7.7638 - SMAPE: 14.0358 - val_loss: 4.7557 - val_SMAPE: 12.2463\n",
      "Epoch 43/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 7.4789 - SMAPE: 13.4571 - val_loss: 4.1744 - val_SMAPE: 10.6562\n",
      "Epoch 44/200\n",
      "58/58 [==============================] - 4s 77ms/step - loss: 7.0869 - SMAPE: 12.8707 - val_loss: 4.3050 - val_SMAPE: 11.0990\n",
      "Epoch 45/200\n",
      "58/58 [==============================] - 4s 74ms/step - loss: 6.8592 - SMAPE: 12.2133 - val_loss: 4.0398 - val_SMAPE: 10.4692\n",
      "Epoch 46/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 6.6029 - SMAPE: 11.9040 - val_loss: 4.0187 - val_SMAPE: 10.4885\n",
      "Epoch 47/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 6.3731 - SMAPE: 11.2724 - val_loss: 4.1091 - val_SMAPE: 10.7941\n",
      "Epoch 48/200\n",
      "58/58 [==============================] - 4s 69ms/step - loss: 6.2365 - SMAPE: 11.0751 - val_loss: 3.8580 - val_SMAPE: 10.0517\n",
      "Epoch 49/200\n",
      "58/58 [==============================] - 4s 73ms/step - loss: 5.9498 - SMAPE: 10.6119 - val_loss: 4.0627 - val_SMAPE: 10.6839\n",
      "Epoch 50/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 5.8950 - SMAPE: 10.4671 - val_loss: 3.9183 - val_SMAPE: 10.4531\n",
      "Epoch 51/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 5.6197 - SMAPE: 10.0507 - val_loss: 3.6973 - val_SMAPE: 9.6648\n",
      "Epoch 52/200\n",
      "58/58 [==============================] - 5s 77ms/step - loss: 5.4232 - SMAPE: 9.6615 - val_loss: 3.8229 - val_SMAPE: 10.0488\n",
      "Epoch 53/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 5.2651 - SMAPE: 9.3385 - val_loss: 4.1180 - val_SMAPE: 11.0597\n",
      "Epoch 54/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 5.1484 - SMAPE: 9.3999 - val_loss: 3.9368 - val_SMAPE: 10.4121\n",
      "Epoch 55/200\n",
      "58/58 [==============================] - 4s 66ms/step - loss: 5.1759 - SMAPE: 9.3046 - val_loss: 3.9705 - val_SMAPE: 10.3187\n",
      "Epoch 56/200\n",
      "58/58 [==============================] - 4s 73ms/step - loss: 4.8979 - SMAPE: 8.7527 - val_loss: 3.4132 - val_SMAPE: 9.1312\n",
      "Epoch 57/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 4.7182 - SMAPE: 8.4113 - val_loss: 3.6445 - val_SMAPE: 9.7054\n",
      "Epoch 58/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 4.6560 - SMAPE: 8.3379 - val_loss: 3.3392 - val_SMAPE: 9.1037\n",
      "Epoch 59/200\n",
      "58/58 [==============================] - 4s 68ms/step - loss: 4.4387 - SMAPE: 8.2393 - val_loss: 3.1976 - val_SMAPE: 8.7123\n",
      "Epoch 60/200\n",
      "58/58 [==============================] - 4s 67ms/step - loss: 4.3263 - SMAPE: 7.8962 - val_loss: 3.3558 - val_SMAPE: 9.2007\n",
      "Epoch 61/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 4.2467 - SMAPE: 7.7100 - val_loss: 3.3928 - val_SMAPE: 9.2684\n",
      "Epoch 62/200\n",
      "58/58 [==============================] - 4s 67ms/step - loss: 4.1374 - SMAPE: 7.4302 - val_loss: 3.3190 - val_SMAPE: 9.0055\n",
      "Epoch 63/200\n",
      "58/58 [==============================] - 4s 68ms/step - loss: 4.0673 - SMAPE: 7.4665 - val_loss: 3.3541 - val_SMAPE: 9.4891\n",
      "Epoch 64/200\n",
      "58/58 [==============================] - 4s 70ms/step - loss: 3.9087 - SMAPE: 7.0885 - val_loss: 3.2599 - val_SMAPE: 9.1016\n",
      "Epoch 65/200\n",
      "58/58 [==============================] - 4s 68ms/step - loss: 3.8604 - SMAPE: 6.9912 - val_loss: 3.0526 - val_SMAPE: 8.3714\n",
      "Epoch 66/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 3.7342 - SMAPE: 6.8340 - val_loss: 3.1421 - val_SMAPE: 8.8317\n",
      "Epoch 67/200\n",
      "58/58 [==============================] - 4s 69ms/step - loss: 3.6954 - SMAPE: 6.7124 - val_loss: 2.9610 - val_SMAPE: 8.1296\n",
      "Epoch 68/200\n",
      "58/58 [==============================] - 4s 70ms/step - loss: 3.6173 - SMAPE: 6.5846 - val_loss: 3.0837 - val_SMAPE: 8.7798\n",
      "Epoch 69/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 3.6206 - SMAPE: 6.6930 - val_loss: 3.1051 - val_SMAPE: 8.5479\n",
      "Epoch 70/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 3.5175 - SMAPE: 6.4420 - val_loss: 2.8956 - val_SMAPE: 8.0881\n",
      "Epoch 71/200\n",
      "58/58 [==============================] - 4s 73ms/step - loss: 3.4583 - SMAPE: 6.2945 - val_loss: 3.2398 - val_SMAPE: 8.8425\n",
      "Epoch 72/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 3.4163 - SMAPE: 6.2996 - val_loss: 3.1181 - val_SMAPE: 8.8626\n",
      "Epoch 73/200\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 3.3544 - SMAPE: 6.1328 - val_loss: 3.0145 - val_SMAPE: 8.2394\n",
      "Epoch 74/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 3.3245 - SMAPE: 6.1022 - val_loss: 3.3334 - val_SMAPE: 9.3292\n",
      "Epoch 75/200\n",
      "58/58 [==============================] - 4s 73ms/step - loss: 3.2642 - SMAPE: 6.0311 - val_loss: 3.1102 - val_SMAPE: 8.6813\n",
      "Epoch 76/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 3.1888 - SMAPE: 5.9109 - val_loss: 3.1870 - val_SMAPE: 9.0147\n",
      "Epoch 77/200\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 3.1315 - SMAPE: 5.7972 - val_loss: 3.1131 - val_SMAPE: 8.4354\n",
      "Epoch 78/200\n",
      "58/58 [==============================] - 4s 73ms/step - loss: 3.1420 - SMAPE: 5.8563 - val_loss: 3.0952 - val_SMAPE: 8.6696\n",
      "Epoch 79/200\n",
      "58/58 [==============================] - 4s 74ms/step - loss: 3.0867 - SMAPE: 5.7489 - val_loss: 2.9827 - val_SMAPE: 8.4009\n",
      "Epoch 80/200\n",
      "58/58 [==============================] - 5s 78ms/step - loss: 3.0281 - SMAPE: 5.5720 - val_loss: 2.9404 - val_SMAPE: 8.2023\n"
     ]
    }
   ],
   "source": [
    "nn3 = compile_fit(nn, (X_train, y_train), (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSO</th>\n",
       "      <th>nn1</th>\n",
       "      <th>nn2</th>\n",
       "      <th>nn3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sMAPE_train</th>\n",
       "      <td>16.030</td>\n",
       "      <td>2.910315</td>\n",
       "      <td>3.962906</td>\n",
       "      <td>6.398275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sMAPE_val</th>\n",
       "      <td>16.922</td>\n",
       "      <td>4.179681</td>\n",
       "      <td>3.454599</td>\n",
       "      <td>7.789913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_train</th>\n",
       "      <td>0.954</td>\n",
       "      <td>0.980623</td>\n",
       "      <td>0.983838</td>\n",
       "      <td>0.869208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_val</th>\n",
       "      <td>0.971</td>\n",
       "      <td>0.977432</td>\n",
       "      <td>0.980697</td>\n",
       "      <td>0.853570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                TSO       nn1       nn2       nn3\n",
       "sMAPE_train  16.030  2.910315  3.962906  6.398275\n",
       "sMAPE_val    16.922  4.179681  3.454599  7.789913\n",
       "r2_train      0.954  0.980623  0.983838  0.869208\n",
       "r2_val        0.971  0.977432  0.980697  0.853570"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_actual['nn3'] = compute_metrics(nn3,(X_train,y_train), (X_val,y_val))\n",
    "results_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input shape\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(83, activation='tanh', input_shape=input_shape))\n",
    "nn.add(layers.RepeatVector(y_train.shape[1]))\n",
    "nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compilte and Fit\n",
    "nn4 = compile_fit(nn, (X_train, y_train), (X_val, y_val))\n",
    "\n",
    "# Compute metrics and add to table\n",
    "results_actual['nn4'] = compute_metrics(nn4,(X_train,y_train), (X_val,y_val))\n",
    "results_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 9s 115ms/step - loss: 53.0826 - SMAPE: 173.9812 - val_loss: 34.9559 - val_SMAPE: 142.9651\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 49.5362 - SMAPE: 150.9076 - val_loss: 33.0619 - val_SMAPE: 128.9688\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 7s 114ms/step - loss: 47.8252 - SMAPE: 141.9558 - val_loss: 31.4508 - val_SMAPE: 117.9920\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 46.2551 - SMAPE: 133.8312 - val_loss: 29.9100 - val_SMAPE: 108.1947\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 7s 120ms/step - loss: 44.7333 - SMAPE: 126.7064 - val_loss: 28.4060 - val_SMAPE: 99.2231\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 6s 112ms/step - loss: 43.2401 - SMAPE: 118.9267 - val_loss: 26.9321 - val_SMAPE: 90.9449\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 6s 111ms/step - loss: 41.7710 - SMAPE: 112.6276 - val_loss: 25.4815 - val_SMAPE: 83.2552\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 6s 111ms/step - loss: 40.3222 - SMAPE: 106.4034 - val_loss: 24.0542 - val_SMAPE: 76.0955\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 6s 111ms/step - loss: 38.8916 - SMAPE: 100.3831 - val_loss: 22.6485 - val_SMAPE: 69.4085\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 37.4780 - SMAPE: 94.8776 - val_loss: 21.2707 - val_SMAPE: 63.1951\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 6s 110ms/step - loss: 36.0807 - SMAPE: 89.6828 - val_loss: 19.9223 - val_SMAPE: 57.4090\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 7s 115ms/step - loss: 34.6975 - SMAPE: 84.4981 - val_loss: 18.5981 - val_SMAPE: 52.0070\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 6s 111ms/step - loss: 33.3288 - SMAPE: 79.6008 - val_loss: 17.2992 - val_SMAPE: 46.9473\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 6s 110ms/step - loss: 31.9722 - SMAPE: 74.3512 - val_loss: 16.0288 - val_SMAPE: 42.2515\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 6s 110ms/step - loss: 30.6097 - SMAPE: 69.9676 - val_loss: 14.7871 - val_SMAPE: 38.0187\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 6s 110ms/step - loss: 29.2911 - SMAPE: 66.1647 - val_loss: 13.6569 - val_SMAPE: 34.2216\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 6s 103ms/step - loss: 27.9402 - SMAPE: 62.0668 - val_loss: 12.5522 - val_SMAPE: 30.8606\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 26.6219 - SMAPE: 58.2891 - val_loss: 11.4426 - val_SMAPE: 27.7477\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 25.3116 - SMAPE: 53.9854 - val_loss: 10.6455 - val_SMAPE: 25.7268\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 6s 100ms/step - loss: 24.0348 - SMAPE: 50.0483 - val_loss: 9.9129 - val_SMAPE: 23.9680\n",
      "Epoch 21/200\n",
      "58/58 [==============================] - 5s 92ms/step - loss: 22.8120 - SMAPE: 47.1704 - val_loss: 9.3150 - val_SMAPE: 22.5713\n",
      "Epoch 22/200\n",
      "58/58 [==============================] - 5s 95ms/step - loss: 21.6637 - SMAPE: 43.9233 - val_loss: 8.5308 - val_SMAPE: 20.7063\n",
      "Epoch 23/200\n",
      "58/58 [==============================] - 5s 90ms/step - loss: 20.4143 - SMAPE: 41.2565 - val_loss: 7.5838 - val_SMAPE: 17.9901\n",
      "Epoch 24/200\n",
      "58/58 [==============================] - 5s 88ms/step - loss: 19.2503 - SMAPE: 38.6822 - val_loss: 7.2330 - val_SMAPE: 17.3220\n",
      "Epoch 25/200\n",
      "58/58 [==============================] - 5s 89ms/step - loss: 18.1064 - SMAPE: 35.1629 - val_loss: 6.7119 - val_SMAPE: 16.1162\n",
      "Epoch 26/200\n",
      "58/58 [==============================] - 5s 88ms/step - loss: 17.0871 - SMAPE: 33.3659 - val_loss: 6.6001 - val_SMAPE: 16.2012\n",
      "Epoch 27/200\n",
      "58/58 [==============================] - 5s 88ms/step - loss: 16.0338 - SMAPE: 30.3473 - val_loss: 6.1709 - val_SMAPE: 15.2121\n",
      "Epoch 28/200\n",
      "58/58 [==============================] - 5s 88ms/step - loss: 15.1046 - SMAPE: 28.8735 - val_loss: 6.5177 - val_SMAPE: 16.5046\n",
      "Epoch 29/200\n",
      "58/58 [==============================] - 5s 91ms/step - loss: 14.2719 - SMAPE: 26.5232 - val_loss: 6.1015 - val_SMAPE: 15.2303\n",
      "Epoch 30/200\n",
      "58/58 [==============================] - 6s 99ms/step - loss: 13.5406 - SMAPE: 25.0490 - val_loss: 6.8308 - val_SMAPE: 17.8982\n",
      "Epoch 31/200\n",
      "58/58 [==============================] - 5s 93ms/step - loss: 13.0750 - SMAPE: 24.0811 - val_loss: 8.5015 - val_SMAPE: 23.0571\n",
      "Epoch 32/200\n",
      "58/58 [==============================] - 5s 92ms/step - loss: 12.2835 - SMAPE: 22.8935 - val_loss: 8.0141 - val_SMAPE: 20.5470\n",
      "Epoch 33/200\n",
      "58/58 [==============================] - 6s 97ms/step - loss: 11.6586 - SMAPE: 21.3427 - val_loss: 6.2931 - val_SMAPE: 15.8071\n",
      "Epoch 34/200\n",
      "58/58 [==============================] - 5s 94ms/step - loss: 11.1574 - SMAPE: 20.6124 - val_loss: 6.8705 - val_SMAPE: 17.3878\n",
      "Epoch 35/200\n",
      "58/58 [==============================] - 6s 96ms/step - loss: 10.5446 - SMAPE: 19.4753 - val_loss: 6.5463 - val_SMAPE: 16.2771\n",
      "Epoch 36/200\n",
      "58/58 [==============================] - 6s 97ms/step - loss: 10.1086 - SMAPE: 18.1892 - val_loss: 6.0280 - val_SMAPE: 15.3112\n",
      "Epoch 37/200\n",
      "58/58 [==============================] - 6s 99ms/step - loss: 9.6774 - SMAPE: 17.4095 - val_loss: 6.5858 - val_SMAPE: 16.7705\n"
     ]
    }
   ],
   "source": [
    "nn4 = compile_fit(nn, (X_train, y_train), (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSO</th>\n",
       "      <th>nn1</th>\n",
       "      <th>nn2</th>\n",
       "      <th>nn3</th>\n",
       "      <th>nn4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sMAPE_train</th>\n",
       "      <td>16.030</td>\n",
       "      <td>2.910315</td>\n",
       "      <td>3.962906</td>\n",
       "      <td>6.398275</td>\n",
       "      <td>29.473468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sMAPE_val</th>\n",
       "      <td>16.922</td>\n",
       "      <td>4.179681</td>\n",
       "      <td>3.454599</td>\n",
       "      <td>7.789913</td>\n",
       "      <td>15.500838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_train</th>\n",
       "      <td>0.954</td>\n",
       "      <td>0.980623</td>\n",
       "      <td>0.983838</td>\n",
       "      <td>0.869208</td>\n",
       "      <td>0.234718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_val</th>\n",
       "      <td>0.971</td>\n",
       "      <td>0.977432</td>\n",
       "      <td>0.980697</td>\n",
       "      <td>0.853570</td>\n",
       "      <td>0.365850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                TSO       nn1       nn2       nn3        nn4\n",
       "sMAPE_train  16.030  2.910315  3.962906  6.398275  29.473468\n",
       "sMAPE_val    16.922  4.179681  3.454599  7.789913  15.500838\n",
       "r2_train      0.954  0.980623  0.983838  0.869208   0.234718\n",
       "r2_val        0.971  0.977432  0.980697  0.853570   0.365850"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_actual['nn4'] = compute_metrics(nn4,(X_train,y_train), (X_val,y_val))\n",
    "results_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Model (LSTM-DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 1s 7ms/step - loss: 12.9258 - SMAPE: 33.8020 - val_loss: 2.0069 - val_SMAPE: 7.4309\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.8762 - SMAPE: 6.4971 - val_loss: 1.9000 - val_SMAPE: 6.2775\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.8492 - SMAPE: 5.9981 - val_loss: 1.9445 - val_SMAPE: 5.8030\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.4853 - SMAPE: 5.0428 - val_loss: 1.8522 - val_SMAPE: 5.6545\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.4158 - SMAPE: 4.7639 - val_loss: 2.0548 - val_SMAPE: 6.0862\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.2650 - SMAPE: 4.4600 - val_loss: 2.1696 - val_SMAPE: 6.4270\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.2761 - SMAPE: 4.4573 - val_loss: 3.0143 - val_SMAPE: 8.2934\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.1986 - SMAPE: 4.3664 - val_loss: 3.5865 - val_SMAPE: 9.5707\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.2295 - SMAPE: 4.2532 - val_loss: 2.1688 - val_SMAPE: 6.2765\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.2007 - SMAPE: 4.2129 - val_loss: 2.5162 - val_SMAPE: 7.1304\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.1040 - SMAPE: 4.0826 - val_loss: 2.7559 - val_SMAPE: 7.6785\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.1720 - SMAPE: 4.1482 - val_loss: 2.6795 - val_SMAPE: 7.5277\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.2498 - SMAPE: 4.2680 - val_loss: 3.0746 - val_SMAPE: 8.2730\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2.2305 - SMAPE: 4.2352 - val_loss: 3.1169 - val_SMAPE: 8.5327\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and validation\n",
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "\n",
    "# Get the x cols for lstm network, lagged cols\n",
    "X_train_lstm = X_train.filter(regex='lag')\n",
    "X_train_dnn = X_train.drop(columns=X_train_lstm.columns)\n",
    "\n",
    "# Get the x cols for dnn network, forecast cols\n",
    "X_val_lstm = X_val.filter(regex='lag')\n",
    "X_val_dnn = X_val.drop(columns=X_val_lstm.columns)\n",
    "\n",
    "# Reorganize the training and testing data into batches\n",
    "X_train_dnn, y_train_dnn = resample((X_train_dnn, y_train), 24, 24, 24)\n",
    "X_val_dnn, y_val_dnn = resample((X_val_dnn, y_val), 24, 24, 24)\n",
    "\n",
    "# LSTM\n",
    "X_train_lstm, y_train_lstm = resample((X_train_lstm, y_train), 24, 24, 24)\n",
    "X_val_lstm, y_val_lstm = resample((X_val_lstm, y_val), 24, 24, 24)\n",
    "\n",
    "# Specify input shape for dnn\n",
    "input_shape = (X_train_dnn.shape[1], X_train_dnn.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(59, activation='relu', input_shape=input_shape))\n",
    "nn.add(layers.Dense(239, activation='relu'))\n",
    "nn.add(layers.Dense(162, activation='relu'))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and fit dnn\n",
    "dnn = compile_fit(nn, (X_train_dnn, y_train_dnn), (X_val_dnn, y_val_dnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 4s 37ms/step - loss: 51.2939 - SMAPE: 162.8393 - val_loss: 31.0259 - val_SMAPE: 127.2584\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 2s 27ms/step - loss: 47.8606 - SMAPE: 141.7239 - val_loss: 29.3516 - val_SMAPE: 115.6471\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 2s 28ms/step - loss: 46.2993 - SMAPE: 134.1461 - val_loss: 27.8614 - val_SMAPE: 106.0632\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 2s 27ms/step - loss: 44.8218 - SMAPE: 126.7473 - val_loss: 26.4185 - val_SMAPE: 97.3674\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 2s 27ms/step - loss: 43.3815 - SMAPE: 119.8425 - val_loss: 25.0121 - val_SMAPE: 89.3987\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 2s 26ms/step - loss: 41.9516 - SMAPE: 113.5953 - val_loss: 23.5573 - val_SMAPE: 81.6373\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 1s 26ms/step - loss: 40.4186 - SMAPE: 106.6010 - val_loss: 22.0763 - val_SMAPE: 74.1975\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 1s 25ms/step - loss: 38.9226 - SMAPE: 100.0661 - val_loss: 20.6675 - val_SMAPE: 67.5152\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 2s 28ms/step - loss: 37.4703 - SMAPE: 94.5603 - val_loss: 19.3132 - val_SMAPE: 61.4327\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 1s 25ms/step - loss: 36.0452 - SMAPE: 88.9488 - val_loss: 18.0142 - val_SMAPE: 55.9003\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 1s 25ms/step - loss: 34.6415 - SMAPE: 84.1834 - val_loss: 16.7659 - val_SMAPE: 50.8469\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 1s 26ms/step - loss: 33.2560 - SMAPE: 78.3178 - val_loss: 15.5797 - val_SMAPE: 46.2653\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 2s 27ms/step - loss: 31.8857 - SMAPE: 74.2515 - val_loss: 14.4649 - val_SMAPE: 42.1506\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 2s 26ms/step - loss: 30.5330 - SMAPE: 69.4972 - val_loss: 13.4192 - val_SMAPE: 38.4553\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 1s 25ms/step - loss: 29.2033 - SMAPE: 65.9324 - val_loss: 12.4656 - val_SMAPE: 35.2122\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 1s 26ms/step - loss: 27.8917 - SMAPE: 61.6782 - val_loss: 11.6066 - val_SMAPE: 32.3944\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 2s 27ms/step - loss: 26.6014 - SMAPE: 58.1229 - val_loss: 10.8720 - val_SMAPE: 30.0596\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 2s 29ms/step - loss: 25.3369 - SMAPE: 54.3989 - val_loss: 10.2682 - val_SMAPE: 28.1892\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 24.0998 - SMAPE: 50.7561 - val_loss: 9.7998 - val_SMAPE: 26.7612\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 2s 37ms/step - loss: 22.8930 - SMAPE: 47.7603 - val_loss: 9.4559 - val_SMAPE: 25.7141\n",
      "Epoch 21/200\n",
      "58/58 [==============================] - 2s 26ms/step - loss: 21.7191 - SMAPE: 44.2667 - val_loss: 9.2292 - val_SMAPE: 25.0050\n",
      "Epoch 22/200\n",
      "58/58 [==============================] - 1s 24ms/step - loss: 20.5857 - SMAPE: 41.2024 - val_loss: 9.1086 - val_SMAPE: 24.5843\n",
      "Epoch 23/200\n",
      "58/58 [==============================] - 1s 25ms/step - loss: 19.5097 - SMAPE: 39.0688 - val_loss: 9.0885 - val_SMAPE: 24.4354\n",
      "Epoch 24/200\n",
      "58/58 [==============================] - 2s 29ms/step - loss: 18.4737 - SMAPE: 36.4687 - val_loss: 9.1707 - val_SMAPE: 24.5392\n",
      "Epoch 25/200\n",
      "58/58 [==============================] - 2s 27ms/step - loss: 17.4949 - SMAPE: 33.9890 - val_loss: 9.3521 - val_SMAPE: 24.8689\n",
      "Epoch 26/200\n",
      "58/58 [==============================] - 2s 26ms/step - loss: 16.5937 - SMAPE: 32.0017 - val_loss: 9.6293 - val_SMAPE: 25.4016\n",
      "Epoch 27/200\n",
      "58/58 [==============================] - 2s 28ms/step - loss: 15.7850 - SMAPE: 30.0312 - val_loss: 9.9746 - val_SMAPE: 26.0672\n",
      "Epoch 28/200\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 15.0246 - SMAPE: 28.5170 - val_loss: 10.3895 - val_SMAPE: 26.8611\n",
      "Epoch 29/200\n",
      "58/58 [==============================] - 2s 30ms/step - loss: 14.3467 - SMAPE: 26.9006 - val_loss: 10.8563 - val_SMAPE: 27.7474\n",
      "Epoch 30/200\n",
      "58/58 [==============================] - 2s 30ms/step - loss: 13.7565 - SMAPE: 25.6299 - val_loss: 11.3427 - val_SMAPE: 28.6575\n",
      "Epoch 31/200\n",
      "58/58 [==============================] - 2s 29ms/step - loss: 13.2303 - SMAPE: 24.7661 - val_loss: 11.8614 - val_SMAPE: 29.6180\n",
      "Epoch 32/200\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 12.7703 - SMAPE: 23.7502 - val_loss: 12.3988 - val_SMAPE: 30.5992\n",
      "Epoch 33/200\n",
      "58/58 [==============================] - 2s 29ms/step - loss: 12.3793 - SMAPE: 22.8914 - val_loss: 12.9259 - val_SMAPE: 31.5506\n"
     ]
    }
   ],
   "source": [
    "# Specify shape for lstm\n",
    "input_shape = (X_train_lstm.shape[1], X_train_lstm.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(83, activation='tanh', input_shape=input_shape))\n",
    "nn.add(layers.RepeatVector(y_train_lstm.shape[1]))\n",
    "nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and fit lstm\n",
    "lstm = compile_fit(nn, (X_train_lstm, y_train_lstm), (X_val_lstm, y_val_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 3s 21ms/step - loss: 29.4925 - SMAPE: 69.1477 - val_loss: 6.3737 - val_SMAPE: 16.9974\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 7.8801 - SMAPE: 14.8873 - val_loss: 9.6204 - val_SMAPE: 24.8604\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 3.9114 - SMAPE: 7.6440 - val_loss: 5.1135 - val_SMAPE: 14.5524\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.4627 - SMAPE: 4.8456 - val_loss: 3.1878 - val_SMAPE: 9.2735\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 1s 13ms/step - loss: 2.1665 - SMAPE: 4.2100 - val_loss: 2.7010 - val_SMAPE: 7.8748\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.1329 - SMAPE: 4.1971 - val_loss: 2.7133 - val_SMAPE: 7.8926\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1327 - SMAPE: 4.1674 - val_loss: 2.6508 - val_SMAPE: 7.7309\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.1243 - SMAPE: 4.3738 - val_loss: 2.9366 - val_SMAPE: 8.4497\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.1201 - SMAPE: 4.1337 - val_loss: 2.7913 - val_SMAPE: 8.0734\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 2.1212 - SMAPE: 4.1669 - val_loss: 3.0376 - val_SMAPE: 8.6995\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1286 - SMAPE: 4.1631 - val_loss: 2.6585 - val_SMAPE: 7.7567\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.1095 - SMAPE: 4.2076 - val_loss: 2.6357 - val_SMAPE: 7.6948\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.1197 - SMAPE: 4.1660 - val_loss: 2.7810 - val_SMAPE: 8.0563\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1104 - SMAPE: 4.1857 - val_loss: 2.5826 - val_SMAPE: 7.5530\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.1178 - SMAPE: 4.1457 - val_loss: 2.5697 - val_SMAPE: 7.5322\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.1029 - SMAPE: 4.1223 - val_loss: 2.9486 - val_SMAPE: 8.4860\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.1118 - SMAPE: 4.1266 - val_loss: 2.6349 - val_SMAPE: 7.6937\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1068 - SMAPE: 4.1171 - val_loss: 2.6393 - val_SMAPE: 7.7070\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1278 - SMAPE: 4.1926 - val_loss: 2.6241 - val_SMAPE: 7.6688\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1078 - SMAPE: 4.1425 - val_loss: 2.7858 - val_SMAPE: 8.0689\n",
      "Epoch 21/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1201 - SMAPE: 4.1502 - val_loss: 3.1732 - val_SMAPE: 9.0411\n",
      "Epoch 22/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1359 - SMAPE: 4.2137 - val_loss: 2.5332 - val_SMAPE: 7.4468\n",
      "Epoch 23/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.1273 - SMAPE: 4.2239 - val_loss: 3.4911 - val_SMAPE: 9.8204\n",
      "Epoch 24/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1532 - SMAPE: 4.2425 - val_loss: 2.4242 - val_SMAPE: 7.1520\n",
      "Epoch 25/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1281 - SMAPE: 4.1657 - val_loss: 2.6965 - val_SMAPE: 7.8408\n",
      "Epoch 26/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1085 - SMAPE: 4.1319 - val_loss: 2.9670 - val_SMAPE: 8.5227\n",
      "Epoch 27/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1159 - SMAPE: 4.1622 - val_loss: 2.8523 - val_SMAPE: 8.2397\n",
      "Epoch 28/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1132 - SMAPE: 4.2316 - val_loss: 3.0256 - val_SMAPE: 8.6740\n",
      "Epoch 29/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1366 - SMAPE: 4.2371 - val_loss: 2.5786 - val_SMAPE: 7.5481\n",
      "Epoch 30/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.1199 - SMAPE: 4.1476 - val_loss: 2.6553 - val_SMAPE: 7.7422\n",
      "Epoch 31/200\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.1149 - SMAPE: 4.1285 - val_loss: 2.5237 - val_SMAPE: 7.4125\n",
      "Epoch 32/200\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 2.1071 - SMAPE: 4.1324 - val_loss: 2.8040 - val_SMAPE: 8.1151\n",
      "Epoch 33/200\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2.1092 - SMAPE: 4.2139 - val_loss: 3.0875 - val_SMAPE: 8.8220\n",
      "Epoch 34/200\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 2.1251 - SMAPE: 4.1602 - val_loss: 2.5356 - val_SMAPE: 7.4391\n"
     ]
    }
   ],
   "source": [
    "# Build combine dnn and lstm in ensemble neural network\n",
    "ensemble = ensemble_nn([dnn, lstm])\n",
    "\n",
    "# Compile and fit the ensemble network\n",
    "nn6 = compile_fit(ensemble, ([X_train_dnn, X_train_lstm], y_train_dnn), ([X_val_dnn, X_val_lstm], y_val_dnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameters</th>\n",
       "      <th>sMAPE_train</th>\n",
       "      <th>sMAPE_val</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSO</th>\n",
       "      <td>None</td>\n",
       "      <td>16.03</td>\n",
       "      <td>16.922</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn1</th>\n",
       "      <td>{'Dense1': 59, 'Dense2': 239, 'Dense3': 1}</td>\n",
       "      <td>2.744</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn2</th>\n",
       "      <td>{'Dense1': 59, 'Dense2': 239, 'Dense3': 162, '...</td>\n",
       "      <td>3.721</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn3</th>\n",
       "      <td>{'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...</td>\n",
       "      <td>4.557</td>\n",
       "      <td>7.967</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn4</th>\n",
       "      <td>{'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...</td>\n",
       "      <td>6.24</td>\n",
       "      <td>9.812</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn5</th>\n",
       "      <td>{'LSTM1': 83, 'LSTM2': 24, 'TimeDistributed': ...</td>\n",
       "      <td>4.413</td>\n",
       "      <td>7.982</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn6</th>\n",
       "      <td>{'dnn': ['Dense1 - 59', 'Dense2 - 239', 'Dense...</td>\n",
       "      <td>4.234</td>\n",
       "      <td>7.149</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Parameters sMAPE_train sMAPE_val  \\\n",
       "TSO                                               None       16.03    16.922   \n",
       "nn1         {'Dense1': 59, 'Dense2': 239, 'Dense3': 1}       2.744      4.35   \n",
       "nn2  {'Dense1': 59, 'Dense2': 239, 'Dense3': 162, '...       3.721      3.97   \n",
       "nn3  {'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...       4.557     7.967   \n",
       "nn4  {'LSTM1': 60, 'LSTM2': 24, 'TimeDistributed': ...        6.24     9.812   \n",
       "nn5  {'LSTM1': 83, 'LSTM2': 24, 'TimeDistributed': ...       4.413     7.982   \n",
       "nn6  {'dnn': ['Dense1 - 59', 'Dense2 - 239', 'Dense...       4.234     7.149   \n",
       "\n",
       "    r2_train r2_val  \n",
       "TSO    0.954  0.971  \n",
       "nn1    0.982  0.978  \n",
       "nn2    0.979  0.977  \n",
       "nn3    0.933  0.895  \n",
       "nn4    0.883  0.843  \n",
       "nn5    0.941  0.894  \n",
       "nn6    0.955  0.963  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'dnn':['Dense1 - 59','Dense2 - 239','Dense2 - 162', 'TimeDistributed - 1'],\n",
    "          'lstm':['LSTM - 83', 'LSTM - 24','TimeDistributed - 1']}\n",
    "\n",
    "results_actual['nn6'] = compute_metrics(nn6,\n",
    "                                        params,\n",
    "                                        ([X_train_dnn, X_train_lstm], y_train_dnn),\n",
    "                                        ([X_val_dnn, X_val_lstm],y_val_dnn))\n",
    "results_actual.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find Optimal DNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation\n",
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "\n",
    "# Get the x cols for lstm network, lagged cols\n",
    "X_train_lstm = X_train.filter(regex='lag')\n",
    "X_train_dnn = X_train.drop(columns=X_train_lstm.columns)\n",
    "\n",
    "# Get the x cols for dnn network, forecast cols\n",
    "X_val_lstm = X_val.filter(regex='lag')\n",
    "X_val_dnn = X_val.drop(columns=X_val_lstm.columns)\n",
    "\n",
    "# Reorganize the training and testing data into batches\n",
    "X_train_dnn, y_train_dnn = resample((X_train_dnn, y_train), 24, 24, 24)\n",
    "X_val_dnn, y_val_dnn = resample((X_val_dnn, y_val), 24, 24, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 1s 6ms/step - loss: 10.2232 - SMAPE: 25.9317 - val_loss: 2.0234 - val_SMAPE: 6.9500\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.8417 - SMAPE: 6.9592 - val_loss: 1.8433 - val_SMAPE: 6.2413\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.7930 - SMAPE: 5.9195 - val_loss: 2.2850 - val_SMAPE: 6.4726\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.6040 - SMAPE: 5.2516 - val_loss: 2.3266 - val_SMAPE: 6.7413\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.4849 - SMAPE: 4.8491 - val_loss: 2.0624 - val_SMAPE: 6.2279\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.5557 - SMAPE: 4.9518 - val_loss: 2.0965 - val_SMAPE: 6.2502\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.3329 - SMAPE: 4.8340 - val_loss: 2.0499 - val_SMAPE: 6.1459\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.3649 - SMAPE: 4.6035 - val_loss: 1.9955 - val_SMAPE: 6.0033\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.2742 - SMAPE: 4.3981 - val_loss: 2.4488 - val_SMAPE: 6.9789\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.1505 - SMAPE: 4.1504 - val_loss: 3.1749 - val_SMAPE: 8.6343\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.2968 - SMAPE: 4.3734 - val_loss: 2.2791 - val_SMAPE: 6.5867\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.1729 - SMAPE: 4.2442 - val_loss: 3.5859 - val_SMAPE: 9.5369\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.2884 - SMAPE: 4.3840 - val_loss: 1.9834 - val_SMAPE: 5.8778\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.1498 - SMAPE: 4.1284 - val_loss: 1.8640 - val_SMAPE: 5.5148\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.2084 - SMAPE: 4.1788 - val_loss: 2.3138 - val_SMAPE: 6.6079\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.0537 - SMAPE: 3.9248 - val_loss: 2.7184 - val_SMAPE: 7.5129\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.1586 - SMAPE: 4.1231 - val_loss: 2.2983 - val_SMAPE: 6.5918\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.0684 - SMAPE: 3.9647 - val_loss: 1.7081 - val_SMAPE: 5.0513\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.1468 - SMAPE: 4.1370 - val_loss: 2.0213 - val_SMAPE: 5.9789\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.2491 - SMAPE: 4.2115 - val_loss: 4.5876 - val_SMAPE: 11.8410\n",
      "Epoch 21/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.1350 - SMAPE: 4.0092 - val_loss: 2.7043 - val_SMAPE: 7.6337\n",
      "Epoch 22/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.0460 - SMAPE: 3.8674 - val_loss: 2.4027 - val_SMAPE: 6.8970\n",
      "Epoch 23/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.1276 - SMAPE: 3.9871 - val_loss: 3.1914 - val_SMAPE: 8.7598\n",
      "Epoch 24/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.0763 - SMAPE: 3.9044 - val_loss: 3.5639 - val_SMAPE: 9.6687\n",
      "Epoch 25/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.2562 - SMAPE: 4.2225 - val_loss: 2.3442 - val_SMAPE: 6.7091\n",
      "Epoch 26/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.0582 - SMAPE: 3.8779 - val_loss: 2.2004 - val_SMAPE: 6.3594\n",
      "Epoch 27/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.0011 - SMAPE: 3.9116 - val_loss: 4.6707 - val_SMAPE: 12.1547\n",
      "Epoch 28/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.1960 - SMAPE: 4.1869 - val_loss: 2.2407 - val_SMAPE: 6.3853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Dense1': 59, 'Dense2': 239, 'Dense3': 162, 'TimeDistributed': 1},\n",
       " 5.403,\n",
       " 5.089,\n",
       " 0.961,\n",
       " 0.971]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input_shape\n",
    "input_shape = (X_train_dnn.shape[1], X_train_dnn.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(59, activation='relu', input_shape=input_shape))\n",
    "nn.add(layers.Dense(239, activation='relu'))\n",
    "nn.add(layers.Dense(162, activation='relu'))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and Fit\n",
    "dnn = compile_fit(nn, (X_train_dnn, y_train_dnn), (X_val_dnn, y_val_dnn))\n",
    "\n",
    "params = {\n",
    "    'Dense1':59,\n",
    "    'Dense2':239,\n",
    "    'Dense3':162,\n",
    "    'TimeDistributed':1,\n",
    "}\n",
    "compute_metrics(dnn, params, (X_train_dnn, y_train_dnn), (X_val_dnn, y_val_dnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 1s 4ms/step - loss: 21.3318 - SMAPE: 57.4670 - val_loss: 1.9528 - val_SMAPE: 7.1790\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.7912 - SMAPE: 6.2061 - val_loss: 1.8255 - val_SMAPE: 6.0815\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.5871 - SMAPE: 5.5212 - val_loss: 2.0126 - val_SMAPE: 6.0136\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.3944 - SMAPE: 4.8211 - val_loss: 2.0827 - val_SMAPE: 6.2232\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.2884 - SMAPE: 4.5134 - val_loss: 2.1573 - val_SMAPE: 6.4496\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.2530 - SMAPE: 4.3760 - val_loss: 2.0766 - val_SMAPE: 6.3123\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.2033 - SMAPE: 4.2830 - val_loss: 2.5279 - val_SMAPE: 7.3094\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.1488 - SMAPE: 4.1610 - val_loss: 2.2234 - val_SMAPE: 6.5639\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.1633 - SMAPE: 4.1754 - val_loss: 2.8555 - val_SMAPE: 7.9440\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.1105 - SMAPE: 4.0627 - val_loss: 2.3740 - val_SMAPE: 6.8539\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.1929 - SMAPE: 4.1914 - val_loss: 3.3626 - val_SMAPE: 9.1747\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.1141 - SMAPE: 4.0453 - val_loss: 2.5348 - val_SMAPE: 7.1917\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.0774 - SMAPE: 4.0013 - val_loss: 3.0251 - val_SMAPE: 8.2836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Dense1': 59, 'Dense2': 184, 'TimeDistributed': 1},\n",
       " 5.064,\n",
       " 6.032,\n",
       " 0.954,\n",
       " 0.96]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input_shape\n",
    "input_shape = (X_train_dnn.shape[1], X_train_dnn.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(59, activation='relu', input_shape=input_shape))\n",
    "nn.add(layers.Dense(184, activation='relu'))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and Fit\n",
    "dnn1 = compile_fit(nn, (X_train_dnn, y_train_dnn), (X_val_dnn, y_val_dnn))\n",
    "\n",
    "params = {\n",
    "    'Dense1':59,\n",
    "    'Dense2':184,\n",
    "    'TimeDistributed':1,\n",
    "}\n",
    "compute_metrics(dnn1, params, (X_train_dnn, y_train_dnn), (X_val_dnn, y_val_dnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 30.7502 - SMAPE: 83.6909 - val_loss: 1.9972 - val_SMAPE: 7.1738\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 3.1367 - SMAPE: 6.8278 - val_loss: 1.8919 - val_SMAPE: 6.7974\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.7179 - SMAPE: 6.2400 - val_loss: 1.9459 - val_SMAPE: 6.4917\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.6290 - SMAPE: 5.8891 - val_loss: 1.8560 - val_SMAPE: 5.9709\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.5100 - SMAPE: 5.3971 - val_loss: 1.7908 - val_SMAPE: 5.5446\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.3995 - SMAPE: 4.9953 - val_loss: 2.2540 - val_SMAPE: 6.3752\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.3212 - SMAPE: 4.6851 - val_loss: 2.0430 - val_SMAPE: 5.9659\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.2436 - SMAPE: 4.4660 - val_loss: 2.1231 - val_SMAPE: 6.1929\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.2161 - SMAPE: 4.4817 - val_loss: 2.1644 - val_SMAPE: 6.2793\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.1658 - SMAPE: 4.2803 - val_loss: 2.2422 - val_SMAPE: 6.4948\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.1551 - SMAPE: 4.4412 - val_loss: 2.7588 - val_SMAPE: 7.6799\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.1454 - SMAPE: 4.1824 - val_loss: 2.2914 - val_SMAPE: 6.5815\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 2.1262 - SMAPE: 4.1192 - val_loss: 2.1659 - val_SMAPE: 6.3191\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.0886 - SMAPE: 4.0997 - val_loss: 2.7068 - val_SMAPE: 7.5244\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.0755 - SMAPE: 4.0283 - val_loss: 2.2991 - val_SMAPE: 6.5688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Dense1': 184, 'TimeDistributed': 1}, 5.276, 5.556, 0.956, 0.967]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input_shape\n",
    "input_shape = (X_train_dnn.shape[1], X_train_dnn.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(184, activation='relu', input_shape=input_shape))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and Fit\n",
    "dnn2 = compile_fit(nn, (X_train_dnn, y_train_dnn), (X_val_dnn, y_val_dnn))\n",
    "\n",
    "params = {\n",
    "    'Dense1':184,\n",
    "    'TimeDistributed':1,\n",
    "}\n",
    "compute_metrics(dnn2, params, (X_train_dnn, y_train_dnn), (X_val_dnn, y_val_dnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find Optimal LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, create resample the data in 1-14 day input windows and save the arrays in to data folder.  This way, will not have to resample each time want to change the input window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for days in range(1,15):\n",
    "    X_train_temp, y_train_temp = resample((X_train_lstm, y_train), 24*days, 24, 24)\n",
    "    X_val_temp, y_val_temp = resample((X_val_lstm, y_val), 24*days, 24, 24)\n",
    "    \n",
    "    # Save data\n",
    "    np.save(f'../data/nn_data/X_train_inputWin_{days}_days', X_train_temp)\n",
    "    np.save(f'../data/nn_data/X_val_inputWin_{days}_days', X_val_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, run lstm models with varying input_windows. Record results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input_window (days)</th>\n",
       "      <th>sMAPE_train</th>\n",
       "      <th>sMAPE_val</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>38.518</td>\n",
       "      <td>24.193</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>37.481</td>\n",
       "      <td>24.189</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>36.835</td>\n",
       "      <td>24.210</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>37.960</td>\n",
       "      <td>24.187</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>36.714</td>\n",
       "      <td>24.215</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>36.618</td>\n",
       "      <td>24.221</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>36.855</td>\n",
       "      <td>24.209</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>36.697</td>\n",
       "      <td>24.217</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>37.072</td>\n",
       "      <td>24.199</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>36.988</td>\n",
       "      <td>24.202</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Input_window (days)  sMAPE_train  sMAPE_val  r2_train  r2_val\n",
       "1                   1.0       38.518     24.193     0.006   0.007\n",
       "2                   2.0       37.481     24.189     0.006   0.007\n",
       "3                   3.0       36.835     24.210     0.006   0.007\n",
       "4                   4.0       37.960     24.187     0.006   0.007\n",
       "5                   5.0       36.714     24.215     0.006   0.007\n",
       "6                   6.0       36.618     24.221     0.006   0.007\n",
       "7                   7.0       36.855     24.209     0.006   0.007\n",
       "8                   8.0       36.697     24.217     0.006   0.007\n",
       "9                   9.0       37.072     24.199     0.006   0.007\n",
       "10                 10.0       36.988     24.202     0.006   0.007"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe to hold results\n",
    "results_lstm = pd.DataFrame(index=['Input_window (days)','sMAPE_train', 'sMAPE_val', 'r2_train', 'r2_val'])\n",
    "\n",
    "# For each number of days in input window\n",
    "for days in range(1,11):\n",
    "    \n",
    "    # Load resampled data\n",
    "    X_train_lstm = np.load(f'../data/nn_data/X_train_inputWin_{days}_days.npy')\n",
    "    X_val_lstm = np.load(f'../data/nn_data/X_val_inputWin_{days}_days.npy')\n",
    "\n",
    "    # Specify shape for lstm\n",
    "    input_shape = (X_train_lstm.shape[1], X_train_lstm.shape[2])\n",
    "\n",
    "    # Instantiate model and build layers\n",
    "    nn = models.Sequential()\n",
    "    nn.add(layers.LSTM(83, activation='tanh', input_shape=input_shape))\n",
    "    nn.add(layers.RepeatVector(y_train_lstm.shape[1]))\n",
    "    nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "    nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "    # Compile and fit lstm\n",
    "    lstm = compile_fit(nn, (X_train_lstm, y_train_lstm), (X_val_lstm, y_val_lstm))\n",
    "    results_lstm[days] = compute_metrics(lstm, days, (X_train_lstm, y_train_lstm), (X_val_lstm, y_val_lstm))\n",
    "    \n",
    "# Preview results\n",
    "results_lstm.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the input window did not change the performance of the lstm at all. The r-squared value is very low because the range of predictions is very low. See the histogram below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Reorganize the training and testing data into batches\n",
    "X_train_lstm, y_train_lstm = resample((X_train, y_train), 24*7, 24, 24)\n",
    "#X_val_dnn, y_val_dnn = resample((X_val_dnn, y_val), 24, 24, 24)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 7s 87ms/step - loss: 52.3703 - SMAPE: 169.3599 - val_loss: 32.5719 - val_SMAPE: 138.8301\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 5s 83ms/step - loss: 49.3716 - SMAPE: 150.7822 - val_loss: 30.8153 - val_SMAPE: 125.7232\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 5s 84ms/step - loss: 47.7239 - SMAPE: 141.8956 - val_loss: 29.2408 - val_SMAPE: 114.9059\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 5s 87ms/step - loss: 46.1768 - SMAPE: 133.8129 - val_loss: 27.7278 - val_SMAPE: 105.2313\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 44.6694 - SMAPE: 126.2597 - val_loss: 26.2473 - val_SMAPE: 96.3691\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 6s 103ms/step - loss: 43.1880 - SMAPE: 119.1487 - val_loss: 24.7979 - val_SMAPE: 88.2241\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 41.7276 - SMAPE: 112.4202 - val_loss: 23.3795 - val_SMAPE: 80.7190\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 40.2861 - SMAPE: 106.1313 - val_loss: 21.9916 - val_SMAPE: 73.7836\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 38.8617 - SMAPE: 100.4964 - val_loss: 20.6333 - val_SMAPE: 67.3563\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 6s 108ms/step - loss: 37.4539 - SMAPE: 93.9291 - val_loss: 19.3134 - val_SMAPE: 61.4328\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 36.0625 - SMAPE: 89.2715 - val_loss: 18.0430 - val_SMAPE: 56.0191\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 34.6847 - SMAPE: 83.2114 - val_loss: 16.8135 - val_SMAPE: 51.0344\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 6s 110ms/step - loss: 33.3274 - SMAPE: 79.6685 - val_loss: 15.6502 - val_SMAPE: 46.5309\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 6s 112ms/step - loss: 31.9743 - SMAPE: 74.7834 - val_loss: 14.5434 - val_SMAPE: 42.4333\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 7s 115ms/step - loss: 30.6352 - SMAPE: 70.4841 - val_loss: 13.4985 - val_SMAPE: 38.7294\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 7s 118ms/step - loss: 29.3091 - SMAPE: 65.9163 - val_loss: 12.5399 - val_SMAPE: 35.4598\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 28.0041 - SMAPE: 62.0601 - val_loss: 11.6835 - val_SMAPE: 32.6421\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 8s 131ms/step - loss: 26.7239 - SMAPE: 58.5517 - val_loss: 10.9393 - val_SMAPE: 30.2700\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 8s 132ms/step - loss: 25.4650 - SMAPE: 54.9808 - val_loss: 10.3256 - val_SMAPE: 28.3649\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 8s 132ms/step - loss: 24.2344 - SMAPE: 51.3448 - val_loss: 9.8443 - val_SMAPE: 26.8961\n",
      "Epoch 21/200\n",
      "58/58 [==============================] - 8s 132ms/step - loss: 23.0311 - SMAPE: 47.7860 - val_loss: 9.4900 - val_SMAPE: 25.8181\n",
      "Epoch 22/200\n",
      "58/58 [==============================] - 8s 132ms/step - loss: 21.8616 - SMAPE: 45.1328 - val_loss: 9.2519 - val_SMAPE: 25.0784\n",
      "Epoch 23/200\n",
      "58/58 [==============================] - 8s 133ms/step - loss: 20.7290 - SMAPE: 42.2736 - val_loss: 9.1189 - val_SMAPE: 24.6244\n",
      "Epoch 24/200\n",
      "58/58 [==============================] - 8s 132ms/step - loss: 19.6351 - SMAPE: 39.3445 - val_loss: 9.0856 - val_SMAPE: 24.4393\n",
      "Epoch 25/200\n",
      "58/58 [==============================] - 8s 130ms/step - loss: 18.5914 - SMAPE: 37.0077 - val_loss: 9.1548 - val_SMAPE: 24.5129\n",
      "Epoch 26/200\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 17.6101 - SMAPE: 34.3805 - val_loss: 9.3247 - val_SMAPE: 24.8167\n",
      "Epoch 27/200\n",
      "58/58 [==============================] - 7s 122ms/step - loss: 16.6962 - SMAPE: 32.6276 - val_loss: 9.5928 - val_SMAPE: 25.3311\n",
      "Epoch 28/200\n",
      "58/58 [==============================] - 7s 116ms/step - loss: 15.8582 - SMAPE: 30.4788 - val_loss: 9.9404 - val_SMAPE: 26.0013\n",
      "Epoch 29/200\n",
      "58/58 [==============================] - 7s 117ms/step - loss: 15.0917 - SMAPE: 29.0316 - val_loss: 10.3517 - val_SMAPE: 26.7891\n",
      "Epoch 30/200\n",
      "58/58 [==============================] - 6s 112ms/step - loss: 14.4042 - SMAPE: 27.0472 - val_loss: 10.8113 - val_SMAPE: 27.6624\n",
      "Epoch 31/200\n",
      "58/58 [==============================] - 6s 107ms/step - loss: 13.8022 - SMAPE: 26.1855 - val_loss: 11.2990 - val_SMAPE: 28.5762\n",
      "Epoch 32/200\n",
      "58/58 [==============================] - 6s 107ms/step - loss: 13.2662 - SMAPE: 24.5861 - val_loss: 11.8201 - val_SMAPE: 29.5420\n",
      "Epoch 33/200\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 12.8091 - SMAPE: 23.7476 - val_loss: 12.3438 - val_SMAPE: 30.4995\n",
      "Epoch 34/200\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 12.4201 - SMAPE: 23.2605 - val_loss: 12.8667 - val_SMAPE: 31.4444\n"
     ]
    }
   ],
   "source": [
    "'''# Split the data into train and validation\n",
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "\n",
    "# Get the x cols for lstm network, lagged cols\n",
    "X_train_lstm = X_train.filter(regex='lag')\n",
    "X_train_dnn = X_train.drop(columns=X_train_lstm.columns)\n",
    "\n",
    "# Get the x cols for dnn network, forecast cols\n",
    "X_val_lstm = X_val.filter(regex='lag')\n",
    "X_val_dnn = X_val.drop(columns=X_val_lstm.columns)\n",
    "\n",
    "# Reorganize the training and testing data into batches\n",
    "X_train_lstm, y_train_lstm = resample((X_train_lstm, y_train), 24*7, 24, 24)\n",
    "X_val_lstm, y_val_lstm = resample((X_val_lstm, y_val), 24*7, 24, 24)\n",
    "\n",
    "\n",
    "# Specify shape for lstm\n",
    "input_shape = (X_train_lstm.shape[1], X_train_lstm.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(83, activation='tanh', input_shape=input_shape))\n",
    "nn.add(layers.RepeatVector(y_train_lstm.shape[1]))\n",
    "nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and fit lstm\n",
    "lstm = compile_fit(nn, (X_train_lstm, y_train_lstm), (X_val_lstm, y_val_lstm))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#preds = lstm.predict(X_train_lstm).flatten()\n",
    "sns.set_context('notebook')\n",
    "lstm_preds = pd.DataFrame({'lstm_preds':preds})\n",
    "hist = lstm_preds.hist(bins=50);\n",
    "plt.title('Histogram: LSTM Predictions');\n",
    "plt.xlabel('Prediction');\n",
    "plt.ylabel('Count');\n",
    "plt.savefig('../images/LSTM_hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since I have already run the Lasso models and noticed that the only other important variables other than `price_day_ahead` was were `humidities_bilbao_lag`, `oil_lag`, `renewable_lag`, `waste_lag`, I'll try a few model below with input_window = 24 and use only these columns as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "58/58 [==============================] - 4s 28ms/step - loss: 51.9567 - SMAPE: 166.4658 - val_loss: 31.2894 - val_SMAPE: 129.1665\n",
      "Epoch 2/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 48.0381 - SMAPE: 143.7504 - val_loss: 29.3972 - val_SMAPE: 115.9520\n",
      "Epoch 3/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 46.2801 - SMAPE: 134.2613 - val_loss: 27.7872 - val_SMAPE: 105.6032\n",
      "Epoch 4/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 44.7031 - SMAPE: 126.2457 - val_loss: 26.2572 - val_SMAPE: 96.4296\n",
      "Epoch 5/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 43.1816 - SMAPE: 119.1672 - val_loss: 24.7781 - val_SMAPE: 88.1183\n",
      "Epoch 6/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 41.6962 - SMAPE: 112.3563 - val_loss: 23.3369 - val_SMAPE: 80.5021\n",
      "Epoch 7/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 40.2358 - SMAPE: 106.2076 - val_loss: 21.9349 - val_SMAPE: 73.5100\n",
      "Epoch 8/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 38.7970 - SMAPE: 99.8968 - val_loss: 20.5659 - val_SMAPE: 67.0474\n",
      "Epoch 9/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 37.3772 - SMAPE: 94.5169 - val_loss: 19.2385 - val_SMAPE: 61.1068\n",
      "Epoch 10/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 35.9756 - SMAPE: 89.2803 - val_loss: 17.9572 - val_SMAPE: 55.6644\n",
      "Epoch 11/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 34.5875 - SMAPE: 84.0389 - val_loss: 16.7258 - val_SMAPE: 50.6887\n",
      "Epoch 12/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 33.2149 - SMAPE: 78.5731 - val_loss: 15.5512 - val_SMAPE: 46.1581\n",
      "Epoch 13/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 31.8560 - SMAPE: 73.6850 - val_loss: 14.4463 - val_SMAPE: 42.0835\n",
      "Epoch 14/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 30.5402 - SMAPE: 70.4015 - val_loss: 13.4337 - val_SMAPE: 38.5054\n",
      "Epoch 15/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 29.2181 - SMAPE: 65.4012 - val_loss: 12.4739 - val_SMAPE: 35.2401\n",
      "Epoch 16/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 27.9162 - SMAPE: 61.4577 - val_loss: 11.6242 - val_SMAPE: 32.4511\n",
      "Epoch 17/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 26.6312 - SMAPE: 58.3726 - val_loss: 10.8913 - val_SMAPE: 30.1200\n",
      "Epoch 18/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 25.3703 - SMAPE: 54.3792 - val_loss: 10.2835 - val_SMAPE: 28.2362\n",
      "Epoch 19/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 24.1368 - SMAPE: 51.3368 - val_loss: 9.8101 - val_SMAPE: 26.7924\n",
      "Epoch 20/200\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 22.9301 - SMAPE: 48.0306 - val_loss: 9.4656 - val_SMAPE: 25.7436\n",
      "Epoch 21/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 21.7588 - SMAPE: 44.5543 - val_loss: 9.2353 - val_SMAPE: 25.0249\n",
      "Epoch 22/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 20.6249 - SMAPE: 41.8619 - val_loss: 9.1114 - val_SMAPE: 24.5954\n",
      "Epoch 23/200\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 19.5334 - SMAPE: 39.4215 - val_loss: 9.0880 - val_SMAPE: 24.4361\n",
      "Epoch 24/200\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 18.4930 - SMAPE: 36.9075 - val_loss: 9.1676 - val_SMAPE: 24.5341\n",
      "Epoch 25/200\n",
      "58/58 [==============================] - 1s 23ms/step - loss: 17.5168 - SMAPE: 34.1780 - val_loss: 9.3479 - val_SMAPE: 24.8609\n",
      "Epoch 26/200\n",
      "58/58 [==============================] - 2s 27ms/step - loss: 16.6089 - SMAPE: 31.8217 - val_loss: 9.6237 - val_SMAPE: 25.3907\n",
      "Epoch 27/200\n",
      "58/58 [==============================] - 2s 29ms/step - loss: 15.7813 - SMAPE: 30.7162 - val_loss: 9.9726 - val_SMAPE: 26.0634\n",
      "Epoch 28/200\n",
      "58/58 [==============================] - 2s 28ms/step - loss: 15.0233 - SMAPE: 28.4582 - val_loss: 10.3959 - val_SMAPE: 26.8735\n",
      "Epoch 29/200\n",
      "58/58 [==============================] - 1s 26ms/step - loss: 14.3521 - SMAPE: 27.4492 - val_loss: 10.8543 - val_SMAPE: 27.7437\n",
      "Epoch 30/200\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 13.7486 - SMAPE: 25.9893 - val_loss: 11.3530 - val_SMAPE: 28.6767\n",
      "Epoch 31/200\n",
      "58/58 [==============================] - 2s 27ms/step - loss: 13.2156 - SMAPE: 24.8582 - val_loss: 11.8768 - val_SMAPE: 29.6463\n",
      "Epoch 32/200\n",
      "58/58 [==============================] - 2s 27ms/step - loss: 12.7616 - SMAPE: 23.7835 - val_loss: 12.4115 - val_SMAPE: 30.6224\n",
      "Epoch 33/200\n",
      "58/58 [==============================] - 1s 25ms/step - loss: 12.3707 - SMAPE: 23.0108 - val_loss: 12.9431 - val_SMAPE: 31.5814\n",
      "[1, 37.723, 24.187, 0.006, 0.007]\n"
     ]
    }
   ],
   "source": [
    "days=1\n",
    "lasso_cols = ['humidities_bilbao_lag', 'oil_lag', 'renewable_lag', 'waste_lag']\n",
    "\n",
    "# Split the data into train and validation\n",
    "X_train, y_train, X_val, y_val = split_data(df_lag.drop(columns=price_drop), 2020, 'price_actual')\n",
    "\n",
    "# Get the x cols for lstm network, lagged cols\n",
    "X_train_lstm = X_train.loc[:,lasso_cols].copy()\n",
    "\n",
    "# Get the x cols for dnn network, forecast cols\n",
    "X_val_lstm = X_val.loc[:,lasso_cols].copy()\n",
    "\n",
    "# Reorganize the training and testing data into batches\n",
    "X_train_temp, y_train_temp = resample((X_train_lstm, y_train), 24*days, 24, 24)\n",
    "X_val_temp, y_val_temp = resample((X_val_lstm, y_val), 24*days, 24, 24)\n",
    "\n",
    "# Specify shape for lstm\n",
    "input_shape = (X_train_temp.shape[1], X_train_temp.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(83, activation='tanh', input_shape=input_shape))\n",
    "nn.add(layers.RepeatVector(y_train_lstm.shape[1]))\n",
    "nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and fit lstm\n",
    "lstm = compile_fit(nn, (X_train_temp, y_train_temp), (X_val_temp, y_val_temp))\n",
    "print(compute_metrics(lstm, days, (X_train_temp, y_train_temp), (X_val_temp, y_val_temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 38.399, 24.191, 0.006, 0.007]\n"
     ]
    }
   ],
   "source": [
    "days=7\n",
    "\n",
    "# Reorganize the training and testing data into batches\n",
    "X_train_temp, y_train_temp = resample((X_train_lstm, y_train), 24*days, 24, 24)\n",
    "X_val_temp, y_val_temp = resample((X_val_lstm, y_val), 24*days, 24, 24)\n",
    "\n",
    "# Specify shape for lstm\n",
    "input_shape = (X_train_temp.shape[1], X_train_temp.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(83, activation='tanh', input_shape=input_shape))\n",
    "nn.add(layers.RepeatVector(y_train_lstm.shape[1]))\n",
    "nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and fit lstm\n",
    "lstm = compile_fit(nn, (X_train_temp, y_train_temp), (X_val_temp, y_val_temp), verbose=0)\n",
    "print(compute_metrics(lstm, days, (X_train_temp, y_train_temp), (X_val_temp, y_val_temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 36.454, 24.231, 0.006, 0.007]\n"
     ]
    }
   ],
   "source": [
    "days=14\n",
    "\n",
    "# Reorganize the training and testing data into batches\n",
    "X_train_temp, y_train_temp = resample((X_train_lstm, y_train), 24*days, 24, 24)\n",
    "X_val_temp, y_val_temp = resample((X_val_lstm, y_val), 24*days, 24, 24)\n",
    "\n",
    "# Specify shape for lstm\n",
    "input_shape = (X_train_temp.shape[1], X_train_temp.shape[2])\n",
    "\n",
    "# Instantiate model and build layers\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.LSTM(83, activation='tanh', input_shape=input_shape))\n",
    "nn.add(layers.RepeatVector(y_train_lstm.shape[1]))\n",
    "nn.add(layers.LSTM(24, activation='tanh', return_sequences=True))\n",
    "nn.add(TimeDistributed(layers.Dense(1)))\n",
    "\n",
    "# Compile and fit lstm\n",
    "lstm = compile_fit(nn, (X_train_temp, y_train_temp), (X_val_temp, y_val_temp), verbose=0)\n",
    "print(compute_metrics(lstm, days, (X_train_temp, y_train_temp), (X_val_temp, y_val_temp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, using only the important columns from Lasso did not help the model improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
