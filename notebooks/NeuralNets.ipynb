{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from keras import layers, models, regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import Xception\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score, roc_curve, auc, \n",
    "                             confusion_matrix, ConfusionMatrixDisplay, classification_report)\n",
    "from sklearn.preprocessing import power_transform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import PIL\n",
    "import os\n",
    "import cv2 as cv\n",
    "import math\n",
    "import winsound\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "os.chdir('../scripts')\n",
    "from functions import impute_immediate_mean\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up alarm for notification of model completion\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv('../data/df_clean.csv', index_col=0, parse_dates=True)\n",
    "X = df.drop(columns = 'price_tomorrow')\n",
    "y = df.price_tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([], )"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby(by=[df.index.year, df.index.month, df.index.day]).count()\n",
    "grouped.loc[grouped['generation biomass']<24].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns='diff', inplace=True)\n",
    "\n",
    "continuous = X.select_dtypes(exclude='object').columns\n",
    "\n",
    "# Get rid of negatives\n",
    "time = dt.datetime(2021,3,23,22)\n",
    "X.loc[time, 'dew_point_bilbao'] = impute_immediate_mean(X['dew_point_bilbao'], time)\n",
    "\n",
    "# Add 0.0001 to everything\n",
    "X[continuous] += .0001\n",
    "\n",
    "# Box-Cox transformation\n",
    "X[continuous] = power_transform(X[continuous], method='box-cox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='diff', inplace=True)\n",
    "\n",
    "continuous = df.select_dtypes(exclude='object').drop(columns='price_tomorrow').columns\n",
    "\n",
    "# Get rid of negatives\n",
    "time = dt.datetime(2021,3,23,22)\n",
    "df.loc[time, 'dew_point_bilbao'] = impute_immediate_mean(df['dew_point_bilbao'], time)\n",
    "\n",
    "# Add 0.0001 to everything\n",
    "df[continuous] += .0001\n",
    "\n",
    "# Box-Cox transformation\n",
    "df[continuous] = power_transform(df[continuous], method='box-cox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Categorical columns\n",
    "categorical = X.select_dtypes(include='object')\n",
    "\n",
    "# Instationate LabelEncoder, fit and transform on wind_direction cols\n",
    "wind_dir_coder = LabelEncoder()\n",
    "wind_dir_coder.fit(X['wind_madrid'])\n",
    "for col in categorical.filter(regex='wind').columns:\n",
    "    X[col] = wind_dir_coder.transform(X[col])\n",
    "    \n",
    "\n",
    "# Stack condition columns into single col\n",
    "stacked_conditions = categorical.filter(regex='condition').stack()\n",
    "\n",
    "# Instantiate Label encoder, fit and transform on condition cols\n",
    "condition_coder = LabelEncoder()\n",
    "condition_coder.fit(stacked_conditions)\n",
    "for col in categorical.filter(regex='condition').columns:\n",
    "    X[col] = condition_coder.transform(X[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Categorical columns\n",
    "categorical = df.select_dtypes(include='object')\n",
    "\n",
    "# Instationate LabelEncoder, fit and transform on wind_direction cols\n",
    "wind_dir_coder = LabelEncoder()\n",
    "wind_dir_coder.fit(df['wind_madrid'])\n",
    "for col in categorical.filter(regex='wind').columns:\n",
    "    df[col] = wind_dir_coder.transform(df[col])\n",
    "    \n",
    "\n",
    "# Stack condition columns into single col\n",
    "stacked_conditions = categorical.filter(regex='condition').stack()\n",
    "\n",
    "# Instantiate Label encoder, fit and transform on condition cols\n",
    "condition_coder = LabelEncoder()\n",
    "condition_coder.fit(stacked_conditions)\n",
    "for col in categorical.filter(regex='condition').columns:\n",
    "    df[col] = condition_coder.transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='price_tomorrow'), df['price_tomorrow'], test_size=.3,\n",
    "                                                    random_state=17)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=.5, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_y_train = y.loc[:'2019'].to_numpy()\n",
    "array_y_val = y.loc['2020'].to_numpy()\n",
    "array_X_train = X.loc[:'2019'].to_numpy()\n",
    "array_X_val = X.loc['2020'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df.loc[:'2019'].drop(columns='price_tomorrow').values\n",
    "train_y = df.loc[:'2019', 'price_tomorrow'].values\n",
    "val_X = df.loc['2020'].drop(columns='price_tomorrow').values\n",
    "val_y = df.loc['2020', 'price_tomorrow'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 62)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (train_X.shape[1], train_X.shape[2])\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43800, 1, 62)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61008"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_y = df_array[21]\n",
    "array_y = df.drop(columns='price_tomorrow').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43800"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(array_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_X_train = df.loc[:'2019'].drop(columns='price_tomorrow').values\n",
    "array_y_train = df.loc[:'2019', 'price_tomorrow'].values\n",
    "array_X_val = df.loc['2020'].drop(columns='price_tomorrow').values\n",
    "array_y_val = df.loc['2020', 'price_tomorrow'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1825\n"
     ]
    }
   ],
   "source": [
    "batch_size = 24\n",
    "sequence_length = batch_size\n",
    "input_shape = (batch_size, array_X_train.shape[1])\n",
    "\n",
    "train_set = timeseries_dataset_from_array(data=array_X_train, \n",
    "                                          targets=array_y_train, \n",
    "                                          sequence_length=sequence_length, \n",
    "                                          batch_size=batch_size)\n",
    "val_set = timeseries_dataset_from_array(data=array_X_val, \n",
    "                                        targets=array_y_val, \n",
    "                                        sequence_length=sequence_length, \n",
    "                                        batch_size=batch_size)\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Neural Network\n",
    "Two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and build layers\n",
    "bm = models.Sequential()\n",
    "bm.add(layers.Dense(239, activation='relu', input_shape=input_shape))\n",
    "bm.add(layers.Dense(162, activation='relu'))\n",
    "bm.add(layers.Dense(24, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-544-f462fdf26fe2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1825/1825 [==============================] - 10s 6ms/step - loss: 14.3172 - mean_absolute_percentage_error: 14.3171 - val_loss: 37.1268 - val_mean_absolute_percentage_error: 37.1268\n",
      "Epoch 2/2\n",
      "1825/1825 [==============================] - 10s 5ms/step - loss: 14.3120 - mean_absolute_percentage_error: 14.3120 - val_loss: 41.7411 - val_mean_absolute_percentage_error: 41.7411\n"
     ]
    }
   ],
   "source": [
    "# Loss Metric to optimize\n",
    "metric = tf.keras.metrics.MeanAbsolutePercentageError(name='mean_absolute_percentage_error')\n",
    "\n",
    "# Create checkpoint to save model weights if this epoch's accuracy is the best so far\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='checkpoints/',\n",
    "    monitor=metric.name,\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True)\n",
    "\n",
    "# Create early stopping point\n",
    "callback = keras.callbacks.EarlyStopping(patience=10)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "bm.compile(loss=metric.name, \n",
    "           optimizer='Adam',\n",
    "           metrics=[metric.name])\n",
    "\n",
    "# Fit the model\n",
    "history = bm.fit(train_set, \n",
    "                 epochs = 2, \n",
    "                 callbacks=[checkpoint],\n",
    "                 validation_data=val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean_absolute_percentage_error'"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m   \u001b[1;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-526-ae1491060eff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'checkpoints/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2327\u001b[0m           'True when by_name is True.')\n\u001b[0;32m   2328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2329\u001b[1;33m     \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_detect_save_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2330\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tf'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2331\u001b[0m       \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_detect_save_format\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m   3012\u001b[0m   \u001b[1;31m# directory. It's possible for filepath to be both a prefix and directory.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3013\u001b[0m   \u001b[1;31m# Prioritize checkpoint over SavedModel.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3014\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3015\u001b[0m     \u001b[0msave_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'tf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3016\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_is_readable_tf_checkpoint\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m   3033\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3034\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3035\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3036\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3037\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m     98\u001b[0m   \u001b[1;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0merror_translator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m     33\u001b[0m       \u001b[1;34m'Failed to find any '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[0;32m     37\u001b[0m       \u001b[1;34m'Data type '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/"
     ]
    }
   ],
   "source": [
    "bm.load_weights('checkpoints/')\n",
    "model.save('models/{}'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.60542916,  1.13491958, -0.53602249, ..., -0.60937955,\n",
       "         1.98402493,  4.        ],\n",
       "       [ 0.62771746,  1.13467981, -0.36416936, ..., -0.60937955,\n",
       "         1.98402493,  4.        ],\n",
       "       [ 0.61657714,  1.13346918, -0.52947867, ..., -0.13194659,\n",
       "         2.15110321,  4.        ],\n",
       "       ...,\n",
       "       [ 1.66451673, -0.84885601, -0.11902295, ..., -0.60937955,\n",
       "         1.00834037,  4.        ],\n",
       "       [ 1.60146487, -0.84885601, -0.20472313, ..., -0.13194659,\n",
       "         0.85011052,  4.        ],\n",
       "       [ 1.42163019, -0.84885601, -0.46096862, ..., -0.13194659,\n",
       "         1.00834037,  4.        ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>generation biomass</th>\n",
       "      <th>generation fossil brown coal/lignite</th>\n",
       "      <th>generation fossil gas</th>\n",
       "      <th>generation fossil hard coal</th>\n",
       "      <th>generation fossil oil</th>\n",
       "      <th>generation hydro pumped storage consumption</th>\n",
       "      <th>generation hydro run-of-river and poundage</th>\n",
       "      <th>generation hydro water reservoir</th>\n",
       "      <th>generation nuclear</th>\n",
       "      <th>generation other</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_speeds_bilbao</th>\n",
       "      <th>pressures_bilbao</th>\n",
       "      <th>condition_bilbao</th>\n",
       "      <th>temp_valencia</th>\n",
       "      <th>dew_point_valencia</th>\n",
       "      <th>humidities_valencia</th>\n",
       "      <th>wind_valencia</th>\n",
       "      <th>wind_speeds_valencia</th>\n",
       "      <th>pressures_valencia</th>\n",
       "      <th>condition_valencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <th>5</th>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           generation biomass  generation fossil brown coal/lignite  \\\n",
       "2021 5 19                   8                                     8   \n",
       "\n",
       "           generation fossil gas  generation fossil hard coal  \\\n",
       "2021 5 19                      8                            8   \n",
       "\n",
       "           generation fossil oil  generation hydro pumped storage consumption  \\\n",
       "2021 5 19                      8                                            8   \n",
       "\n",
       "           generation hydro run-of-river and poundage  \\\n",
       "2021 5 19                                           8   \n",
       "\n",
       "           generation hydro water reservoir  generation nuclear  \\\n",
       "2021 5 19                                 8                   8   \n",
       "\n",
       "           generation other  ...  wind_speeds_bilbao  pressures_bilbao  \\\n",
       "2021 5 19                 8  ...                   8                 8   \n",
       "\n",
       "           condition_bilbao  temp_valencia  dew_point_valencia  \\\n",
       "2021 5 19                 8              8                   8   \n",
       "\n",
       "           humidities_valencia  wind_valencia  wind_speeds_valencia  \\\n",
       "2021 5 19                    8              8                     8   \n",
       "\n",
       "           pressures_valencia  condition_valencia  \n",
       "2021 5 19                   8                   8  \n",
       "\n",
       "[1 rows x 62 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.groupby(by=[X.index.year, X.index.month, X.index.day]).count().loc[X.groupby(by=[X.index.year, X.index.month, X.index.day]).count()['generation biomass']<24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2543"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.groupby(by=[X.index.year, X.index.month, X.index.day]).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
